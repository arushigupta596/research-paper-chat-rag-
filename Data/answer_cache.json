{
  "What video generation models are discussed in the papers?": {
    "question": "What video generation models are discussed in the papers?",
    "answer": "The papers discuss several video generation models, including:\n\nThe Hunyuan Foundation Model [Evidence 1, Evidence 4, Evidence 8] is a foundation model that establishes a new state-of-the-art among generation models, capable of producing initial video sequences with resolutions from 480p to 720p and durations of 5 seconds, and then upscales them to 1080p using a dedicated Video Super-Resolution Network.\n\nSora2 [Evidence 1] is a video foundation model that achieves state-of-the-art performance in video generation.\n\nVeo3 [Evidence 1, Evidence 7] is a scalable and controllable text-to-video model that achieves higher performance than Sora2 and Veo3.\n\nStepVideo [Evidence 1] is an open-source video generation model that achieves state-of-the-art performance in video generation.\n\nWan2.2 [Evidence 4] employs a MoE architecture that leverages two 14-billion-parameter expert models to enhance visual fidelity, but it inherently introduces computational inefficiencies and demands multiple large parameter sets.\n\nLumina-t2x [Evidence 2] is a model that transforms text into any modality, resolution, and duration via flow-based large diffusion transformers.\n\nI2vgen-xl [Evidence 6] is a high-quality image-to-video synthesis model via cascaded diffusion models.\n\nWaver [Evidence 6] is a wave-based video generation model that achieves lifelike video generation.\n\nOpen-sora [Evidence 6] is a democratizing video production model for all, enabling efficient video generation.\n\nMagicvideo [Evidence 6] is an efficient video generation model with latent diffusion models.\n\nDiffPhy [Evidence 9] and PhyT2V [Evidence 9] are models that adopt an LLM to extend the text prompt with explicit physics laws and phenomena, and then generate the video or fine-tune a T2V model with extended prompts.\n\nGoku [Evidence 10] is a flow-based video generative foundation model.\n\nVideocrafterl [Evidence 10] and Videocrafter2 [Evidence 10] are open diffusion models for high-quality video generation.\n\nHierarchical fine-grained preference optimization for physically plausible video generation [Evidence 10] is a model that optimizes video generation for physically plausible results.\n\n---\n\n**Evidence:**\n- Evidence 1 (from Hunyuan Foundation Model): Discusses several video generation models, including Sora2, Veo3, and StepVideo.\n- Evidence 2 (from Text-to-Video Generation with Physical Consistency): Introduces Lumina-t2x, a model that transforms text into any modality, resolution, and duration via flow-based large diffusion transformers.\n- Evidence 4 (from Hunyuan Foundation Model): Introduces the Hunyuan Foundation Model, a foundation model that establishes a new state-of-the-art among generation models.\n- Evidence 6 (from Text-to-Video Generation with Physical Consistency): Discusses several video generation models, including I2vgen-xl, Waver, Open-sora, Magicvideo, DiffPhy, and PhyT2V.\n- Evidence 8 (from Hunyuan Foundation Model): Introduces the Hunyuan Foundation Model's ability to produce initial video sequences with resolutions from 480p to 720p and durations of 5 seconds.\n- Evidence 9 (from Text-to-Video Generation with Physical Consistency): Discusses several video generation models, including DiffPhy and PhyT2V.\n- Evidence 10 (from Text-to-Video Generation with Physical Consistency): Discusses several video generation models, including Goku, Videocrafterl, Videocrafter2, and Hierarchical fine-grained preference optimization for physically plausible video generation.\n\n**Sources:**\n- \"Hunyuan Foundation Model\" (Topic: Foundation Models) - Pages referenced: 1, 13\n- \"Text-to-Video Generation with Physical Consistency\" (Topic: Video Generation) - Pages referenced: 8, 9, 10, 12, 14\n- \"Computer Vision and Image Processing\" (Topic: Computer Vision) - Pages referenced: 3\n- \"Hunyuan Foundation Model\" (Topic: Foundation Models) - Pages referenced: 1\n- \"Text-to-Video Generation with Physical Consistency\" (Topic: Video Generation) - Pages referenced: 8, 9, 10, 11, 14\n- \"Text-to-Video Generation with Physical Consistency\" (Topic: Video Generation) - Pages referenced: 8, 9, 10, 11, 14\n- \"Text-to-Video Generation with Physical Consistency\" (Topic: Video Generation) - Pages referenced: 8, 9, 10, 11, 14\n- \"Text-to-Video Generation with Physical Consistency\" (Topic: Video Generation) - Pages referenced: 8, 9, 10, 11, 14\n- \"Text-to-Video Generation with Physical Consistency\" (Topic: Video Generation) - Pages referenced: 8, 9, 10, 11, 14\n- \"Text-to-Video Generation with Physical Consistency\" (Topic: Video Generation) - Pages referenced: 8, 9, 10, 11, 14\n- \"Text-to-Video Generation with Physical Consistency\" (Topic: Video Generation) - Pages referenced: 8, 9, 10, 11, 14\n- \"Text-to-Video Generation with Physical Consistency\" (Topic: Video Generation) - Pages referenced: 8, 9, 10, 11, 14\n- \"Text-to-Video Generation with Physical Consistency\" (Topic: Video Generation) - Pages referenced: 8,",
    "evidence": [
      {
        "text": "Sora 2. https: 2025. Hunyuan Foundation Model Team. Hunyuanvideo: A systematic framework for large generative models, 2025. URL\\https://arxiv.org/abs/2412. 03603) Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Ranchen Ming, Xiaoniu Song, Xing Chen, Yu Zhou, and Deshan Sun et al. Step-video-t2v report: The practice, challenges, and future of video foundation model, 2025. URL //arxiv.org/abs/2502.10248 Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, and Jingfeng Zhang et al. Wan: and advanced large-scale video generative models. arXiv preprint arXiv:2503.203 14, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. Flashattention-3: Fast and accurate attention with asynchrony and low-precision, 2024. URL //arxiv.org/abs/2407 .08608 Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Chen, Yuankun Chen, Yutian Chen, Zhuofu Chen, and Jialei Cui et al. Kimi k2: Open intelligence, 2025. URL https: //arxiv.org/abs/2507. 20534| Hangting Chen, Peng Chen, Yiji Cheng, Yutao Cui, Xinchi Deng, Ying Dong, Kipper Tianpeng Gu, Xiusen Gu, et al. Hunyuanimage 3.0 technical report. arXiv preprint arXiv:2509.23951, 2025. Contributors. Pyscenedetect: A python library for video scene detection, 2020. https://github. com/Breakthrough/PySceneDetect| Accessed: 2023-11-20. Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Yan, and Weisi Lin. Exploring video quality assessment on user generated contents from and technical perspectives. In Proceedings of the IEEE/CVF International Conference Computer Vision, pages 20144-20154, 2023. Yang, Xufang Luo, Dongqi Han, Yunjian Xu, and Dongsheng Li. Mitigating hallucina- large vision-language models via dpo: On-policy data hold the key. In Proceedings of Computer Vision and Pattern Recognition Conference, pages 10610-10620, 2025.",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 13,
          "region_type": "list",
          "region_id": "page13_region0"
        },
        "score": 0.659,
        "citation": "2511.18870v2.pdf, Page 13"
      },
      {
        "text": "arXiv:2501.08453, 2025. Peng Gao, Le Zhuo, Dongyang Liu, Ruoyi Du, Xu Luo, Longtian Qiu, Yuhang Zhang, Chen Lin, Rongjie Huang, Shijie Geng, et al. Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers. arXiv preprint arXiv:2405.05945, 2024. Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Fei-Fei Li, Irfan Essa, Lu Jiang, and José Lezama. Photorealistic video generation with diffusion models. In ECCV, 2024. Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. arXiv preprint arXiv:2211.13221, 2022. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. NeurIPS, 2022. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In JCLR, 2022. Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video generative modeling. In ICLR, 2025. Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video generative modeling. In JCLR, 2025. Xuan Ju, Tianyu Wang, Yugian Zhou, He Zhang, Qing Liu, Nanxuan Zhao, Zhifei Zhang, Yijun Li, Yuanhao Cai, Shaoteng Liu, et al.",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 12,
          "region_type": "list",
          "region_id": "page12_region0"
        },
        "score": 0.592,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 12"
      },
      {
        "text": "images in InternVL3 (Zhu et al., 2025) 10K-frame videos in Video-XL-2 (Qin et al., enabling strong performance on image (Bai 2025), video (Yang et al., 2025b), and multi- reasoning tasks (Shen et al., 2025). More Complex Content in Generation AI content generation has advanced sig- with the application of Transformers to domains (Peebles and Xie, 2023; Brooks 2024; Li et al., 2024f). Early diffusion mod- Stable Diffusion (Rombach et al., 2022) only 512 x 512 resolution images. With now successfully applied to gener- (Peebles and Xie, 2023; Gao et al., 2023; et al., 2024; Li et al., 2024f), DiT-based produce high-quality 4K images in PixArt- et al., 2024a) and hour-long videos in (Brooks et al., 2024). These models capture spatiotemporal dependencies, enabling content generation (Labs, 2024; Yang 2024c; Wan et al., 2025a; Kang et al., 2025). these advancements across domains have outstanding performance, they now et al., 2024b; Zou et al., 2025). (1) Efficient Computation Since computational efficiency architectural design, optimizing tal. Unlike Transformers with complexity O(n?) (Vaswani methods achieve linear or (i) linear attention reformulates complexity (Katharopoulos et 2021); (ii) RWKV combines ing with transformer parallelism Duan et al., 2024); (iii) State Mamba use structured state plexity (Gu and Dao, 2023; quire retraining, motivating (II) Model-centric Compression ing parameter complexity and memory costs. Model centric, transforming W to a W’=T(W), where with I as the compression",
        "source": {
          "paper": "2505.19147v3.pdf",
          "page": 3,
          "region_type": "text",
          "region_id": "page3_region7"
        },
        "score": 0.479,
        "citation": "2505.19147v3.pdf, Page 3"
      },
      {
        "text": "and proficient model establishes a new state-of-the-art among generation models. By releasing the code and model weights, community with a high-performance foundation that lowers the creation and research, making advanced video generation ac- broader audience. All open-source assets are publicly available at //github.com/Tencent-Hunyuan/HunyuanVideo-1.5 remarkable advancements in video generation models, with closed- Kling2.5 [I], Veo3.1 [2], and Sora2 achieving state-of-the-art (SOTA) open-source domain, models such as HunyuanVideo [4], StepVideo [5], and as notable contenders. However, despite these strides, most SOTA models accessibility and community-driven innovation. Wan2.2 [6] employs a (MoE) architecture that leverages two 14-billion-parameter expert models Although this MoE architecture enhances visual fidelity by leveraging different denoising stages, it inherently introduces computational inefficiencies, multiple large parameter sets (totaling 27B parameters with 14B activated), demands. While Wan2.2 [6] has attempted to address this with a more variant, which utilizes a high-compression 3D VAE to reduce memory model still exhibits limitations. Specifically, its generative capabilities, motion stability across frames and achieving the nuanced aesthetic quality applications, continue to fall short of practical demands. This situation",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 1,
          "region_type": "text",
          "region_id": "page1_region2"
        },
        "score": 0.658,
        "citation": "2511.18870v2.pdf, Page 1"
      },
      {
        "text": "480x832. We set tr = 3, N = 100, Qmin = 0.5, ky = 2.0, b, = 0.4, A = 0.6, ka = 5.0, ba = 0.5. The rank of LoRA to 48. We follow the same metrics of VideoPhy2 and PhyGenBench to evaluate the T2V generation models. Comparison with State-of-the-Art Methods",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 8,
          "region_type": "text",
          "region_id": "page8_region5"
        },
        "score": 0.556,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 8"
      },
      {
        "text": "efficient high-resolution video generation. arXiv preprint arXiv:2502.05179, 2025. Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models. arXiv preprint arXiv:2311.04145, 2023. Yifu Zhang, Hao Yang, Yugi Zhang, Yifei Hu, Fengda Zhu, Chuang Lin, Xiaofeng Mei, Yi Jiang, Bingyue Peng, and Zehuan Yuan. Waver: Wave your way to lifelike video generation. arXiv preprint arXiv:2508.15761, 2025. Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and ‘Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. arXiv preprint arXiv:2211.11018, 2022.",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 14,
          "region_type": "list",
          "region_id": "page14_region0"
        },
        "score": 0.556,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 14"
      },
      {
        "text": "higher than Sora2 and Veo3. (ii) Compared to the SOTA DPO algorithm for video generation",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 9,
          "region_type": "text",
          "region_id": "page9_region8"
        },
        "score": 0.536,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 9"
      },
      {
        "text": "producing initial video sequences with resolutions from 480p to 720p and durations of 5 The second stage utilizes a dedicated Video Super-Resolution Network to upscale 1080p, significantly enhancing the final visual fidelity. The main contributions of this summarized as follows:",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 1,
          "region_type": "text",
          "region_id": "page1_region1"
        },
        "score": 0.518,
        "citation": "2511.18870v2.pdf, Page 1"
      },
      {
        "text": "Text-to-Video Generation Models generation [5-7, 18, 25-27, 30, 62, 71, 74, 77| has witnessed significant progress. Many existing works follow [49] to adopt a Transformer [56] to predict the noise in diffusion [9, 22, 24, 35, 37, 42-44, 51] or estimate the field in flow matching [8, 14, 19, 23, 29, 31, 34, 58, 59, 73, 75]. Although high visual quality is achieved, modeling the underlying physics-related effects remains challenging. Two recent works, DiffPhy [72] PhyT2V [67], adopt an LLM to extend the text prompt with explicit physics laws and phenomena, and then generates the video or finetunes a T2V model with extended prompts. Yet, these prompt extension-based are easily misled by the mistakes of LLM and struggle to learn the implicit physics. Different from these we aim to explore the physics reasoning potential of the T2V model itself.",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 10,
          "region_type": "text",
          "region_id": "page10_region3"
        },
        "score": 0.505,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 10"
      },
      {
        "text": "u, Yulun Zhang, Xiaokang Yang, Zhe Lin, and Alan Yuille. Baking gaussian splatting into diffusion denoiser for fast and scalable single-stage image-to-3d generation and reconstruction. In ICCV, 2025. Haoxuan Che, Xuanhua He, Quande Liu, Cheng Jin, and Hao Chen. Gamegen-x: Interactive open-world game video generation. In ICLR, 2025. Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafterl: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In CVPR, 2024. Harold Haodong Chen, Haojian Huang, Qifeng Chen, Harry Yang, and Ser-Nam Lim. Hierarchical fine-grained preference optimization for physically plausible video generation. In NeurIPS, 2025. Shoufa Chen, Chongjian Ge, Yugi Zhang, Yida Zhang, Fengda Zhu, Hao Yang, Hongxiang Hao, Hui Wu, Zhichao Lai, Yifei Hu, et al. Goku: Flow based video generative foundation models. In CVPR, 2025. Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. arXiv preprint 2016. Xiaoyu Chen, Junliang Guo, Tianyu He, Chuheng Zhang, Pushi Zhang, Derek Cathera Yang, Li Zhao, and Jiang Bian. Igor: Image-goal representations are the atomic control units for foundation models in embodied ai. arXiv preprint arXiv:2411.00785, 2024. Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, Nicu Sebe, and Mubarak Shah. Curriculum direct preference optimization for diffusion and consistency models. In CVPR, 2025. Aram Davtyan, Sepehr Sameni, and Paolo Favaro. Efficient video prediction via sparsely conditioned flow matching. In ICCV, 2023. Aram Davtyan, Sepehr Sameni, and Paolo Favaro. Efficient video prediction via sparsely conditioned flow matching. In ICCV, 2023. Google DeepMind. Veo-3: A scalable and controllable text-to-video model.",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 11,
          "region_type": "list",
          "region_id": "page11_region0"
        },
        "score": 0.497,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 11"
      }
    ],
    "sources": [
      "Computer Vision and Image Processing (Topic: Computer Vision) - Pages 3",
      "Hunyuan Foundation Model (Topic: Foundation Models) - Pages 1, 13",
      "Text-to-Video Generation with Physical Consistency (Topic: Video Generation) - Pages 8, 9, 10, 11, 12, 14"
    ],
    "has_evidence": true,
    "retrieval_stats": {
      "total_chunks": 10,
      "papers_searched": [
        "2511.18870v2.pdf",
        "605562216_1287553579874010_8068650067268945856_n.pdf",
        "2505.19147v3.pdf"
      ],
      "by_paper": {
        "2511.18870v2.pdf": 3,
        "605562216_1287553579874010_8068650067268945856_n.pdf": 6,
        "2505.19147v3.pdf": 1
      },
      "by_region_type": {
        "list": 4,
        "text": 6
      }
    },
    "cached_at": "2026-02-11T00:32:03.324607"
  },
  "How does HunyuanVideo achieve state-of-the-art video generation?": {
    "question": "How does HunyuanVideo achieve state-of-the-art video generation?",
    "answer": "HunyuanVideo achieves state-of-the-art video generation through a multi-stage pipeline that includes meticulous data curation, a compact 8.3B-parameter Diffusion Transformer, and a dedicated video super-resolution network [Evidence 8]. This pipeline enables the model to produce high-quality text-to-video and image-synthesis, with exceptional bilingual prompt understanding, rendering, and reliable instruction-following [Evidence 8]. Additionally, HunyuanVideo employs Reinforcement Learning from Human Feedback (RLHF) strategies tailored for T2V tasks, which corrects artifacts and enhances motion quality [Evidence 9]. The model also utilizes rigorously selected clips per task, filtered strictly considering aesthetic and motion smoothness, through CT and SFT, resulting in evident improvements in visual quality, aesthetic appeal, and temporal consistency for both generation tasks [Evidence 9].\n\nThe HunyuanVideo 1.5 model is built via training from a text-to-image model, and it achieves high-quality text-to-video and image-synthesis through meticulous data curation and a multi-stage pipeline [Evidence 8]. This model is a compact 8.3B-parameter Diffusion Transformer, which integrates SSTA for accelerated inference and a dedicated video super-resolution network output, ensuring state-of-the-art visual quality, motion coherence, and strong alignment [Evidence 8]. The efficient DiT integrates SSTA for accelerated inference and a dedicated video super-resolution network output, ensuring state-of-the-art visual quality, motion coherence, and strong alignment [Evidence 8].\n\nThe HunyuanVideo 1.5 model is also capable of producing initial video sequences with resolutions from 480p to 720p and durations of 5 seconds, and the second stage utilizes a dedicated Video Super-Resolution Network to upscale to 1080p, significantly enhancing the final visual fidelity [Evidence 3]. This allows HunyuanVideo to achieve state-of-the-art video generation by producing high-quality video sequences with resolutions up to 1080p.\n\nThe HunyuanVideo model is also compared to its competitors, including Sora 2, Wan, and other advanced large-scale video generative models, and it is shown to be competitive with these models in terms of visual quality and motion coherence [Evidence 6].\n\nOverall, HunyuanVideo achieves state-of-the-art video generation through a combination of meticulous data curation, a compact 8.3B-parameter Diffusion Transformer, and a dedicated video super-resolution network, as well as the use of Reinforcement Learning from Human Feedback (RLHF) strategies tailored for T2V tasks.\n\n---\n**Evidence:**\n- Evidence 1 (from Foundation Models): Describes the HunyuanVideo model and its capabilities in video generation.\n- Evidence 2 (from Video Generation): Lists recent papers on video generation, including HunyuanVideo.\n- Evidence 3 (from Foundation Models): Describes the multi-stage pipeline of HunyuanVideo and its ability to produce high-quality video sequences.\n- Evidence 4 (from Foundation Models): Lists recent papers on video generation, including HunyuanVideo.\n- Evidence 5 (from Foundation Models): Describes the filtering stages used in HunyuanVideo to select high-quality video segments.\n- Evidence 6 (from Foundation Models): Compares HunyuanVideo to its competitors in terms of visual quality and motion coherence.\n- Evidence 7 (from Video Generation): Lists recent papers on video generation, including HunyuanVideo.\n- Evidence 8 (from Foundation Models): Describes the HunyuanVideo 1.5 model and its capabilities in text-to-video and image-synthesis.\n- Evidence 9 (from Foundation Models): Describes the use of Reinforcement Learning from Human Feedback (RLHF) strategies in HunyuanVideo.\n- Evidence 10 (from Video Generation): Lists recent papers on video generation, including HunyuanVideo.\n\n**Sources:**\n- Hunyuan Foundation Model (Topic: Foundation Models) - Pages 1, 3, 7, 8, 10, 11\n- Text-to-Video Generation with Physical Consistency (Topic: Video Generation) - Pages 11, 12",
    "evidence": [
      {
        "text": "and proficient model establishes a new state-of-the-art among generation models. By releasing the code and model weights, community with a high-performance foundation that lowers the creation and research, making advanced video generation ac- broader audience. All open-source assets are publicly available at //github.com/Tencent-Hunyuan/HunyuanVideo-1.5 remarkable advancements in video generation models, with closed- Kling2.5 [I], Veo3.1 [2], and Sora2 achieving state-of-the-art (SOTA) open-source domain, models such as HunyuanVideo [4], StepVideo [5], and as notable contenders. However, despite these strides, most SOTA models accessibility and community-driven innovation. Wan2.2 [6] employs a (MoE) architecture that leverages two 14-billion-parameter expert models Although this MoE architecture enhances visual fidelity by leveraging different denoising stages, it inherently introduces computational inefficiencies, multiple large parameter sets (totaling 27B parameters with 14B activated), demands. While Wan2.2 [6] has attempted to address this with a more variant, which utilizes a high-compression 3D VAE to reduce memory model still exhibits limitations. Specifically, its generative capabilities, motion stability across frames and achieving the nuanced aesthetic quality applications, continue to fall short of practical demands. This situation",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 1,
          "region_type": "text",
          "region_id": "page1_region2"
        },
        "score": 0.653,
        "citation": "2511.18870v2.pdf, Page 1"
      },
      {
        "text": "efficient high-resolution video generation. arXiv preprint arXiv:2502.05179, 2025. Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models. arXiv preprint arXiv:2311.04145, 2023. Yifu Zhang, Hao Yang, Yugi Zhang, Yifei Hu, Fengda Zhu, Chuang Lin, Xiaofeng Mei, Yi Jiang, Bingyue Peng, and Zehuan Yuan. Waver: Wave your way to lifelike video generation. arXiv preprint arXiv:2508.15761, 2025. Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and ‘Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. arXiv preprint arXiv:2211.11018, 2022.",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 14,
          "region_type": "list",
          "region_id": "page14_region0"
        },
        "score": 0.553,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 14"
      },
      {
        "text": "producing initial video sequences with resolutions from 480p to 720p and durations of 5 The second stage utilizes a dedicated Video Super-Resolution Network to upscale 1080p, significantly enhancing the final visual fidelity. The main contributions of this summarized as follows:",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 1,
          "region_type": "text",
          "region_id": "page1_region1"
        },
        "score": 0.582,
        "citation": "2511.18870v2.pdf, Page 1"
      },
      {
        "text": "Sora 2. https: 2025. Hunyuan Foundation Model Team. Hunyuanvideo: A systematic framework for large generative models, 2025. URL\\https://arxiv.org/abs/2412. 03603) Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Ranchen Ming, Xiaoniu Song, Xing Chen, Yu Zhou, and Deshan Sun et al. Step-video-t2v report: The practice, challenges, and future of video foundation model, 2025. URL //arxiv.org/abs/2502.10248 Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, and Jingfeng Zhang et al. Wan: and advanced large-scale video generative models. arXiv preprint arXiv:2503.203 14, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. Flashattention-3: Fast and accurate attention with asynchrony and low-precision, 2024. URL //arxiv.org/abs/2407 .08608 Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Chen, Yuankun Chen, Yutian Chen, Zhuofu Chen, and Jialei Cui et al. Kimi k2: Open intelligence, 2025. URL https: //arxiv.org/abs/2507. 20534| Hangting Chen, Peng Chen, Yiji Cheng, Yutao Cui, Xinchi Deng, Ying Dong, Kipper Tianpeng Gu, Xiusen Gu, et al. Hunyuanimage 3.0 technical report. arXiv preprint arXiv:2509.23951, 2025. Contributors. Pyscenedetect: A python library for video scene detection, 2020. https://github. com/Breakthrough/PySceneDetect| Accessed: 2023-11-20. Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Yan, and Weisi Lin. Exploring video quality assessment on user generated contents from and technical perspectives. In Proceedings of the IEEE/CVF International Conference Computer Vision, pages 20144-20154, 2023. Yang, Xufang Luo, Dongqi Han, Yunjian Xu, and Dongsheng Li. Mitigating hallucina- large vision-language models via dpo: On-policy data hold the key. In Proceedings of Computer Vision and Pattern Recognition Conference, pages 10610-10620, 2025.",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 13,
          "region_type": "list",
          "region_id": "page13_region0"
        },
        "score": 0.56,
        "citation": "2511.18870v2.pdf, Page 13"
      },
      {
        "text": "videos across four dimensions: sharpness, detail retention, noise & artifacts, and This allows us to discard clips with poor visual quality. Filtering: We applied an aesthetic scoring operator based on to evaluate the of the videos, filtering out those with low aesthetic scores. these filtering stages, approximately 800 million high-quality video segments play a crucial role in video generation, significantly influencing precise",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 3,
          "region_type": "list",
          "region_id": "page3_region2"
        },
        "score": 0.544,
        "citation": "2511.18870v2.pdf, Page 3"
      },
      {
        "text": "HunyuanVideo 1.5 720P 12V model and its competitors",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 10,
          "region_type": "text",
          "region_id": "page10_region9"
        },
        "score": 0.506,
        "citation": "2511.18870v2.pdf, Page 10"
      },
      {
        "text": "al flow matching for efficient video generative modeling. In JCLR, 2025. Xuan Ju, Tianyu Wang, Yugian Zhou, He Zhang, Qing Liu, Nanxuan Zhao, Zhifei Zhang, Yijun Li, Yuanhao Cai, Shaoteng Liu, et al. Editverse: Unifying image and video editing and generation with in-context learning. arXiv preprint arXiv:2509.20360, 2025. Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao Huang, and Jiashi Feng. How far is video generation from world model: A physical law perspective. In JCML, 2025. Shyamgopal Karthik, Huseyin Coskun, Zeynep Akata, Sergey Tulyakov, Jian Ren, and Anil Kag. Scalable ranked preference optimization for text-to-image generation. In ICCV, 2025. Lei Ke, Haohang Xu, Xuefei Ning, Yu Li, Jiajun Li, Haoling Li, Yuxuan Lin, Dongsheng Jiang, Yujiu Yang, and Linfeng Zhang. Proreflow: Progressive reflow with decomposed velocity. In CVPR, 2025. Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, et al. Videopoet: A large language model for zero-shot video generation. In ICML, 2024. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: A systematic framework for large video generative models. arXiv preprint arXiv:2412.03608, 2024. Xin Li, Wenqing Chu, Ye Wu, Weihang Yuan, Fanglong Liu, Qi Zhang, Fu Li, Haocheng Feng, Errui Ding, and Jingdong Wang. Videogen: A reference-guided latent diffusion approach for high definition text-to-video generation. arXiv preprint arXiv:2309.00398, 2023. Jie Liu, Gongye Liu, Jiajun Liang, Ziyang Yuan, Xiaokun Liu, Mingwu Zheng, Xiele Wu, Qiulin Wang, Wenyu Qin, Menghan Xia, et al. Improving video generation with human feedback. arXiv preprint arXiv:2501.13918, 2025. Runtao Liu, Haoyu Wu, Ziqiang Zheng, Chen Wei, Yingqing He, Renjie Pi, and Qifeng Chen. Videodpo: Omni-preference alignment for video diffusion generation. In CVPR, 2025. Ilya Loshchilov and Frank Hutter.",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 12,
          "region_type": "list",
          "region_id": "page12_region0"
        },
        "score": 0.491,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 12"
      },
      {
        "text": "we present HunyuanVideo 1.5, a compact 8.3B-parameter Diffusion Transformer open-source video generation that competes with leading proprietary systems. Built via training from a text-to-image model, it achieves high-quality text-to-video and image- synthesis through meticulous data curation and a multi-stage pipeline. The efficient DiT integrates SSTA for accelerated inference and a dedicated video super-resolution network output, ensuring state-of-the-art visual quality, motion coherence, and strong alignment. HunyuanVideo 1.5 exhibits exceptional bilingual prompt understanding, rendering, and reliable instruction-following. Comprehensive evaluations confirm it sets among open-source video generators. By releasing this high-performance, lightweight provide an accessible foundation to lower barriers for creative and research applications,",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 11,
          "region_type": "text",
          "region_id": "page11_region2"
        },
        "score": 0.483,
        "citation": "2511.18870v2.pdf, Page 11"
      },
      {
        "text": "checkpoint, enabling upon a high-quality SFT, we utilize rigorously selected clips per task, filtered strictly considering aesthetic and motion smoothness. Through CT and SFT, we observe evident improvements in visual quality, aesthetic appeal, and temporal consistency for both generation tasks. employ Reinforcement Learning from Human Feedback (RLHF) strategies tailored for T2V tasks, both mainly aimed at correcting artifacts and enhancing motion quality. task, we apply online reinforcement learning (RL) during post-training to correct",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 7,
          "region_type": "text",
          "region_id": "page7_region1"
        },
        "score": 0.483,
        "citation": "2511.18870v2.pdf, Page 7"
      },
      {
        "text": "u, Yulun Zhang, Xiaokang Yang, Zhe Lin, and Alan Yuille. Baking gaussian splatting into diffusion denoiser for fast and scalable single-stage image-to-3d generation and reconstruction. In ICCV, 2025. Haoxuan Che, Xuanhua He, Quande Liu, Cheng Jin, and Hao Chen. Gamegen-x: Interactive open-world game video generation. In ICLR, 2025. Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafterl: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In CVPR, 2024. Harold Haodong Chen, Haojian Huang, Qifeng Chen, Harry Yang, and Ser-Nam Lim. Hierarchical fine-grained preference optimization for physically plausible video generation. In NeurIPS, 2025. Shoufa Chen, Chongjian Ge, Yugi Zhang, Yida Zhang, Fengda Zhu, Hao Yang, Hongxiang Hao, Hui Wu, Zhichao Lai, Yifei Hu, et al. Goku: Flow based video generative foundation models. In CVPR, 2025. Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. arXiv preprint 2016. Xiaoyu Chen, Junliang Guo, Tianyu He, Chuheng Zhang, Pushi Zhang, Derek Cathera Yang, Li Zhao, and Jiang Bian. Igor: Image-goal representations are the atomic control units for foundation models in embodied ai. arXiv preprint arXiv:2411.00785, 2024. Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, Nicu Sebe, and Mubarak Shah. Curriculum direct preference optimization for diffusion and consistency models. In CVPR, 2025. Aram Davtyan, Sepehr Sameni, and Paolo Favaro. Efficient video prediction via sparsely conditioned flow matching. In ICCV, 2023. Aram Davtyan, Sepehr Sameni, and Paolo Favaro. Efficient video prediction via sparsely conditioned flow matching. In ICCV, 2023. Google DeepMind. Veo-3: A scalable and controllable text-to-video model.",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 11,
          "region_type": "list",
          "region_id": "page11_region0"
        },
        "score": 0.471,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 11"
      }
    ],
    "sources": [
      "Hunyuan Foundation Model (Topic: Foundation Models) - Pages 1, 3, 7, 10, 11, 13",
      "Text-to-Video Generation with Physical Consistency (Topic: Video Generation) - Pages 11, 12, 14"
    ],
    "has_evidence": true,
    "retrieval_stats": {
      "total_chunks": 10,
      "papers_searched": [
        "2511.18870v2.pdf",
        "605562216_1287553579874010_8068650067268945856_n.pdf"
      ],
      "by_paper": {
        "2511.18870v2.pdf": 7,
        "605562216_1287553579874010_8068650067268945856_n.pdf": 3
      },
      "by_region_type": {
        "text": 5,
        "list": 5
      }
    },
    "cached_at": "2026-02-11T00:32:17.566584"
  },
  "What are the main components of the video generation pipeline?": {
    "question": "What are the main components of the video generation pipeline?",
    "answer": "The main components of the video generation pipeline can be identified from various research papers on the topic. According to the Hunyuan Foundation Model [Evidence 1], the pipeline consists of two stages: the first stage generates initial video sequences with resolutions from 480p to 720p and durations of 5 seconds, while the second stage utilizes a dedicated Video Super-Resolution Network to upscale the resolution to 1080p, significantly enhancing the final visual fidelity [Evidence 5].\n\nAdditionally, the Text-to-Video Generation with Physical Consistency paper [Evidence 7] mentions that the pipeline involves a DPO (Direct Preference Optimization) algorithm for video generation, which is compared to the SOTA (State-of-the-Art) DPO algorithm for video generation.\n\nFurthermore, the Hunyuan Foundation Model [Evidence 9] highlights the importance of filtering stages in the pipeline, which includes aesthetic scoring and filtering out clips with poor visual quality. This is crucial for generating high-quality video segments, which play a significant role in video generation.\n\nIn summary, the main components of the video generation pipeline include:\n\n* Initial video sequence generation with resolutions from 480p to 720p and durations of 5 seconds\n* Video Super-Resolution Network for upsampling the resolution to 1080p\n* DPO algorithm for video generation\n* Filtering stages, including aesthetic scoring and filtering out clips with poor visual quality\n\nThese components work together to generate high-quality video segments, which are essential for various applications, including video generation, text-to-video generation, and image-to-video generation.\n\n---\n**Evidence:**\n- Evidence 1 (from Foundation Models): Describes the two-stage video generation pipeline, including initial video sequence generation and video super-resolution.\n- Evidence 5 (from Foundation Models): Highlights the importance of the second stage in the pipeline, which utilizes a dedicated Video Super-Resolution Network to upscale the resolution to 1080p.\n- Evidence 7 (from Video Generation): Mentions the DPO algorithm for video generation and compares it to the SOTA DPO algorithm.\n- Evidence 9 (from Foundation Models): Emphasizes the role of filtering stages in the pipeline, including aesthetic scoring and filtering out clips with poor visual quality.\n\n**Sources:**\n- \"Hunyuan Foundation Model\" (Topic: Foundation Models) - Pages 1, 3, 5, 8\n- \"Text-to-Video Generation with Physical Consistency\" (Topic: Video Generation) - Pages 7, 9, 12, 14\n- \"Computer Vision and Image Processing\" (Topic: Computer Vision) - Page 3\n- \"MiniMax-Speech: Autoregressive Text-to-Speech Model\" (Topic: Text-to-Speech Generation) - Page 6",
    "evidence": [
      {
        "text": "and proficient model establishes a new state-of-the-art among generation models. By releasing the code and model weights, community with a high-performance foundation that lowers the creation and research, making advanced video generation ac- broader audience. All open-source assets are publicly available at //github.com/Tencent-Hunyuan/HunyuanVideo-1.5 remarkable advancements in video generation models, with closed- Kling2.5 [I], Veo3.1 [2], and Sora2 achieving state-of-the-art (SOTA) open-source domain, models such as HunyuanVideo [4], StepVideo [5], and as notable contenders. However, despite these strides, most SOTA models accessibility and community-driven innovation. Wan2.2 [6] employs a (MoE) architecture that leverages two 14-billion-parameter expert models Although this MoE architecture enhances visual fidelity by leveraging different denoising stages, it inherently introduces computational inefficiencies, multiple large parameter sets (totaling 27B parameters with 14B activated), demands. While Wan2.2 [6] has attempted to address this with a more variant, which utilizes a high-compression 3D VAE to reduce memory model still exhibits limitations. Specifically, its generative capabilities, motion stability across frames and achieving the nuanced aesthetic quality applications, continue to fall short of practical demands. This situation",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 1,
          "region_type": "text",
          "region_id": "page1_region2"
        },
        "score": 0.544,
        "citation": "2511.18870v2.pdf, Page 1"
      },
      {
        "text": "dels are real-time game engines. In ICLR, 2025. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, 2017. Bram Wallace, Meihua Dang, Rafael Rafailov, Lingi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In CVPR, 2024. Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Duomin Wang, Wei Zuo, Aojie Li, Ling-Hao Chen, Xinyao Liao, Deyu Zhou, Zixin Yin, Xili Dai, Daxin Jiang, and Gang Yu. Universe-1: Unified audio-video generation via stitching of experts. arXiv preprint arXiv:2509.06155, 2025. Qinghe Wang, Yawen Luo, Xiaoyu Shi, Xu Jia, Huchuan Lu, Tianfan Xue, Xintao Wang, Pengfei Wan, Di Zhang, and Kun Gai. Cinemaster: A 3d-aware and controllable framework for cinematic text-to-video generation. In SIGGRAPH, 2025. Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, Jiagang Zhu, and Jiwen Lu. Drivedreamer: Towards real-world- drive world models for autonomous driving. In ECCV, 2024. Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. IJCV, 2025. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022. Yuqing Wen, Yucheng Zhao, Yingfei Liu, Fan Jia, Yanhui Wang, Chong Luo, Chi Zhang, Tiancai Wang, Xiaoyan Sun, and Xiangyu Zhang. Panacea: Panoramic and controllable video generation for autonomous driving. In CVPR, 2024.",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 13,
          "region_type": "list",
          "region_id": "page13_region0"
        },
        "score": 0.505,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 13"
      },
      {
        "text": "images in InternVL3 (Zhu et al., 2025) 10K-frame videos in Video-XL-2 (Qin et al., enabling strong performance on image (Bai 2025), video (Yang et al., 2025b), and multi- reasoning tasks (Shen et al., 2025). More Complex Content in Generation AI content generation has advanced sig- with the application of Transformers to domains (Peebles and Xie, 2023; Brooks 2024; Li et al., 2024f). Early diffusion mod- Stable Diffusion (Rombach et al., 2022) only 512 x 512 resolution images. With now successfully applied to gener- (Peebles and Xie, 2023; Gao et al., 2023; et al., 2024; Li et al., 2024f), DiT-based produce high-quality 4K images in PixArt- et al., 2024a) and hour-long videos in (Brooks et al., 2024). These models capture spatiotemporal dependencies, enabling content generation (Labs, 2024; Yang 2024c; Wan et al., 2025a; Kang et al., 2025). these advancements across domains have outstanding performance, they now et al., 2024b; Zou et al., 2025). (1) Efficient Computation Since computational efficiency architectural design, optimizing tal. Unlike Transformers with complexity O(n?) (Vaswani methods achieve linear or (i) linear attention reformulates complexity (Katharopoulos et 2021); (ii) RWKV combines ing with transformer parallelism Duan et al., 2024); (iii) State Mamba use structured state plexity (Gu and Dao, 2023; quire retraining, motivating (II) Model-centric Compression ing parameter complexity and memory costs. Model centric, transforming W to a W’=T(W), where with I as the compression",
        "source": {
          "paper": "2505.19147v3.pdf",
          "page": 3,
          "region_type": "text",
          "region_id": "page3_region7"
        },
        "score": 0.449,
        "citation": "2505.19147v3.pdf, Page 3"
      },
      {
        "text": "output distribution, enhancing the information expression ability of the encoder.",
        "source": {
          "paper": "2505.07916v1.pdf",
          "page": 6,
          "region_type": "text",
          "region_id": "page6_region3"
        },
        "score": 0.421,
        "citation": "2505.07916v1.pdf, Page 6"
      },
      {
        "text": "producing initial video sequences with resolutions from 480p to 720p and durations of 5 The second stage utilizes a dedicated Video Super-Resolution Network to upscale 1080p, significantly enhancing the final visual fidelity. The main contributions of this summarized as follows:",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 1,
          "region_type": "text",
          "region_id": "page1_region1"
        },
        "score": 0.537,
        "citation": "2511.18870v2.pdf, Page 1"
      },
      {
        "text": "efficient high-resolution video generation. arXiv preprint arXiv:2502.05179, 2025. Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models. arXiv preprint arXiv:2311.04145, 2023. Yifu Zhang, Hao Yang, Yugi Zhang, Yifei Hu, Fengda Zhu, Chuang Lin, Xiaofeng Mei, Yi Jiang, Bingyue Peng, and Zehuan Yuan. Waver: Wave your way to lifelike video generation. arXiv preprint arXiv:2508.15761, 2025. Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and ‘Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. arXiv preprint arXiv:2211.11018, 2022.",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 14,
          "region_type": "list",
          "region_id": "page14_region0"
        },
        "score": 0.501,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 14"
      },
      {
        "text": "higher than Sora2 and Veo3. (ii) Compared to the SOTA DPO algorithm for video generation",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 9,
          "region_type": "text",
          "region_id": "page9_region8"
        },
        "score": 0.501,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 9"
      },
      {
        "text": "arXiv:2501.08453, 2025. Peng Gao, Le Zhuo, Dongyang Liu, Ruoyi Du, Xu Luo, Longtian Qiu, Yuhang Zhang, Chen Lin, Rongjie Huang, Shijie Geng, et al. Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers. arXiv preprint arXiv:2405.05945, 2024. Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Fei-Fei Li, Irfan Essa, Lu Jiang, and José Lezama. Photorealistic video generation with diffusion models. In ECCV, 2024. Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. arXiv preprint arXiv:2211.13221, 2022. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. NeurIPS, 2022. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In JCLR, 2022. Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video generative modeling. In ICLR, 2025. Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video generative modeling. In JCLR, 2025. Xuan Ju, Tianyu Wang, Yugian Zhou, He Zhang, Qing Liu, Nanxuan Zhao, Zhifei Zhang, Yijun Li, Yuanhao Cai, Shaoteng Liu, et al.",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 12,
          "region_type": "list",
          "region_id": "page12_region0"
        },
        "score": 0.499,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 12"
      },
      {
        "text": "videos across four dimensions: sharpness, detail retention, noise & artifacts, and This allows us to discard clips with poor visual quality. Filtering: We applied an aesthetic scoring operator based on to evaluate the of the videos, filtering out those with low aesthetic scores. these filtering stages, approximately 800 million high-quality video segments play a crucial role in video generation, significantly influencing precise",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 3,
          "region_type": "list",
          "region_id": "page3_region2"
        },
        "score": 0.478,
        "citation": "2511.18870v2.pdf, Page 3"
      },
      {
        "text": "TableB]and Table/4] we assess text-to-video generation using a comprehensive rating that considers five key dimensions: text-video consistency, aesthetic quality of individual quality, structural stability, and motion effects. For image-to-video generation, the encompasses image-video consistency, instruction responsiveness, visual quality, structural motion effects.",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 8,
          "region_type": "text",
          "region_id": "page8_region0"
        },
        "score": 0.448,
        "citation": "2511.18870v2.pdf, Page 8"
      }
    ],
    "sources": [
      "MiniMax-Speech: Autoregressive Text-to-Speech Model (Topic: Text-to-Speech (TTS) Generation) - Pages 6",
      "Computer Vision and Image Processing (Topic: Computer Vision) - Pages 3",
      "Hunyuan Foundation Model (Topic: Foundation Models) - Pages 1, 3, 8",
      "Text-to-Video Generation with Physical Consistency (Topic: Video Generation) - Pages 9, 12, 13, 14"
    ],
    "has_evidence": true,
    "retrieval_stats": {
      "total_chunks": 10,
      "papers_searched": [
        "2511.18870v2.pdf",
        "605562216_1287553579874010_8068650067268945856_n.pdf",
        "2505.19147v3.pdf",
        "2505.07916v1.pdf"
      ],
      "by_paper": {
        "2511.18870v2.pdf": 4,
        "605562216_1287553579874010_8068650067268945856_n.pdf": 4,
        "2505.19147v3.pdf": 1,
        "2505.07916v1.pdf": 1
      },
      "by_region_type": {
        "text": 6,
        "list": 4
      }
    },
    "cached_at": "2026-02-11T00:32:22.362255"
  },
  "What resolutions and durations can the video generation models produce?": {
    "question": "What resolutions and durations can the video generation models produce?",
    "answer": "The video generation models can produce video sequences with resolutions from 480p to 720p and durations of 5 seconds. This is explicitly stated in [Evidence 1] from the Hunyuan Foundation Model paper, which mentions that the first stage of the model produces initial video sequences with resolutions from 480p to 720p and durations of 5 seconds.\n\nAdditionally, [Evidence 9] from the Hunyuan Foundation Model paper mentions that the HunyuanVideo 1.5 720P 12V model and its competitors can produce video sequences with a resolution of 720p.\n\nIt's worth noting that [Evidence 2] from the Text-to-Video Generation with Physical Consistency paper mentions a specific resolution of 480x832, but it does not provide information on the duration of the video sequences.\n\n---\n**Evidence:**\n- Evidence 1 (from Foundation Models): The first stage of the model produces initial video sequences with resolutions from 480p to 720p and durations of 5 seconds.\n- Evidence 2 (from Video Generation): A specific resolution of 480x832 is mentioned, but no information on duration is provided.\n- Evidence 9 (from Foundation Models): The HunyuanVideo 1.5 720P 12V model and its competitors can produce video sequences with a resolution of 720p.\n\n**Sources:**\n- Hunyuan Foundation Model (Topic: Foundation Models) - Pages 1, 9, 10, 11, 13\n- Text-to-Video Generation with Physical Consistency (Topic: Video Generation) - Pages 8, 12, 14",
    "evidence": [
      {
        "text": "producing initial video sequences with resolutions from 480p to 720p and durations of 5 The second stage utilizes a dedicated Video Super-Resolution Network to upscale 1080p, significantly enhancing the final visual fidelity. The main contributions of this summarized as follows:",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 1,
          "region_type": "text",
          "region_id": "page1_region1"
        },
        "score": 0.605,
        "citation": "2511.18870v2.pdf, Page 1"
      },
      {
        "text": "480x832. We set tr = 3, N = 100, Qmin = 0.5, ky = 2.0, b, = 0.4, A = 0.6, ka = 5.0, ba = 0.5. The rank of LoRA to 48. We follow the same metrics of VideoPhy2 and PhyGenBench to evaluate the T2V generation models. Comparison with State-of-the-Art Methods",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 8,
          "region_type": "text",
          "region_id": "page8_region5"
        },
        "score": 0.554,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 8"
      },
      {
        "text": "Compared with the similar-sized models, Qwen3-32B-Base outperforms Qwen2.5-32B-Base and Gemma-3-27B Base on most benchmarks. Notably, Qwen3-32B-Base achieves 65.54 on MMLU- Pro and 39.78 on SuperGPQA, significantly outperforming its predecessor Qwen2.5-32B-Base. In addition, Qwen3-32B-Base achieves significantly higher encoding benchmark scores than all",
        "source": {
          "paper": "2505.09388v1.pdf",
          "page": 9,
          "region_type": "text",
          "region_id": "page9_region2"
        },
        "score": 0.425,
        "citation": "2505.09388v1.pdf, Page 9"
      },
      {
        "text": "720p (720 x 1280) 241 v 58.39 1.1679",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 11,
          "region_type": "text",
          "region_id": "page11_region6"
        },
        "score": 0.599,
        "citation": "2511.18870v2.pdf, Page 11"
      },
      {
        "text": "and proficient model establishes a new state-of-the-art among generation models. By releasing the code and model weights, community with a high-performance foundation that lowers the creation and research, making advanced video generation ac- broader audience. All open-source assets are publicly available at //github.com/Tencent-Hunyuan/HunyuanVideo-1.5 remarkable advancements in video generation models, with closed- Kling2.5 [I], Veo3.1 [2], and Sora2 achieving state-of-the-art (SOTA) open-source domain, models such as HunyuanVideo [4], StepVideo [5], and as notable contenders. However, despite these strides, most SOTA models accessibility and community-driven innovation. Wan2.2 [6] employs a (MoE) architecture that leverages two 14-billion-parameter expert models Although this MoE architecture enhances visual fidelity by leveraging different denoising stages, it inherently introduces computational inefficiencies, multiple large parameter sets (totaling 27B parameters with 14B activated), demands. While Wan2.2 [6] has attempted to address this with a more variant, which utilizes a high-compression 3D VAE to reduce memory model still exhibits limitations. Specifically, its generative capabilities, motion stability across frames and achieving the nuanced aesthetic quality applications, continue to fall short of practical demands. This situation",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 1,
          "region_type": "text",
          "region_id": "page1_region2"
        },
        "score": 0.575,
        "citation": "2511.18870v2.pdf, Page 1"
      },
      {
        "text": "Sora 2. https: 2025. Hunyuan Foundation Model Team. Hunyuanvideo: A systematic framework for large generative models, 2025. URL\\https://arxiv.org/abs/2412. 03603) Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Ranchen Ming, Xiaoniu Song, Xing Chen, Yu Zhou, and Deshan Sun et al. Step-video-t2v report: The practice, challenges, and future of video foundation model, 2025. URL //arxiv.org/abs/2502.10248 Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, and Jingfeng Zhang et al. Wan: and advanced large-scale video generative models. arXiv preprint arXiv:2503.203 14, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. Flashattention-3: Fast and accurate attention with asynchrony and low-precision, 2024. URL //arxiv.org/abs/2407 .08608 Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Chen, Yuankun Chen, Yutian Chen, Zhuofu Chen, and Jialei Cui et al. Kimi k2: Open intelligence, 2025. URL https: //arxiv.org/abs/2507. 20534| Hangting Chen, Peng Chen, Yiji Cheng, Yutao Cui, Xinchi Deng, Ying Dong, Kipper Tianpeng Gu, Xiusen Gu, et al. Hunyuanimage 3.0 technical report. arXiv preprint arXiv:2509.23951, 2025. Contributors. Pyscenedetect: A python library for video scene detection, 2020. https://github. com/Breakthrough/PySceneDetect| Accessed: 2023-11-20. Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Yan, and Weisi Lin. Exploring video quality assessment on user generated contents from and technical perspectives. In Proceedings of the IEEE/CVF International Conference Computer Vision, pages 20144-20154, 2023. Yang, Xufang Luo, Dongqi Han, Yunjian Xu, and Dongsheng Li. Mitigating hallucina- large vision-language models via dpo: On-policy data hold the key. In Proceedings of Computer Vision and Pattern Recognition Conference, pages 10610-10620, 2025.",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 13,
          "region_type": "list",
          "region_id": "page13_region0"
        },
        "score": 0.515,
        "citation": "2511.18870v2.pdf, Page 13"
      },
      {
        "text": "arXiv:2501.08453, 2025. Peng Gao, Le Zhuo, Dongyang Liu, Ruoyi Du, Xu Luo, Longtian Qiu, Yuhang Zhang, Chen Lin, Rongjie Huang, Shijie Geng, et al. Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers. arXiv preprint arXiv:2405.05945, 2024. Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Fei-Fei Li, Irfan Essa, Lu Jiang, and José Lezama. Photorealistic video generation with diffusion models. In ECCV, 2024. Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. arXiv preprint arXiv:2211.13221, 2022. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. NeurIPS, 2022. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In JCLR, 2022. Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video generative modeling. In ICLR, 2025. Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video generative modeling. In JCLR, 2025. Xuan Ju, Tianyu Wang, Yugian Zhou, He Zhang, Qing Liu, Nanxuan Zhao, Zhifei Zhang, Yijun Li, Yuanhao Cai, Shaoteng Liu, et al.",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 12,
          "region_type": "list",
          "region_id": "page12_region0"
        },
        "score": 0.5,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 12"
      },
      {
        "text": "efficient high-resolution video generation. arXiv preprint arXiv:2502.05179, 2025. Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models. arXiv preprint arXiv:2311.04145, 2023. Yifu Zhang, Hao Yang, Yugi Zhang, Yifei Hu, Fengda Zhu, Chuang Lin, Xiaofeng Mei, Yi Jiang, Bingyue Peng, and Zehuan Yuan. Waver: Wave your way to lifelike video generation. arXiv preprint arXiv:2508.15761, 2025. Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and ‘Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. arXiv preprint arXiv:2211.11018, 2022.",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 14,
          "region_type": "list",
          "region_id": "page14_region0"
        },
        "score": 0.477,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 14"
      },
      {
        "text": "HunyuanVideo 1.5 720P 12V model and its competitors",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 10,
          "region_type": "text",
          "region_id": "page10_region9"
        },
        "score": 0.456,
        "citation": "2511.18870v2.pdf, Page 10"
      },
      {
        "text": "parameter count while fully leveraging the potential of the model, yielding comparable to state-of-the-art video generation models. Super-Resolution Enhancement: We develop an efficient few-step super-resolution upscales outputs to 1080p. It enhances sharpness while correcting distortions, details and overall visual texture. Optimization: We introduce a novel Selective and Sliding Tile Attention mechanism that dynamically prunes redundant spatiotemporal tokens. This signifi- computational overhead for long video sequences and accelerates inference, end-to-end speedup of 1.87x in 10-second 720p video synthesis compared to [7]. Multimodal Understanding: Our framework utilizes a large multimodal model bilingual (Chinese-English) understanding, combined with ByT5 for dedicated to enhance text generation accuracy in videos. Additionally, detailed captions are generated for both images and videos. Training Optimization: We demonstrate that the Muon optimizer [8] signifi- convergence in video generation model training, while the multi-phase training strategy—spanning from pre-training to post-training stages—enhances aesthetic quality, and alignment with human preferences, thereby enabling of professional-grade content. comprehensive pipeline, beginning with meticulous data preparation—including captioning, as detailed in Section|2| Section [3]introduces the core architecture such as the unified diffusion transformer, the video super-resolution network, sparse attention. Section |4|describes training strategies for text-to-video and covering both pre-training and post-training methodologies. Finally, Section[5| evaluation of the model’s performance against state-of-the-art models using qualitative benchmarks. and Filtering comprises both image and video data. For image data, we adopted the acquisition outlined in [9], curating 5 billion images from a pool of over 10 billion for",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 2,
          "region_type": "list",
          "region_id": "page2_region4"
        },
        "score": 0.449,
        "citation": "2511.18870v2.pdf, Page 2"
      }
    ],
    "sources": [
      "Research Paper on Machine Learning (Topic: Machine Learning) - Pages 9",
      "Hunyuan Foundation Model (Topic: Foundation Models) - Pages 1, 2, 10, 11, 13",
      "Text-to-Video Generation with Physical Consistency (Topic: Video Generation) - Pages 8, 12, 14"
    ],
    "has_evidence": true,
    "retrieval_stats": {
      "total_chunks": 10,
      "papers_searched": [
        "2511.18870v2.pdf",
        "605562216_1287553579874010_8068650067268945856_n.pdf",
        "2505.09388v1.pdf"
      ],
      "by_paper": {
        "2511.18870v2.pdf": 6,
        "605562216_1287553579874010_8068650067268945856_n.pdf": 3,
        "2505.09388v1.pdf": 1
      },
      "by_region_type": {
        "text": 6,
        "list": 4
      }
    },
    "cached_at": "2026-02-11T00:32:28.149186"
  },
  "How is video super-resolution implemented in these models?": {
    "question": "How is video super-resolution implemented in these models?",
    "answer": "The implementation of video super-resolution in these models is a crucial aspect of enhancing the visual fidelity of generated videos. According to [Evidence 1], the Hunyuan Foundation Model utilizes a dedicated Video Super-Resolution Network to upscale 1080p, significantly enhancing the final visual fidelity. This suggests that the model employs a specialized network architecture designed specifically for video super-resolution.\n\nHowever, the specific details of the video super-resolution implementation are not explicitly stated in the provided evidence. Nevertheless, we can infer that the model uses a combination of techniques to achieve high-quality video super-resolution. For instance, [Evidence 8] mentions that the Hunyuan Foundation Model achieves a PSNR score of 58.39 on a 720p video, indicating that the model is capable of producing high-quality video super-resolution.\n\nIt's worth noting that the implementation of video super-resolution may vary across different models and architectures. For example, [Evidence 10] lists several papers that explore efficient high-resolution video generation, including I2VGen-XL, which uses cascaded diffusion models for image-to-video synthesis. However, the specific details of the video super-resolution implementation in these models are not provided in the evidence.\n\nIn summary, while the evidence suggests that the Hunyuan Foundation Model employs a dedicated Video Super-Resolution Network for video super-resolution, the specific details of the implementation are not explicitly stated. Further research would be necessary to fully understand the video super-resolution techniques used in these models.\n\n---\n**Evidence:**\n- Evidence 1 (from Foundation Models): Describes the implementation of a dedicated Video Super-Resolution Network in the Hunyuan Foundation Model to upscale 1080p and enhance visual fidelity.\n- Evidence 8 (from Foundation Models): Mentions that the Hunyuan Foundation Model achieves a PSNR score of 58.39 on a 720p video, indicating high-quality video super-resolution capabilities.\n- Evidence 10 (from Video Generation): Lists several papers that explore efficient high-resolution video generation, including I2VGen-XL, which uses cascaded diffusion models for image-to-video synthesis.\n\n**Sources:**\n- \"Hunyuan Foundation Model\" (Topic: Foundation Models) - Pages 1, 8, 11, 13\n- \"Text-to-Video Generation with Physical Consistency\" (Topic: Video Generation) - Pages 8, 14\n- \"Computer Vision and Image Processing\" (Topic: Computer Vision) - Page 10\n- \"Research Paper on Machine Learning\" (Topic: Machine Learning) - Page 9\n- \"MiniMax-Speech: Autoregressive Text-to-Speech Model\" (Topic: Text-to-Speech (TTS) Generation) - Page 6\n- \"Hunyuan Foundation Model\" (Topic: Foundation Models) - Pages 6, 8, 11, 13\n- \"Text-to-Video Generation with Physical Consistency\" (Topic: Video Generation) - Page 14\n- \"Computer Vision and Image Processing\" (Topic: Computer Vision) - Page 10\n- \"Research Paper on Machine Learning\" (Topic: Machine Learning) - Page 9\n- \"MiniMax-Speech: Autoregressive Text-to-Speech Model\" (Topic: Text-to-Speech (TTS) Generation) - Page 6",
    "evidence": [
      {
        "text": "producing initial video sequences with resolutions from 480p to 720p and durations of 5 The second stage utilizes a dedicated Video Super-Resolution Network to upscale 1080p, significantly enhancing the final visual fidelity. The main contributions of this summarized as follows:",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 1,
          "region_type": "text",
          "region_id": "page1_region1"
        },
        "score": 0.724,
        "citation": "2511.18870v2.pdf, Page 1"
      },
      {
        "text": "480x832. We set tr = 3, N = 100, Qmin = 0.5, ky = 2.0, b, = 0.4, A = 0.6, ka = 5.0, ba = 0.5. The rank of LoRA to 48. We follow the same metrics of VideoPhy2 and PhyGenBench to evaluate the T2V generation models. Comparison with State-of-the-Art Methods",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 8,
          "region_type": "text",
          "region_id": "page8_region5"
        },
        "score": 0.415,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 8"
      },
      {
        "text": "Scardapane, and and effective compression. on Empiri- Processing, pages Association for Zang, Yuhang Songyang Zhang, Yining Li, and 1 A pioneer- handling resolutions in Neural Infor- 37:42566-42592. Hongyuan Zhu, of embodied ai: [EEE Transac- Computational Intelli- Chen, Xizhou Zhu, Hongsheng Li, Jifeng Vision-rwkv: Effi- with rwkv-like arXiv:2403.02308. Hoefler, and post-training transformers. Conference on Luo, Lei Li, Wang, Chenyu Zhang, and | oth- comprehensive Ilms in video Zhongmin Wang, driving secu- IEEE Internet of Laurence Gold- Jason Phang, Nabeshima, and 1 dataset of di- arXiv preprint Cheng, and diffusion transformer Proceedings of the Vision and Pat- Minjia Zhang, Model tells you",
        "source": {
          "paper": "2505.19147v3.pdf",
          "page": 10,
          "region_type": "list",
          "region_id": "page10_region0"
        },
        "score": 0.403,
        "citation": "2505.19147v3.pdf, Page 10"
      },
      {
        "text": "Compared with the similar-sized models, Qwen3-32B-Base outperforms Qwen2.5-32B-Base and Gemma-3-27B Base on most benchmarks. Notably, Qwen3-32B-Base achieves 65.54 on MMLU- Pro and 39.78 on SuperGPQA, significantly outperforming its predecessor Qwen2.5-32B-Base. In addition, Qwen3-32B-Base achieves significantly higher encoding benchmark scores than all",
        "source": {
          "paper": "2505.09388v1.pdf",
          "page": 9,
          "region_type": "text",
          "region_id": "page9_region2"
        },
        "score": 0.392,
        "citation": "2505.09388v1.pdf, Page 9"
      },
      {
        "text": "output distribution, enhancing the information expression ability of the encoder.",
        "source": {
          "paper": "2505.07916v1.pdf",
          "page": 6,
          "region_type": "text",
          "region_id": "page6_region3"
        },
        "score": 0.385,
        "citation": "2505.07916v1.pdf, Page 6"
      },
      {
        "text": "and proficient model establishes a new state-of-the-art among generation models. By releasing the code and model weights, community with a high-performance foundation that lowers the creation and research, making advanced video generation ac- broader audience. All open-source assets are publicly available at //github.com/Tencent-Hunyuan/HunyuanVideo-1.5 remarkable advancements in video generation models, with closed- Kling2.5 [I], Veo3.1 [2], and Sora2 achieving state-of-the-art (SOTA) open-source domain, models such as HunyuanVideo [4], StepVideo [5], and as notable contenders. However, despite these strides, most SOTA models accessibility and community-driven innovation. Wan2.2 [6] employs a (MoE) architecture that leverages two 14-billion-parameter expert models Although this MoE architecture enhances visual fidelity by leveraging different denoising stages, it inherently introduces computational inefficiencies, multiple large parameter sets (totaling 27B parameters with 14B activated), demands. While Wan2.2 [6] has attempted to address this with a more variant, which utilizes a high-compression 3D VAE to reduce memory model still exhibits limitations. Specifically, its generative capabilities, motion stability across frames and achieving the nuanced aesthetic quality applications, continue to fall short of practical demands. This situation",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 1,
          "region_type": "text",
          "region_id": "page1_region2"
        },
        "score": 0.463,
        "citation": "2511.18870v2.pdf, Page 1"
      },
      {
        "text": "on overall video perception quality.",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 8,
          "region_type": "title",
          "region_id": "page8_region11"
        },
        "score": 0.443,
        "citation": "2511.18870v2.pdf, Page 8"
      },
      {
        "text": "720p (720 x 1280) 241 v 58.39 1.1679",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 11,
          "region_type": "text",
          "region_id": "page11_region6"
        },
        "score": 0.441,
        "citation": "2511.18870v2.pdf, Page 11"
      },
      {
        "text": "Sora 2. https: 2025. Hunyuan Foundation Model Team. Hunyuanvideo: A systematic framework for large generative models, 2025. URL\\https://arxiv.org/abs/2412. 03603) Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Ranchen Ming, Xiaoniu Song, Xing Chen, Yu Zhou, and Deshan Sun et al. Step-video-t2v report: The practice, challenges, and future of video foundation model, 2025. URL //arxiv.org/abs/2502.10248 Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, and Jingfeng Zhang et al. Wan: and advanced large-scale video generative models. arXiv preprint arXiv:2503.203 14, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. Flashattention-3: Fast and accurate attention with asynchrony and low-precision, 2024. URL //arxiv.org/abs/2407 .08608 Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Chen, Yuankun Chen, Yutian Chen, Zhuofu Chen, and Jialei Cui et al. Kimi k2: Open intelligence, 2025. URL https: //arxiv.org/abs/2507. 20534| Hangting Chen, Peng Chen, Yiji Cheng, Yutao Cui, Xinchi Deng, Ying Dong, Kipper Tianpeng Gu, Xiusen Gu, et al. Hunyuanimage 3.0 technical report. arXiv preprint arXiv:2509.23951, 2025. Contributors. Pyscenedetect: A python library for video scene detection, 2020. https://github. com/Breakthrough/PySceneDetect| Accessed: 2023-11-20. Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Yan, and Weisi Lin. Exploring video quality assessment on user generated contents from and technical perspectives. In Proceedings of the IEEE/CVF International Conference Computer Vision, pages 20144-20154, 2023. Yang, Xufang Luo, Dongqi Han, Yunjian Xu, and Dongsheng Li. Mitigating hallucina- large vision-language models via dpo: On-policy data hold the key. In Proceedings of Computer Vision and Pattern Recognition Conference, pages 10610-10620, 2025.",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 13,
          "region_type": "list",
          "region_id": "page13_region0"
        },
        "score": 0.406,
        "citation": "2511.18870v2.pdf, Page 13"
      },
      {
        "text": "efficient high-resolution video generation. arXiv preprint arXiv:2502.05179, 2025. Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models. arXiv preprint arXiv:2311.04145, 2023. Yifu Zhang, Hao Yang, Yugi Zhang, Yifei Hu, Fengda Zhu, Chuang Lin, Xiaofeng Mei, Yi Jiang, Bingyue Peng, and Zehuan Yuan. Waver: Wave your way to lifelike video generation. arXiv preprint arXiv:2508.15761, 2025. Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and ‘Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. arXiv preprint arXiv:2211.11018, 2022.",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 14,
          "region_type": "list",
          "region_id": "page14_region0"
        },
        "score": 0.396,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 14"
      }
    ],
    "sources": [
      "MiniMax-Speech: Autoregressive Text-to-Speech Model (Topic: Text-to-Speech (TTS) Generation) - Pages 6",
      "Research Paper on Machine Learning (Topic: Machine Learning) - Pages 9",
      "Computer Vision and Image Processing (Topic: Computer Vision) - Pages 10",
      "Hunyuan Foundation Model (Topic: Foundation Models) - Pages 1, 8, 11, 13",
      "Text-to-Video Generation with Physical Consistency (Topic: Video Generation) - Pages 8, 14"
    ],
    "has_evidence": true,
    "retrieval_stats": {
      "total_chunks": 10,
      "papers_searched": [
        "2511.18870v2.pdf",
        "605562216_1287553579874010_8068650067268945856_n.pdf",
        "2505.19147v3.pdf",
        "2505.09388v1.pdf",
        "2505.07916v1.pdf"
      ],
      "by_paper": {
        "2511.18870v2.pdf": 5,
        "605562216_1287553579874010_8068650067268945856_n.pdf": 2,
        "2505.19147v3.pdf": 1,
        "2505.09388v1.pdf": 1,
        "2505.07916v1.pdf": 1
      },
      "by_region_type": {
        "text": 6,
        "list": 3,
        "title": 1
      }
    },
    "cached_at": "2026-02-11T00:32:39.806489"
  },
  "What training procedures and optimization strategies are used?": {
    "question": "What training procedures and optimization strategies are used?",
    "answer": "The training procedures and optimization strategies used in the papers provided are quite diverse, but I'll do my best to summarize them based on the evidence.\n\nAccording to the papers, several training procedures and optimization strategies are employed. For instance, [Evidence 1] from the Machine Learning paper mentions that the pretraining data is used, but it doesn't specify the exact procedures. However, [Evidence 2] from the Natural Language Processing Study mentions that pretraining is used, which suggests that some form of pretraining is a common practice.\n\nIn the Deep Learning Architectures paper, [Evidence 3] mentions that the rollout captures used by the trainer is responsible for updating the model weights, which implies that some form of reinforcement learning (RL) is used. Additionally, [Evidence 8] from the same paper highlights that aligning training with the agent's execution logic directly enhances agent performance in real-world applications.\n\nMoreover, [Evidence 9] from the Deep Learning Architectures paper introduces the concept of Component of Interest (Col) to support optimization, which is a principle way for agent optimization, limiting to RL-based methods. This suggests that RL-based methods are used for optimization.\n\nFurthermore, [Evidence 10] from the same paper mentions that the Agent Lightning client provides Intermediate Rewarding (AIR) mechanisms to convert system monitoring intermediate rewards, enabling the training framework to leverage these signals for more learning. This implies that AIR is used as an optimization strategy.\n\nLastly, [Evidence 7] from the AI Research and Development paper mentions that souping of the final post-trained checkpoints is tested, which suggests that souping is a training procedure used in some cases.\n\nIn terms of specific optimization strategies, [Evidence 6] from the Multi-Modal AI Research paper mentions that SimpleRL-Zoo and help isolate the impact of initialization, ensuring performance gains arise from alignment rather than optimization differences. This suggests that SimpleRL-Zoo is used as an optimization strategy.\n\nOverall, the training procedures and optimization strategies used in these papers include pretraining, RL, souping, AIR, and SimpleRL-Zoo.\n\n---\n**Evidence:**\n- Evidence 1 (from Machine Learning): Describes the use of pretraining data.\n- Evidence 2 (from Natural Language Processing Study): Mentions the use of pretraining.\n- Evidence 3 (from Deep Learning Architectures): Implies the use of reinforcement learning (RL) for updating model weights.\n- Evidence 8 (from Deep Learning Architectures): Highlights the importance of aligning training with the agent's execution logic.\n- Evidence 9 (from Deep Learning Architectures): Introduces the concept of Component of Interest (Col) for optimization.\n- Evidence 10 (from Deep Learning Architectures): Describes the use of Intermediate Rewarding (AIR) mechanisms.\n- Evidence 7 (from AI Research and Development): Mentions the use of souping of the final post-trained checkpoints.\n- Evidence 6 (from Multi-Modal AI Research): Describes the use of SimpleRL-Zoo for optimization.\n\n**Sources:**\n- 2505.09388v1.pdf (Machine Learning) - Page 3\n- 2505.11820v2.pdf (Natural Language Processing Study) - Page 14\n- 2508.03680v1.pdf (Deep Learning Architectures) - Pages 3, 10, 15\n- 2511.18870v2.pdf (Hunyuan Foundation Model) - Page 6\n- 2505.19147v3.pdf (Computer Vision and Image Processing) - Page 4\n- 2505.10554v2.pdf (Multi-Modal AI Research) - Page 5\n- 583127822_872908961738101_8157213289131333559_n.pdf (AI Research and Development) - Page 10",
    "evidence": [
      {
        "text": "of our pretraining data, the details of our pretraining",
        "source": {
          "paper": "2505.09388v1.pdf",
          "page": 3,
          "region_type": "text",
          "region_id": "page3_region12"
        },
        "score": 0.558,
        "citation": "2505.09388v1.pdf, Page 3"
      },
      {
        "text": "used in pretraining.",
        "source": {
          "paper": "2505.11820v2.pdf",
          "page": 14,
          "region_type": "text",
          "region_id": "page14_region7"
        },
        "score": 0.491,
        "citation": "2505.11820v2.pdf, Page 14"
      },
      {
        "text": "is responsible for updating the model weights, while the rollout captures used by the trainer. For single-turn RL tasks, such as mathematical reasoning, the",
        "source": {
          "paper": "2508.03680v1.pdf",
          "page": 10,
          "region_type": "title",
          "region_id": "page10_region7"
        },
        "score": 0.462,
        "citation": "2508.03680v1.pdf, Page 10"
      },
      {
        "text": "of subsequent T2V & I2V training. the",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 6,
          "region_type": "title",
          "region_id": "page6_region6"
        },
        "score": 0.452,
        "citation": "2511.18870v2.pdf, Page 6"
      },
      {
        "text": "param- models, these cri- main approaches: auxiliary net- X + {s:}7_), A@ beyond the methods in- (Rao et al., 2024d; Kim et al., training to learn {s,}_,, and (ii) et al., 2024; employ pre-trained : KX + utilize parameter- without introduc- approaches can be computation meth- al., 2025; Chen Ge et al., 2024) calculations for to- such as using where a; repre- and (ii) ex- et al., 2023a;",
        "source": {
          "paper": "2505.19147v3.pdf",
          "page": 4,
          "region_type": "text",
          "region_id": "page4_region9"
        },
        "score": 0.435,
        "citation": "2505.19147v3.pdf, Page 4"
      },
      {
        "text": "objective in place of more complex objectives such as REINFORCE++. These choices SimpleRL-Zoo and help isolate the impact of initialization, ensuring performance gains arise alignment rather than optimization differences.",
        "source": {
          "paper": "2505.10554v2.pdf",
          "page": 5,
          "region_type": "text",
          "region_id": "page5_region6"
        },
        "score": 0.401,
        "citation": "2505.10554v2.pdf, Page 5"
      },
      {
        "text": "an development leaking any in the model training practice: In this work, we have only tested the souping of the ’ final’ posttrained checkpoints. As other works have shown, the models can be souped after pre-training Li et al. (2025), after Wortsman et al. (2022), with adapters’. We do not recommend souping of the checkpoints from different as well as souping unaligned or uncensored models with the aligned ones to avoid the inheritance of",
        "source": {
          "paper": "583127822_872908961738101_8157213289131333559_n.pdf",
          "page": 10,
          "region_type": "text",
          "region_id": "page10_region3"
        },
        "score": 0.388,
        "citation": "583127822_872908961738101_8157213289131333559_n.pdf, Page 10"
      },
      {
        "text": "of implementation approach, with almost ZERO code modifications. By aligning training with the agent’s execution logic, it directly enhances agent performance in real-world applications.",
        "source": {
          "paper": "2508.03680v1.pdf",
          "page": 3,
          "region_type": "text",
          "region_id": "page3_region7"
        },
        "score": 0.443,
        "citation": "2508.03680v1.pdf, Page 3"
      },
      {
        "text": "AI agent, bridging AI agents and RL training methods. Future Work Optimization Methods Besides RL, our modeling framework well supports other optimization as well, such as automatic prompt optimization mentioned in Section 3.1.3. More generally focusing on the key components and its invocations is a principle way for agent optimization, limiting to RL-based methods. Therefore, we introduce the concept of Component of Interest (Col) to the subset of components in the execution trace that are subject to optimization. For example, template rendering can be treated as a tool call, and by treating this tool as a Col, Agent Light- facilitate prompt optimization methods. This unified and extensible data structure supports comprehensive downstream optimization and analysis of agent behaviors. Extending support to more methods is of future",
        "source": {
          "paper": "2508.03680v1.pdf",
          "page": 15,
          "region_type": "text",
          "region_id": "page15_region5"
        },
        "score": 0.417,
        "citation": "2508.03680v1.pdf, Page 15"
      },
      {
        "text": "not properly handled by the agent code, such as crashes or long-hanging tool calls, and ensures failures do not disrupt the overall training process. Failed tasks can be retried or reassigned to agent instances, and detailed logs are maintained for debugging and monitoring purposes. This minimizes downtime and maximizes training efficiency, allowing for continuous improvement of performance. Intermediate Rewarding (AIR) The delayed and sparse nature of rewards in RL training the learning process. Intermediate rewards, which provide feedback at various stages of execution, can significantly enhance the training process by offering more frequent and informative But it often requires high overhead such as costly human annotation or complex reward logic. How to mine intermediate rewards from the agent execution is of great importance to process. The Agent Lightning client provides AIR mechanisms to convert system monitoring intermediate rewards, enabling the training framework to leverage these signals for more learning. and Reward Services for Scalability Environments and reward functions are critical of RL training. For those environments and reward functions that are light-weight and locally, they can be executed directly within the same worker with the agent instance. But for",
        "source": {
          "paper": "2508.03680v1.pdf",
          "page": 11,
          "region_type": "text",
          "region_id": "page11_region0"
        },
        "score": 0.406,
        "citation": "2508.03680v1.pdf, Page 11"
      }
    ],
    "sources": [
      "Research Paper on Machine Learning (Topic: Machine Learning) - Pages 3",
      "Multi-Modal AI Research (Topic: Multi-Modal Learning) - Pages 5",
      "Natural Language Processing Study (Topic: Natural Language Processing) - Pages 14",
      "Computer Vision and Image Processing (Topic: Computer Vision) - Pages 4",
      "Deep Learning Architectures (Topic: Deep Learning) - Pages 3, 10, 11, 15",
      "Hunyuan Foundation Model (Topic: Foundation Models) - Pages 6",
      "AI Research and Development (Topic: AI Research) - Pages 10"
    ],
    "has_evidence": true,
    "retrieval_stats": {
      "total_chunks": 10,
      "papers_searched": [
        "2505.09388v1.pdf",
        "2505.11820v2.pdf",
        "2508.03680v1.pdf",
        "2511.18870v2.pdf",
        "2505.19147v3.pdf",
        "2505.10554v2.pdf",
        "583127822_872908961738101_8157213289131333559_n.pdf"
      ],
      "by_paper": {
        "2505.09388v1.pdf": 1,
        "2505.11820v2.pdf": 1,
        "2508.03680v1.pdf": 4,
        "2511.18870v2.pdf": 1,
        "2505.19147v3.pdf": 1,
        "2505.10554v2.pdf": 1,
        "583127822_872908961738101_8157213289131333559_n.pdf": 1
      },
      "by_region_type": {
        "text": 8,
        "title": 2
      }
    },
    "cached_at": "2026-02-11T00:32:52.507526"
  },
  "How does the Muon optimizer compare to AdamW?": {
    "question": "How does the Muon optimizer compare to AdamW?",
    "answer": "The Muon optimizer is compared to AdamW in the Hunyuan Foundation Model paper, where it is stated that the Muon optimizer achieves faster convergence and lower training loss than AdamW in only half the number of training steps [Evidence 1]. This suggests that the Muon optimizer is more efficient than AdamW in terms of training speed and loss reduction.\n\n---\n**Evidence:**\n- Evidence 1 (from Foundation Models): The Muon optimizer achieves faster convergence and lower training loss than AdamW in only half the number of training steps.\n- Evidence 2 (from Machine Learning): This paper does not provide information on the comparison between Muon optimizer and AdamW.\n- Evidence 3 (from Computer Vision and Image Processing): This paper does not provide information on the comparison between Muon optimizer and AdamW.\n- Evidence 4 (from AI Research and Development): This paper does not provide information on the comparison between Muon optimizer and AdamW.\n- Evidence 5 (from Multi-Modal AI Research): This paper does not provide information on the comparison between Muon optimizer and AdamW.\n- Evidence 6 (from Machine Learning): This paper does not provide information on the comparison between Muon optimizer and AdamW.\n- Evidence 7 (from Machine Learning): This paper does not provide information on the comparison between Muon optimizer and AdamW.\n- Evidence 8 (from Machine Learning): This paper does not provide information on the comparison between Muon optimizer and AdamW.\n- Evidence 9 (from Machine Learning): This paper does not provide information on the comparison between Muon optimizer and AdamW.\n- Evidence 10 (from AI Research and Development): This paper does not provide information on the comparison between Muon optimizer and AdamW.\n\n**Sources:**\n- \"Hunyuan Foundation Model\" (Topic: Foundation Models) - Page 5\n- \"Research Paper on Machine Learning\" (Topic: Machine Learning) - Pages 15, 9, 23, 9, 15\n- \"Computer Vision and Image Processing\" (Topic: Computer Vision) - Page 18\n- \"AI Research and Development\" (Topic: AI Research) - Pages 6, 14\n- \"Multi-Modal AI Research\" (Topic: Multi-Modal Learning) - Page 5",
    "evidence": [
      {
        "text": "The Muon optimizer [8] is used in this work to achieve faster convergence. We observe a lower training loss than AdamW in only half the number of training steps, while also performance across multiple text-to-image benchmarks. To ensure training stability, of 0.01 is applied.",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 5,
          "region_type": "text",
          "region_id": "page5_region1"
        },
        "score": 0.529,
        "citation": "2511.18870v2.pdf, Page 5"
      },
      {
        "text": "Our flagship model, Qwen3-235B-A22B, demonstrates the state-of-the-art overall performance among open-source models in both the thinking and non-thinking modes, surpassing strong baselines such as DeepSeek-R1 and DeepSeek-V3. Qwen3-235B-A22B is also highly competitive to closed-source models, such Gemini2.5-Pro, and GPT-40, its",
        "source": {
          "paper": "2505.09388v1.pdf",
          "page": 15,
          "region_type": "text",
          "region_id": "page15_region7"
        },
        "score": 0.408,
        "citation": "2505.09388v1.pdf, Page 15"
      },
      {
        "text": "quantization (Rokh et al., 2023; Wang et al., Zhou et al., 2018; Yang et al., 2019; Lin 2024a; Frantar et al., 2022), and knowledge (Hinton et al., 2015; Zhang et al., 2019, Park et al., 2019), have long been estab- as effective techniques for reducing model computational cost. Proponents argue paradigm is reliable for deployment in 2021) utilize a window-based nism, which reduces the of the standard attention sible to scale models to much sequences. Proponents of this these advanced models continue may be less need for aggressive as these models are inherently",
        "source": {
          "paper": "2505.19147v3.pdf",
          "page": 18,
          "region_type": "text",
          "region_id": "page18_region5"
        },
        "score": 0.379,
        "citation": "2505.19147v3.pdf, Page 18"
      },
      {
        "text": "weight having parameters souping: MetaMathOctopus-7B (Chen MetaMathOctopus-MAPO-DPO-7B (She et al., 2024), MathOctopus-MAPO-DPO-7B (She et al., 2024), and Mathoctopus-Parallel-7B (She et al., 2024). We present the results for the uniform souping (Wortsman et al., 2022), souping with SoCE candidate selection, and SoCE in Table 2a. Uniform souping, considering all candidates, performance regression compared to the best candidate models. MGSM Model Accuracy MetaMathOctopus-7B 41.9% (Chen et al., 2023) Model ooBench MetaMathOctopus-MAPO- 50.9% Accuracy",
        "source": {
          "paper": "583127822_872908961738101_8157213289131333559_n.pdf",
          "page": 6,
          "region_type": "text",
          "region_id": "page6_region2"
        },
        "score": 0.373,
        "citation": "583127822_872908961738101_8157213289131333559_n.pdf, Page 6"
      },
      {
        "text": "objective in place of more complex objectives such as REINFORCE++. These choices SimpleRL-Zoo and help isolate the impact of initialization, ensuring performance gains arise alignment rather than optimization differences.",
        "source": {
          "paper": "2505.10554v2.pdf",
          "page": 5,
          "region_type": "text",
          "region_id": "page5_region6"
        },
        "score": 0.352,
        "citation": "2505.10554v2.pdf, Page 5"
      },
      {
        "text": "Surprisingly, we find that Qwen3-32B-Base achieves competitive results compared to Qwen2.5- 72B-Base. Although Qwen3-32B-Base has less than half the number of parameters of Qwen2.5- 72B-Base, it outperforms Qwen2.5-72B-Base in 10 of the 15 evaluation benchmarks. On coding, mathematics, and reasoning benchmarks, Qwen3-32B-Base has remarkable advantages. Compared to Llama-4-Scout-Base, Qwen3-32B-Base significantly outperforms it on all 15 bench- with only one-third of the number of parameters of Llama-4-Scout-Base, but twice the of activated parameters. & Qwen3-30B-A3B-Base The evaluation of the Qwen3-14B-Base and Qwen3-30B- compared against baselines of similar sizes, including Gemma-3-12B Base, Qwen2.5-14B we also introduce two strong baselines: (1) Qwen2.5-Turbo (Yang et al., 2024b), which parameters and 6B activated parameters. Note that its activated parameters are twice those of (2) Qwen2.5-32B-Base, which has 11 times the activated parameters of Qwen3- more than twice that of Qwen3-14B. The results are shown in Table 5, where we can draw conclusions. Compared with the similar-sized models, Qwen3-14B-Base significantly performs better than Qwen?.5-14B-Base and Gemma-3-12B-Base on all 15 benchmarks.",
        "source": {
          "paper": "2505.09388v1.pdf",
          "page": 9,
          "region_type": "list",
          "region_id": "page9_region3"
        },
        "score": 0.391,
        "citation": "2505.09388v1.pdf, Page 9"
      },
      {
        "text": "Table 23: Performance of Qwen3 Models on",
        "source": {
          "paper": "2505.09388v1.pdf",
          "page": 23,
          "region_type": "title",
          "region_id": "page23_region5"
        },
        "score": 0.39,
        "citation": "2505.09388v1.pdf, Page 23"
      },
      {
        "text": "Compared with the similar-sized models, Qwen3-32B-Base outperforms Qwen2.5-32B-Base and Gemma-3-27B Base on most benchmarks. Notably, Qwen3-32B-Base achieves 65.54 on MMLU- Pro and 39.78 on SuperGPQA, significantly outperforming its predecessor Qwen2.5-32B-Base. In addition, Qwen3-32B-Base achieves significantly higher encoding benchmark scores than all",
        "source": {
          "paper": "2505.09388v1.pdf",
          "page": 9,
          "region_type": "text",
          "region_id": "page9_region2"
        },
        "score": 0.38,
        "citation": "2505.09388v1.pdf, Page 9"
      },
      {
        "text": "close-source models. From Table 12, Qwen3-235B-A22B (Non-thinking) exceeds the other leading open-source models, including DeepSeek-V3, LLaMA-4-Maverick, and our previous flagship model Qwen2.5-72B- Instruct, and also surpasses the closed-source GPT-40-2024-11-20 in 18/23 the benchmarks, indicating its inherent strong capabilities even when not enhanced with the deliberate thinking process. For our flagship dense model, Qwen3-32B, we take DeepSeek-R1-Distill-Llama-70B, OpenAI- (medium), and our previous strongest reasoning model, QWQ-32B (Qwen Team, 2025), as the",
        "source": {
          "paper": "2505.09388v1.pdf",
          "page": 15,
          "region_type": "text",
          "region_id": "page15_region0"
        },
        "score": 0.377,
        "citation": "2505.09388v1.pdf, Page 15"
      },
      {
        "text": "(2024a). examine the proportion of tasks solved by SoCE that were also solved by each individual model. SoCE completes 97.2% of the tasks solved by xLAM-2-70b-fe-r (Prabhakar et al., 2025), 97.1% of those solved CoALM-70B (Acikgoz et al., 2025), and 97.1% of those solved by watt-tool-70B gm8.ai (2024a). This indicates that retains most of the capabilities of individual models in the soup.",
        "source": {
          "paper": "583127822_872908961738101_8157213289131333559_n.pdf",
          "page": 14,
          "region_type": "text",
          "region_id": "page14_region9"
        },
        "score": 0.37,
        "citation": "583127822_872908961738101_8157213289131333559_n.pdf, Page 14"
      }
    ],
    "sources": [
      "Research Paper on Machine Learning (Topic: Machine Learning) - Pages 9, 15, 23",
      "Multi-Modal AI Research (Topic: Multi-Modal Learning) - Pages 5",
      "Computer Vision and Image Processing (Topic: Computer Vision) - Pages 18",
      "Hunyuan Foundation Model (Topic: Foundation Models) - Pages 5",
      "AI Research and Development (Topic: AI Research) - Pages 6, 14"
    ],
    "has_evidence": true,
    "retrieval_stats": {
      "total_chunks": 10,
      "papers_searched": [
        "2511.18870v2.pdf",
        "2505.09388v1.pdf",
        "2505.19147v3.pdf",
        "583127822_872908961738101_8157213289131333559_n.pdf",
        "2505.10554v2.pdf"
      ],
      "by_paper": {
        "2511.18870v2.pdf": 1,
        "2505.09388v1.pdf": 5,
        "2505.19147v3.pdf": 1,
        "583127822_872908961738101_8157213289131333559_n.pdf": 2,
        "2505.10554v2.pdf": 1
      },
      "by_region_type": {
        "text": 8,
        "list": 1,
        "title": 1
      }
    },
    "cached_at": "2026-02-11T00:33:08.953446"
  },
  "What is the role of Reinforcement Learning in video generation?": {
    "question": "What is the role of Reinforcement Learning in video generation?",
    "answer": "Reinforcement Learning (RL) plays a crucial role in video generation, particularly in tasks that require complex motion and temporal consistency. According to [Evidence 8], the Hunyuan Foundation Model employs Reinforcement Learning from Human Feedback (RLHF) strategies tailored for Text-to-Video (T2V) tasks, mainly aimed at correcting artifacts and enhancing motion quality. This approach involves using a VLM-based reward model to evaluate videos across four key dimensions: text alignment, visual quality, and motion dynamics.\n\nMoreover, [Evidence 9] highlights the importance of RL in video generation by introducing the Physics-Guided Rewarding (PGR) scheme, which leverages a physics-aware VLM to provide a principled physics-aware Groupwise Direct Preference Optimization (DPO) framework for physical consistent T2V generation. This framework stimulates the probability distribution over physics-consistent samples, exerting stronger influence during training and improving stability.\n\nIn addition, [Evidence 5] from the Research Paper on Machine Learning mentions that RL allows agents to learn desirable behaviors directly from environmental feedback across diverse tasks, making it a natural fit for training models in agent-based systems. This capability opens up the potential for transforming LLM-generated text into real-world actions.\n\nOverall, the evidence suggests that RL is a key component in video generation, particularly in tasks that require complex motion and temporal consistency. By leveraging RL, researchers can improve the quality and consistency of generated videos, and even enable the transformation of LLM-generated text into real-world actions.\n\n---\n**Evidence:**\n- Evidence 8 (from Hunyuan Foundation Model): Describes the use of Reinforcement Learning from Human Feedback (RLHF) strategies for Text-to-Video (T2V) tasks, aimed at correcting artifacts and enhancing motion quality.\n- Evidence 9 (from Text-to-Video Generation with Physical Consistency): Introduces the Physics-Guided Rewarding (PGR) scheme, which leverages a physics-aware VLM to provide a principled physics-aware Groupwise Direct Preference Optimization (DPO) framework for physical consistent T2V generation.\n- Evidence 5 (from Research Paper on Machine Learning): Mentions that RL allows agents to learn desirable behaviors directly from environmental feedback across diverse tasks, making it a natural fit for training models in agent-based systems.\n\n**Sources:**\n- [Hunyuan Foundation Model] (Topic: Foundation Models) - Pages 7\n- [Text-to-Video Generation with Physical Consistency] (Topic: Video Generation) - Pages 2, 7, 9, 11, 13\n- [Research Paper on Machine Learning] (Topic: Machine Learning) - Page 22",
    "evidence": [
      {
        "text": "checkpoint, enabling upon a high-quality SFT, we utilize rigorously selected clips per task, filtered strictly considering aesthetic and motion smoothness. Through CT and SFT, we observe evident improvements in visual quality, aesthetic appeal, and temporal consistency for both generation tasks. employ Reinforcement Learning from Human Feedback (RLHF) strategies tailored for T2V tasks, both mainly aimed at correcting artifacts and enhancing motion quality. task, we apply online reinforcement learning (RL) during post-training to correct",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 7,
          "region_type": "text",
          "region_id": "page7_region1"
        },
        "score": 0.598,
        "citation": "2511.18870v2.pdf, Page 7"
      },
      {
        "text": "al flow matching for efficient video generative modeling. In JCLR, 2025. Xuan Ju, Tianyu Wang, Yugian Zhou, He Zhang, Qing Liu, Nanxuan Zhao, Zhifei Zhang, Yijun Li, Yuanhao Cai, Shaoteng Liu, et al. Editverse: Unifying image and video editing and generation with in-context learning. arXiv preprint arXiv:2509.20360, 2025. Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao Huang, and Jiashi Feng. How far is video generation from world model: A physical law perspective. In JCML, 2025. Shyamgopal Karthik, Huseyin Coskun, Zeynep Akata, Sergey Tulyakov, Jian Ren, and Anil Kag. Scalable ranked preference optimization for text-to-image generation. In ICCV, 2025. Lei Ke, Haohang Xu, Xuefei Ning, Yu Li, Jiajun Li, Haoling Li, Yuxuan Lin, Dongsheng Jiang, Yujiu Yang, and Linfeng Zhang. Proreflow: Progressive reflow with decomposed velocity. In CVPR, 2025. Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, et al. Videopoet: A large language model for zero-shot video generation. In ICML, 2024. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: A systematic framework for large video generative models. arXiv preprint arXiv:2412.03608, 2024. Xin Li, Wenqing Chu, Ye Wu, Weihang Yuan, Fanglong Liu, Qi Zhang, Fu Li, Haocheng Feng, Errui Ding, and Jingdong Wang. Videogen: A reference-guided latent diffusion approach for high definition text-to-video generation. arXiv preprint arXiv:2309.00398, 2023. Jie Liu, Gongye Liu, Jiajun Liang, Ziyang Yuan, Xiaokun Liu, Mingwu Zheng, Xiele Wu, Qiulin Wang, Wenyu Qin, Menghan Xia, et al. Improving video generation with human feedback. arXiv preprint arXiv:2501.13918, 2025. Runtao Liu, Haoyu Wu, Ziqiang Zheng, Chen Wei, Yingqing He, Renjie Pi, and Qifeng Chen. Videodpo: Omni-preference alignment for video diffusion generation. In CVPR, 2025. Ilya Loshchilov and Frank Hutter.",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 12,
          "region_type": "list",
          "region_id": "page12_region0"
        },
        "score": 0.561,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 12"
      },
      {
        "text": "training purposes compression, scaling to long contexts, etc. In addition, we plan to increase computational resources for reinforcement with a particular emphasis on agent-based RL systems that learn from environmental feedback. allow us to build agents capable of tackling complex tasks that require inference time scaling. Authors Contributors: An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Kai Kexin Le",
        "source": {
          "paper": "2505.09388v1.pdf",
          "page": 22,
          "region_type": "text",
          "region_id": "page22_region2"
        },
        "score": 0.504,
        "citation": "2505.09388v1.pdf, Page 22"
      },
      {
        "text": "images in InternVL3 (Zhu et al., 2025) 10K-frame videos in Video-XL-2 (Qin et al., enabling strong performance on image (Bai 2025), video (Yang et al., 2025b), and multi- reasoning tasks (Shen et al., 2025). More Complex Content in Generation AI content generation has advanced sig- with the application of Transformers to domains (Peebles and Xie, 2023; Brooks 2024; Li et al., 2024f). Early diffusion mod- Stable Diffusion (Rombach et al., 2022) only 512 x 512 resolution images. With now successfully applied to gener- (Peebles and Xie, 2023; Gao et al., 2023; et al., 2024; Li et al., 2024f), DiT-based produce high-quality 4K images in PixArt- et al., 2024a) and hour-long videos in (Brooks et al., 2024). These models capture spatiotemporal dependencies, enabling content generation (Labs, 2024; Yang 2024c; Wan et al., 2025a; Kang et al., 2025). these advancements across domains have outstanding performance, they now et al., 2024b; Zou et al., 2025). (1) Efficient Computation Since computational efficiency architectural design, optimizing tal. Unlike Transformers with complexity O(n?) (Vaswani methods achieve linear or (i) linear attention reformulates complexity (Katharopoulos et 2021); (ii) RWKV combines ing with transformer parallelism Duan et al., 2024); (iii) State Mamba use structured state plexity (Gu and Dao, 2023; quire retraining, motivating (II) Model-centric Compression ing parameter complexity and memory costs. Model centric, transforming W to a W’=T(W), where with I as the compression",
        "source": {
          "paper": "2505.19147v3.pdf",
          "page": 3,
          "region_type": "text",
          "region_id": "page3_region7"
        },
        "score": 0.504,
        "citation": "2505.19147v3.pdf, Page 3"
      },
      {
        "text": "costly complex on signals. the need for task-specific curated data and allows agents to learn desirable behaviors directly environment feedback across diverse tasks. Moreover, the trial-and-error nature of RL closely how humans acquire problem-solving skills, enabling models to learn action policies grounded deployment contexts. This capability opens up the potential for transforming LLM-generated text into real-world actions, making RL a natural fit for training models in agent-based systems. authors.",
        "source": {
          "paper": "2508.03680v1.pdf",
          "page": 1,
          "region_type": "text",
          "region_id": "page1_region0"
        },
        "score": 0.469,
        "citation": "2508.03680v1.pdf, Page 1"
      },
      {
        "text": "CVPR, 2024. Fanqing Meng, Jiaqi Liao, Xinyu Tan, Wenqi Shao, Quanfeng Lu, Kaipeng Zhang, Yu Cheng, Dianqi Li, Yu Qiao, and Ping Luo. Towards world simulator: Crafting physical commonsense-based benchmark for video generation. In ICML, 2025. Sanghyeon Na, Yonggyu Kim, and Hyunjoon Lee. Boost your human image generation model via direct preference optimization. In CVPR, 2025. Sharan Narang, Gregory Diamos, Erich Elsen, Paulius Micikevicius, Jonah Alben, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. In ICLR, 2018. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019. William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. Pika. Pika art. https://pika.art/. Accessed: 2025-05-04. Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et al. Movie gen: A cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. In NeurIPS, 2023. Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In EMNLP, 2019. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseeckmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Dani Valevski, Yaniv Leviathan, Moab Arar, and Shlomi Fruchter. Diffusion models are real-time game engines. In ICLR, 2025. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need.",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 13,
          "region_type": "list",
          "region_id": "page13_region0"
        },
        "score": 0.549,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 13"
      },
      {
        "text": "Hassan Abu Alhaija, Jose Alvarez, Maciej Bala, Tiffany Cai, Tianshi Cao, Liz Cha, Joshua Chen, Mike Chen, Francesco Ferroni, Sanja Fidler, et al. Cosmos-transferl: Conditional world generation with adaptive multimodal control. arXiv preprint arXiv:2503.14492, 2025. Hritik Bansal, Zongyu Lin, Tianyi Xie, Zeshun Zong, Michal Yarom, Yonatan Bitton, Chenfanfu Jiang, Yizhou Sun, Kai-Wei Chang, and Aditya Grover. Videophy: Evaluating physical commonsense for video generation. In ICLR, 2025. Hritik Bansal, Clark Peng, Yonatan Bitton, Roman Goldenberg, Aditya Grover, and Kai-Wei Chang. Videophy-2: A challenging action-centric physical commonsense evaluation in video generation. In ICMLW, 2025. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators. OpenAI Blog, 2024. Yuanhao Cai, He Zhang, Xi Chen, Jinbo Xing, Yiwei Hu, Yugian Zhou, Kai Zhang, Zhifei Zhang, Soo Ye Kim, Tianyu Wang, et al. Omnivcus: Feedforward subject-driven video customization with multimodal control conditions. In NeurIPS, 2025. Yuanhao Cai, He Zhang, Kai Zhang, Yixun Liang, Mengwei Ren, Fujun Luan, Qing Liu, Soo Ye Kim, Jianming Zhang, Zhifei Zhang, Yuqian Zhou, Yulun Zhang, Xiaokang Yang, Zhe Lin, and Alan Yuille. Baking gaussian splatting into diffusion denoiser for fast and scalable single-stage image-to-3d generation and reconstruction. In ICCV, 2025.",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 11,
          "region_type": "list",
          "region_id": "page11_region0"
        },
        "score": 0.529,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 11"
      },
      {
        "text": "motion artifacts. Our approach begins with a curated prompt set spanning 100+ constructed from high-aesthetic images. Candidate prompts are first generated via a model (VLM), then manually verified to ensure strict text-image consistency. We VLM-based reward model to evaluate videos across four key dimensions: text alignment, visual quality, and motion dynamics. During RL training, we employ mixed strategies by varying both random seeds and CFG scales, and adopt a hybrid ODE-SDE [19] to enrich exploration while maintaining sampling quality. This RLHF process improvements across all evaluation metrics, with particularly notable gains in motion this, our T2V alignment strategy uses a more comprehensive hybrid offline-then-online designed to address the increased complexity of T2V motion artifacts. We found that",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 7,
          "region_type": "text",
          "region_id": "page7_region5"
        },
        "score": 0.513,
        "citation": "2511.18870v2.pdf, Page 7"
      },
      {
        "text": "[63] ru le for each data samp. over 135K text-video da Optimization (PhyGDPO) framework for physical on the groupwise Plackett-Luce (PL) probabilistic model, list of candidate videos and thus enables holistic preference adaptation ra DPO training uses the model itsel! pairwise judgments. Plus, can not during more challenging physical ac opt: imization. To improve he origina rovide sufficiently good signals and knowledge of training. Unlike GRP’ Physics-Guided Rewarding (PGR) scheme that O [54], whic ions and al the DPO tr aining efficiency h suffers from s leverages a phy: sics-aware VLM to e. Then we use PhyAugPipe to collect a training datase a pairs. Secondly, we formulate a principled Physics-aware Groupwise Direct y consistent T2V generation. which stimulates the probabi! her than being constrained to as rewar hysics. Hence, we propose to ow speed and unstable optimization, we propose a guide the ; PhyVidGen-135K, Our PhyGDPO is ity distribution over [52] but the model embed more physics DPO data sampling ow physics-violating samples to exert stronger influence during and stability, we also propose a LoRA [28]-Switch",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 2,
          "region_type": "text",
          "region_id": "page2_region1"
        },
        "score": 0.509,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 2"
      },
      {
        "text": "models struggle to effectively differentiate fine-grained motion quality. Therefore, an offline optimization stage using Direct Preference Optimization (DPO). For a balanced prompt set (from LLMs-generated prompts and training video covering diverse dimensions (motion, scene, subject, etc.). Using a selected high-quality we generate N video candidates per prompt to create non-repetitive pairs. These pairs",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 7,
          "region_type": "text",
          "region_id": "page7_region6"
        },
        "score": 0.503,
        "citation": "2511.18870v2.pdf, Page 7"
      }
    ],
    "sources": [
      "Research Paper on Machine Learning (Topic: Machine Learning) - Pages 22",
      "Computer Vision and Image Processing (Topic: Computer Vision) - Pages 3",
      "Deep Learning Architectures (Topic: Deep Learning) - Pages 1",
      "Hunyuan Foundation Model (Topic: Foundation Models) - Pages 7",
      "Text-to-Video Generation with Physical Consistency (Topic: Video Generation) - Pages 2, 11, 12, 13"
    ],
    "has_evidence": true,
    "retrieval_stats": {
      "total_chunks": 10,
      "papers_searched": [
        "2511.18870v2.pdf",
        "605562216_1287553579874010_8068650067268945856_n.pdf",
        "2505.09388v1.pdf",
        "2505.19147v3.pdf",
        "2508.03680v1.pdf"
      ],
      "by_paper": {
        "2511.18870v2.pdf": 3,
        "605562216_1287553579874010_8068650067268945856_n.pdf": 4,
        "2505.09388v1.pdf": 1,
        "2505.19147v3.pdf": 1,
        "2508.03680v1.pdf": 1
      },
      "by_region_type": {
        "text": 7,
        "list": 3
      }
    },
    "cached_at": "2026-02-11T00:33:15.335619"
  },
  "How are the models trained for multi-task learning?": {
    "question": "How are the models trained for multi-task learning?",
    "answer": "The models are trained for multi-task learning through various techniques, including parameter-space model merging and model selection and weighting. According to [Evidence 2], the paper on Multi-Modal AI Research, parameter-space model merging is used to integrate meta-abilities into a single network, improving average accuracy across math, coding, and science by 2% on a 32B model over the instruction-tuned baseline. This suggests that the models are trained to learn multiple tasks simultaneously.\n\nAdditionally, [Evidence 9] from the same paper on Multi-Modal AI Research mentions that parameter-space model merging enables a cost-efficient combination of complementary competencies without training, and a high-quality initialization for domain-specific fine-tuning in Stage C. This implies that the models are trained to learn multiple tasks and then fine-tuned for specific domains.\n\nFurthermore, [Evidence 1] from the AI Research and Development paper mentions that model merging has been used in continual learning to prevent catastrophic forgetting during fine-tuning of LLMs. This suggests that the models are trained to learn multiple tasks and then fine-tuned to prevent forgetting of previously learned tasks.\n\nIt's worth noting that [Evidence 7] from the Hunyuan Foundation Model paper mentions that the HunyuanVideo 1.5 model is designed for multi-task learning, but it does not provide information on how the model is trained for multi-task learning.\n\nIn summary, the models are trained for multi-task learning through parameter-space model merging, model selection and weighting, and fine-tuning for specific domains.\n\n---\n**Evidence:**\n- Evidence 1 (from AI Research and Development): Model merging has been used in continual learning to prevent catastrophic forgetting during fine-tuning of LLMs.\n- Evidence 2 (from Multi-Modal AI Research): Parameter-space model merging is used to integrate meta-abilities into a single network, improving average accuracy across math, coding, and science by 2% on a 32B model over the instruction-tuned baseline.\n- Evidence 9 (from Multi-Modal AI Research): Parameter-space model merging enables a cost-efficient combination of complementary competencies without training, and a high-quality initialization for domain-specific fine-tuning in Stage C.\n\n**Sources:**\n- AI Research and Development (Topic: AI Research) - Page 2\n- Multi-Modal AI Research (Topic: Multi-Modal Learning) - Pages 2, 5, 6\n- Hunyuan Foundation Model (Topic: Foundation Models) - Page 4\n- Text-to-Video Generation with Physical Consistency (Topic: Video Generation) - Page 3\n- Research Paper on Machine Learning (Topic: Machine Learning) - Page 3\n- Natural Language Processing Study (Topic: Natural Language Processing) - Page 19\n- Deep Learning Architectures (Topic: Deep Learning) - Page 10\n- Computer Vision and Image Processing (Topic: Computer Vision) - Page 4",
    "evidence": [
      {
        "text": "merge to the original input candidates. In another line of work, model souping has been used in continual learning to catastrophic forgetting during finetuning of LLMs Kleiman et al. (2025). model merging. Several works explore automatic model merging techniques: authors in Yang et al. (2024) we can use unsupervised methods to discover optimal merging parameters based on entropy minimization. The is mostly applied to classification tasks with ViT models, leading to gains in the multi-task learning domain classification models, not yet applied to LLMs. Akiba et al. (2025) apply evolutionary algorithms to model merging, that effective combinations of open-source models can be discovered automatically, leading to higher-performing of multimodal capabilities. averaging and merging techniques have gained significant traction, and various studies have collectively demon- that this approach: 1) can result in improved performance for post-trained LLMs; 2) can be directed automatically; leverage the landscape of the open-source derivative models of the same architecture. Methodology: Soup Of Category Experts section, we present our method for model selection and weighting the selected models with the benchmark",
        "source": {
          "paper": "583127822_872908961738101_8157213289131333559_n.pdf",
          "page": 2,
          "region_type": "text",
          "region_id": "page2_region0"
        },
        "score": 0.5,
        "citation": "583127822_872908961738101_8157213289131333559_n.pdf, Page 2"
      },
      {
        "text": "raises overall accuracy by more than 10% relative to a vanilla instruction-tuned incorporate the three competencies into a single network, Rather than training the model task corpus, we utilize parameter-space model merging to integrate these meta-abilities. merging improves average accuracy across math, coding, and science by 2% on and 4% on a 32B model over the instruction-tuned baseline, demonstrating the strong of merged meta-abilities. to evaluate whether meta-ability alignment offers a stronger foundation for subsequent resumed domain-specific RL training from a checkpoint that had already been aligned it with the same procedure applied to an instruction-tuned model. Starting from the checkpoint raises the attainable performance ceiling: after identical continual domain- training, the model achieves an average gain of about 2% over its instruction-only Our key contributions are as follows:",
        "source": {
          "paper": "2505.10554v2.pdf",
          "page": 2,
          "region_type": "text",
          "region_id": "page2_region4"
        },
        "score": 0.499,
        "citation": "2505.10554v2.pdf, Page 2"
      },
      {
        "text": "param- models, these cri- main approaches: auxiliary net- X + {s:}7_), A@ beyond the methods in- (Rao et al., 2024d; Kim et al., training to learn {s,}_,, and (ii) et al., 2024; employ pre-trained : KX + utilize parameter- without introduc- approaches can be computation meth- al., 2025; Chen Ge et al., 2024) calculations for to- such as using where a; repre- and (ii) ex- et al., 2023a;",
        "source": {
          "paper": "2505.19147v3.pdf",
          "page": 4,
          "region_type": "text",
          "region_id": "page4_region9"
        },
        "score": 0.478,
        "citation": "2505.19147v3.pdf, Page 4"
      },
      {
        "text": "is responsible for updating the model weights, while the rollout captures used by the trainer. For single-turn RL tasks, such as mathematical reasoning, the",
        "source": {
          "paper": "2508.03680v1.pdf",
          "page": 10,
          "region_type": "title",
          "region_id": "page10_region7"
        },
        "score": 0.461,
        "citation": "2508.03680v1.pdf, Page 10"
      },
      {
        "text": "training a large-scale at Specifically, layer is compatible with data, pipeline, and context parallelism (DP, PP, CP), but for tensor parallelism (TP). In the naive implementation (Please see Figure 2b or 1), the Chain-of-Linear layer can be viewed as a composition of multiple sub-linear a result, if we apply tensor parallelism to each sub-linear within the Chain-of-Linear 19",
        "source": {
          "paper": "2505.11820v2.pdf",
          "page": 19,
          "region_type": "text",
          "region_id": "page19_region0"
        },
        "score": 0.445,
        "citation": "2505.11820v2.pdf, Page 19"
      },
      {
        "text": "tokens leads to a consistent improvement in the model’s performance across various tasks. following sections, we describe the design of the model architecture, provide details on its training present the experimental results of pre-trained and post-trained models, and finally, conclude technical report by summarizing the key findings and outlining potential directions for future",
        "source": {
          "paper": "2505.09388v1.pdf",
          "page": 3,
          "region_type": "text",
          "region_id": "page3_region2"
        },
        "score": 0.438,
        "citation": "2505.09388v1.pdf, Page 3"
      },
      {
        "text": "transformer (DiT) model with 8.3 billion parameters, designed for multi-task learning. The hyperparameters of HunyuanVideo 1.5 are detailed in Subsequently, a video network is utilized to further enhance visual quality. Comprehensive elaborations",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 4,
          "region_type": "text",
          "region_id": "page4_region6"
        },
        "score": 0.438,
        "citation": "2511.18870v2.pdf, Page 4"
      },
      {
        "text": "semantics matching, thereby determining the action category to which each data sample belongs. We count number of data samples in each action category to obtain the distribution histogram Hy, for the filtered data. Sampling with Physics Rewarding. As different action categories vary significantly in physical complexity thus to different generation difficulties for the T2V model, we propose to balance the training data accordingly. particular, we first evaluate how well the T2V model performs on each action category. Based on the performance we adjust the sampling ratio by allocating more samples to the categories where the model performs",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 3,
          "region_type": "text",
          "region_id": "page3_region4"
        },
        "score": 0.412,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 3"
      },
      {
        "text": "strengths of models specialized in distinct meta-abilities, we adopt parameter-space enables: (i) a cost-efficient combination of complementary competencies without training, and (ii) a high-quality initialization for domain-specific fine-tuning in Stage C. parameters of the deduction-, induction-, and abduction-aligned specialists as 0, respectively. These models, trained separately on their respective meta-abilities, highly complementary behaviors—aggregating their predictions. We construct the Omerge by linearly interpolating the weights of the three specialists: = WOM + 0 + LO (2) the contribution of each specialist model via scalar weights Ag, A;, and Aq. These determine the relative influence of each meta-ability in the merged model. Notably, weighting is not assumed—unequal allocation may better reflect the asymmetry in task",
        "source": {
          "paper": "2505.10554v2.pdf",
          "page": 5,
          "region_type": "text",
          "region_id": "page5_region4"
        },
        "score": 0.478,
        "citation": "2505.10554v2.pdf, Page 5"
      },
      {
        "text": "We sample 200 instances per task per level the 32B model. For further domain-specific",
        "source": {
          "paper": "2505.10554v2.pdf",
          "page": 6,
          "region_type": "title",
          "region_id": "page6_region7"
        },
        "score": 0.449,
        "citation": "2505.10554v2.pdf, Page 6"
      }
    ],
    "sources": [
      "Research Paper on Machine Learning (Topic: Machine Learning) - Pages 3",
      "Multi-Modal AI Research (Topic: Multi-Modal Learning) - Pages 2, 5, 6",
      "Natural Language Processing Study (Topic: Natural Language Processing) - Pages 19",
      "Computer Vision and Image Processing (Topic: Computer Vision) - Pages 4",
      "Deep Learning Architectures (Topic: Deep Learning) - Pages 10",
      "Hunyuan Foundation Model (Topic: Foundation Models) - Pages 4",
      "AI Research and Development (Topic: AI Research) - Pages 2",
      "Text-to-Video Generation with Physical Consistency (Topic: Video Generation) - Pages 3"
    ],
    "has_evidence": true,
    "retrieval_stats": {
      "total_chunks": 10,
      "papers_searched": [
        "583127822_872908961738101_8157213289131333559_n.pdf",
        "2505.10554v2.pdf",
        "2505.19147v3.pdf",
        "2508.03680v1.pdf",
        "2505.11820v2.pdf",
        "2505.09388v1.pdf",
        "2511.18870v2.pdf",
        "605562216_1287553579874010_8068650067268945856_n.pdf"
      ],
      "by_paper": {
        "583127822_872908961738101_8157213289131333559_n.pdf": 1,
        "2505.10554v2.pdf": 3,
        "2505.19147v3.pdf": 1,
        "2508.03680v1.pdf": 1,
        "2505.11820v2.pdf": 1,
        "2505.09388v1.pdf": 1,
        "2511.18870v2.pdf": 1,
        "605562216_1287553579874010_8068650067268945856_n.pdf": 1
      },
      "by_region_type": {
        "text": 8,
        "title": 2
      }
    },
    "cached_at": "2026-02-11T00:33:23.530854"
  },
  "What data acquisition and filtering methods are described?": {
    "question": "What data acquisition and filtering methods are described?",
    "answer": "Based on the provided evidence, it appears that there is no clear information about data acquisition and filtering methods described in the given passages. However, I can try to provide some insights based on the available information.\n\nThe provided evidence does not explicitly mention data acquisition and filtering methods. Evidence 1 mentions the LLM (Large Language Model) and the number of invocations in the data extraction process, but it does not provide any information about data acquisition or filtering methods. Similarly, Evidence 2 mentions efficiency and overhead in data processing, but it does not provide any details about data acquisition or filtering.\n\nEvidence 3 mentions the architecture of a text-to-speech model, which includes a tokenizer, an autoregressive Transformer, and a latent flow matching model. However, it does not provide any information about data acquisition or filtering methods.\n\nEvidence 4 mentions the topic of a research paper on machine learning, but it does not provide any information about data acquisition or filtering methods.\n\nEvidence 5 mentions dynamic inference usage in natural language processing, but it does not provide any information about data acquisition or filtering methods.\n\nEvidence 6 lists the authors of a foundation model, but it does not provide any information about data acquisition or filtering methods.\n\nEvidence 7 mentions limitations in AI research, but it does not provide any information about data acquisition or filtering methods.\n\nEvidence 8 mentions a study on text-to-video generation, but it does not provide any information about data acquisition or filtering methods.\n\nEvidence 9 mentions the details of pretraining data in a machine learning paper, but it does not provide any information about data acquisition or filtering methods.\n\nEvidence 10 mentions SQL queries in deep learning architectures, but it does not provide any information about data acquisition or filtering methods.\n\nTherefore, based on the provided evidence, I must conclude that:\n\nThe provided evidence does not contain sufficient information to answer this question.\n\n---\n**Evidence:**\n- Evidence 1 (from Deep Learning Architectures): Describes the LLM and the number of invocations in the data extraction process.\n- Evidence 2 (from Computer Vision and Image Processing): Mentions efficiency and overhead in data processing.\n- Evidence 3 (from MiniMax-Speech: Autoregressive Text-to-Speech Model): Describes the architecture of a text-to-speech model.\n- Evidence 4 (from Research Paper on Machine Learning): Mentions the topic of a research paper on machine learning.\n- Evidence 5 (from Natural Language Processing Study): Discusses dynamic inference usage in natural language processing.\n- Evidence 6 (from Hunyuan Foundation Model): Lists the authors of a foundation model.\n- Evidence 7 (from AI Research and Development): Mentions limitations in AI research.\n- Evidence 8 (from Text-to-Video Generation with Physical Consistency): Discusses a study on text-to-video generation.\n- Evidence 9 (from Research Paper on Machine Learning): Mentions the details of pretraining data.\n- Evidence 10 (from Deep Learning Architectures): Mentions SQL queries in deep learning architectures.\n\n**Sources:**\n- \"Deep Learning Architectures\" (Topic: Deep Learning) - Pages 7, 11\n- \"Computer Vision and Image Processing\" (Topic: Computer Vision) - Page 3\n- \"MiniMax-Speech: Autoregressive Text-to-Speech Model\" (Topic: Text-to-Speech (TTS) Generation) - Page 3\n- \"Research Paper on Machine Learning\" (Topic: Machine Learning) - Pages 3, 11\n- \"Natural Language Processing Study\" (Topic: Natural Language Processing) - Page 8\n- \"Hunyuan Foundation Model\" (Topic: Foundation Models) - Page 12\n- \"AI Research and Development\" (Topic: AI Research) - Page 9\n- \"Text-to-Video Generation with Physical Consistency\" (Topic: Video Generation) - Page 9\n- \"Research Paper on Machine Learning\" (Topic: Machine Learning) - Page 3\n- \"Deep Learning Architectures\" (Topic: Deep Learning) - Page 11",
    "evidence": [
      {
        "text": "is the LLM to be optimized and T is the number of invocations in this this data extraction process.",
        "source": {
          "paper": "2508.03680v1.pdf",
          "page": 7,
          "region_type": "text",
          "region_id": "page7_region8"
        },
        "score": 0.417,
        "citation": "2508.03680v1.pdf, Page 7"
      },
      {
        "text": "efficiency, partic- overhead Detailed statistical Appendix A. Perspectives a key goal in data X and net- F produces",
        "source": {
          "paper": "2505.19147v3.pdf",
          "page": 3,
          "region_type": "text",
          "region_id": "page3_region0"
        },
        "score": 0.378,
        "citation": "2505.19147v3.pdf, Page 3"
      },
      {
        "text": "on its robust zero-shot capabilities. As shown in Figure 1, it primarily comprises three a tokenizer, an autoregressive Transformer, and a latent flow matching model, which of flow matching module and Flow-VAE module. The text tokenizer utilizes Byte Pair (BPE), while the audio tokenizer employs Encoder-VQ-Decoder architecture (Betker, 2023; Oord et al., 2017) quantization on mel-spectrograms with a rate of 25 tokens per second",
        "source": {
          "paper": "2505.07916v1.pdf",
          "page": 3,
          "region_type": "list",
          "region_id": "page3_region1"
        },
        "score": 0.369,
        "citation": "2505.07916v1.pdf, Page 3"
      },
      {
        "text": "data, on the other hand, is carefully mathematics, instruction-following, multilingual",
        "source": {
          "paper": "2505.09388v1.pdf",
          "page": 11,
          "region_type": "title",
          "region_id": "page11_region7"
        },
        "score": 0.368,
        "citation": "2505.09388v1.pdf, Page 11"
      },
      {
        "text": "is to provide dynamic inference usage to meet the requirements from different",
        "source": {
          "paper": "2505.11820v2.pdf",
          "page": 8,
          "region_type": "text",
          "region_id": "page8_region11"
        },
        "score": 0.355,
        "citation": "2505.11820v2.pdf, Page 8"
      },
      {
        "text": "& Data: Xin Li, Bing Wu, Duojun Huang, Hao Tan, Xinchi Deng, Xuefei Zhenyu Wang Model Accleration: Songtao Liu, Changlin Li, Chang Zou, Fang Yang, Wu, Jack Peng, Patrol, Peizhen Zhang, Penghao Zhao, Weiyan Wang, Xiao Li, Yuanbo Peng & Post Training: Yue Wu, Jiangfeng Xiong, Qi Tian, Weijie Kong, Long, Zuozhuo Dai (Listed alphabetically): Bo Peng, Coopers Li, Gu Gong, Guojian Xiao, Jiaxin Lin, Jie Liu, Jihong Zhang, Jiesong Lian, Kaihang Pan, Lei Wang, Lin Chen, Mingyang Chen, Mingzhe Zheng, Miles Yang, Qiangqiang Hu, Qi Xiao, Runzhou Wu, Ryan Xu, Rui Yuan, Shanshan Sang, Shisheng Huang, Shuo Huang, Weiting Guo, Xiang Yuan, Xiaojia Chen, Xiawei Hu, Wenzhi Xianshun Ren, Xiaoyan Yuan, Xiaoyue Mi, Yepeng Zhang, Yifu Sun, Yiting Li, You Huang, Yu Tang, Yixuan Li, Yuhang Deng, Yuan Zhou, Zhichao Hu, Zhihe Yang, Zilin Yang, Zhenzhi Lu, Zixiang Zhou",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 12,
          "region_type": "list",
          "region_id": "page12_region0"
        },
        "score": 0.341,
        "citation": "2511.18870v2.pdf, Page 12"
      },
      {
        "text": "Limitations General Method Limitations",
        "source": {
          "paper": "583127822_872908961738101_8157213289131333559_n.pdf",
          "page": 9,
          "region_type": "title",
          "region_id": "page9_region9"
        },
        "score": 0.325,
        "citation": "583127822_872908961738101_8157213289131333559_n.pdf, Page 9"
      },
      {
        "text": "reports the human evaluators than all SOTA methods.",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 9,
          "region_type": "text",
          "region_id": "page9_region10"
        },
        "score": 0.313,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 9"
      },
      {
        "text": "of our pretraining data, the details of our pretraining",
        "source": {
          "paper": "2505.09388v1.pdf",
          "page": 3,
          "region_type": "text",
          "region_id": "page3_region12"
        },
        "score": 0.348,
        "citation": "2505.09388v1.pdf, Page 3"
      },
      {
        "text": "SQL queries from",
        "source": {
          "paper": "2508.03680v1.pdf",
          "page": 11,
          "region_type": "title",
          "region_id": "page11_region9"
        },
        "score": 0.345,
        "citation": "2508.03680v1.pdf, Page 11"
      }
    ],
    "sources": [
      "MiniMax-Speech: Autoregressive Text-to-Speech Model (Topic: Text-to-Speech (TTS) Generation) - Pages 3",
      "Research Paper on Machine Learning (Topic: Machine Learning) - Pages 3, 11",
      "Natural Language Processing Study (Topic: Natural Language Processing) - Pages 8",
      "Computer Vision and Image Processing (Topic: Computer Vision) - Pages 3",
      "Deep Learning Architectures (Topic: Deep Learning) - Pages 7, 11",
      "Hunyuan Foundation Model (Topic: Foundation Models) - Pages 12",
      "AI Research and Development (Topic: AI Research) - Pages 9",
      "Text-to-Video Generation with Physical Consistency (Topic: Video Generation) - Pages 9"
    ],
    "has_evidence": true,
    "retrieval_stats": {
      "total_chunks": 10,
      "papers_searched": [
        "2508.03680v1.pdf",
        "2505.19147v3.pdf",
        "2505.07916v1.pdf",
        "2505.09388v1.pdf",
        "2505.11820v2.pdf",
        "2511.18870v2.pdf",
        "583127822_872908961738101_8157213289131333559_n.pdf",
        "605562216_1287553579874010_8068650067268945856_n.pdf"
      ],
      "by_paper": {
        "2508.03680v1.pdf": 2,
        "2505.19147v3.pdf": 1,
        "2505.07916v1.pdf": 1,
        "2505.09388v1.pdf": 2,
        "2505.11820v2.pdf": 1,
        "2511.18870v2.pdf": 1,
        "583127822_872908961738101_8157213289131333559_n.pdf": 1,
        "605562216_1287553579874010_8068650067268945856_n.pdf": 1
      },
      "by_region_type": {
        "text": 5,
        "list": 2,
        "title": 3
      }
    },
    "cached_at": "2026-02-11T00:33:44.324893"
  },
  "What are the model architectures and parameter counts?": {
    "question": "What are the model architectures and parameter counts?",
    "answer": "The provided evidence does not contain sufficient information to answer the question about the model architectures and parameter counts comprehensively. However, we can extract some relevant information from the evidence passages.\n\nAccording to [Evidence 6], the Qwen3 MoE models have the following architectures and parameter counts:\n\n* Qwen3-1.7B: 28 layers, 16/8 heads, 32K context length\n* Qwen3-4B: 36 layers, 32/8 heads, 128K context length\n* Qwen3-8B: 36 layers, 32/8 heads, 128K context length\n* Qwen3-14B: 40 layers, 40/8 heads, 128K context length\n* Qwen3-32B: 64 layers, 64/8 heads, 128K context length\n\nAdditionally, [Evidence 8] mentions that the Qwen3 models have parameter scales ranging from 0.6 to 32B.\n\nHowever, the evidence does not provide a comprehensive list of model architectures and parameter counts. For example, [Evidence 6] only lists the architectures and parameter counts for the Qwen3 MoE models, but does not provide information about other models. Similarly, [Evidence 8] mentions the parameter scales of the Qwen3 models, but does not provide specific information about the architectures.\n\nTo answer the question comprehensively, we would need more information about the model architectures and parameter counts.\n\n---\n**Evidence:**\n- Evidence 6 (from Machine Learning): Lists the architectures and parameter counts for the Qwen3 MoE models.\n- Evidence 8 (from Machine Learning): Mentions the parameter scales of the Qwen3 models, but does not provide specific information about the architectures.\n\n**Sources:**\n- 2505.09388v1.pdf (Topic: Machine Learning) - Pages 3, 6, 15, 1\n- 2505.11820v2.pdf (Topic: Natural Language Processing) - Pages 7, 19",
    "evidence": [
      {
        "text": "from three per- Architecture F (Shen et al., Dao, 2023), (ID model weights 2019; Li et al.,",
        "source": {
          "paper": "2505.19147v3.pdf",
          "page": 3,
          "region_type": "text",
          "region_id": "page3_region1"
        },
        "score": 0.54,
        "citation": "2505.19147v3.pdf, Page 3"
      },
      {
        "text": "Table 23: Performance of Qwen3 Models on",
        "source": {
          "paper": "2505.09388v1.pdf",
          "page": 23,
          "region_type": "title",
          "region_id": "page23_region5"
        },
        "score": 0.482,
        "citation": "2505.09388v1.pdf, Page 23"
      },
      {
        "text": "are reported in Table 1, and have these observations: 1) CoLM-Air with KV sharing will slightly affect model performance, as it only calculates keys and values within However, this design introduces greater flexibility, which will be discussed in the section; 2) using {16, 16} can achieve better performance when compared with {8, 8, 8, 8}.",
        "source": {
          "paper": "2505.11820v2.pdf",
          "page": 7,
          "region_type": "text",
          "region_id": "page7_region3"
        },
        "score": 0.468,
        "citation": "2505.11820v2.pdf, Page 7"
      },
      {
        "text": "benchmarks). By souping models, we allow covering these different skills well, as showcased by an improvement overall metric. The Shapley value indicates the relative contributions of the sub-models using souping as the function in the context of the skills of the set of all models. our Shapley computation by the following parameters; MGSM as the benchmark, the performance metric is accuracy, the characteristic function is souping and the set of candidates are (1) 4 open-source (OSS) models on LLama-7B (2) 6 pairs of the OSS models(3) 4 possible triplets these for model souping using a set of four models, evaluating all possible combinations. Separately, we also apply the SoCE framework to the same set. in Figure 6b, our analysis reveals that model contributions are not uniform; candidates and subsets selected exhibit significantly higher Shapley values. This finding underscores the critical role of SoCE’s candidate",
        "source": {
          "paper": "583127822_872908961738101_8157213289131333559_n.pdf",
          "page": 8,
          "region_type": "text",
          "region_id": "page8_region2"
        },
        "score": 0.455,
        "citation": "583127822_872908961738101_8157213289131333559_n.pdf, Page 8"
      },
      {
        "text": "We sample 200 instances per task per level the 32B model. For further domain-specific",
        "source": {
          "paper": "2505.10554v2.pdf",
          "page": 6,
          "region_type": "title",
          "region_id": "page6_region7"
        },
        "score": 0.422,
        "citation": "2505.10554v2.pdf, Page 6"
      },
      {
        "text": "Qwen3-1.7B 28 16/8 Yes 32K Qwen3-4B 36 32/8 Yes 128K Qwen3-8B 36 32/8 No 128K Qwen3-14B 40 40/8 No 128K Qwen3-32B 64 64/8 No 128K Table 2: Model architecture of Qwen3 MoE models. Models Layers Heads(Q/ KV) # Experts (Total / Activated) Context Length Qwen3-30B-A3B 48 32/4 128 / 8 128K",
        "source": {
          "paper": "2505.09388v1.pdf",
          "page": 3,
          "region_type": "text",
          "region_id": "page3_region5"
        },
        "score": 0.482,
        "citation": "2505.09388v1.pdf, Page 3"
      },
      {
        "text": "tasks (e.g., mathematics, agent, and coding), demonstrating ities of Qwen3-235B-A22B among open-source models.",
        "source": {
          "paper": "2505.09388v1.pdf",
          "page": 15,
          "region_type": "text",
          "region_id": "page15_region8"
        },
        "score": 0.464,
        "citation": "2505.09388v1.pdf, Page 15"
      },
      {
        "text": "latest version of the Qwen model family. Qwen3 models (LLMs) designed to advance performance, The Qwen3 series includes models of both dense with parameter scales ranging from 0.6 to",
        "source": {
          "paper": "2505.09388v1.pdf",
          "page": 1,
          "region_type": "text",
          "region_id": "page1_region4"
        },
        "score": 0.462,
        "citation": "2505.09388v1.pdf, Page 1"
      },
      {
        "text": "are designed from different perspectives to construct experts. In other words, CoM is orthogonal to MoE, meaning both of them can be applied within the same architecture advantages from each. We will explore how to combine CoM and MoE architecture, and work. When do we need the Chain-of-Model architecture? Chain-of-Model (CoM) architecture can be considered as an innovative design to optimize architecture (e.g., increasing model capabilities) of foundation models, rather than altering network (e.g., Transformer). Therefore, it can be generalized to any model with different such as CNNs or parallelized RNNs. However, this does not mean we can apply CoM any scenario. Based on the designs of CoM architecture, it can be considered as a of multiple nested sub-models. Each sub-model will contribute to the capability of models. More specifically, the first chain (sub-model) plays a critical role in CoM as it directly determines the understanding capability of the entire model. Therefore, if of the model is not sufficient to exhibit the power of the scaling law or lacks enough it is unnecessary to use the architecture of the chain-of-model method. In other words, structure is better suitable for large-scale model sizes, rather than smaller ones. This also CoM is an advanced design to optimize the scaling architecture. perspective of methodology, Chain-of-Model (CoM) introduces an innovative and brand-",
        "source": {
          "paper": "2505.11820v2.pdf",
          "page": 19,
          "region_type": "text",
          "region_id": "page19_region1"
        },
        "score": 0.461,
        "citation": "2505.11820v2.pdf, Page 19"
      },
      {
        "text": "overall performance of the Qwen3 dense base models is comparable to the Qwen2.5 base at higher parameter scales. For example, Qwen3-1.7B/4B/8B/14B/32B-Base achieve comparable performance to Qwen2.5-3B/7B/14B/32B/72B-Base, respectively. Especially in coding, and reasoning benchmarks, the performance of Qwen3 dense base models even Qwen2.5 base models at higher parameter scales. results are as follows. We compare Qwen3-235B-A22B-Base to our previous similar-sized MoE (Yang et al., 2024b) and other leading open-source base models: Llama-4-Maverick 2025), Qwen2.5-72B-Base (Yang et al., 2024b), DeepSeek-V3 Base (Liu et al., 2024a). From Table 3, the Qwen3-235B-A22B-Base model attains the highest performance scores across evaluated benchmarks. We further compare Qwen3-235B-A22B-Base with other baselines the detailed analysis. Compared with the recently open-source model Llama-4-Maverick-Base, which has about twice number of parameters, Qwen3-235B-A22B-Base still performs better on most benchmarks. Compared with the previously state-of-the-art open-source model DeepSeek-V3-Base, Qwen3- 235B-A22B-Base outperforms DeepSeek-V3-Base on 14 out of 15 evaluation benchmarks with about 1/3 the total number of parameters and 2/3 activated parameters, demonstrating the and cost-effectiveness of our models. Compared with our previous MoE Qwen2.5-Plus of similar size, Qwen3-235B-A22B-Base sig- outperforms it with fewer parameters and activated parameters, which shows the remarkable advantages of Qwen3 in pre-training data, training strategy, and model architecture.",
        "source": {
          "paper": "2505.09388v1.pdf",
          "page": 5,
          "region_type": "list",
          "region_id": "page5_region5"
        },
        "score": 0.457,
        "citation": "2505.09388v1.pdf, Page 5"
      }
    ],
    "sources": [
      "Research Paper on Machine Learning (Topic: Machine Learning) - Pages 1, 3, 5, 15, 23",
      "Multi-Modal AI Research (Topic: Multi-Modal Learning) - Pages 6",
      "Natural Language Processing Study (Topic: Natural Language Processing) - Pages 7, 19",
      "Computer Vision and Image Processing (Topic: Computer Vision) - Pages 3",
      "AI Research and Development (Topic: AI Research) - Pages 8"
    ],
    "has_evidence": true,
    "retrieval_stats": {
      "total_chunks": 10,
      "papers_searched": [
        "2505.19147v3.pdf",
        "2505.09388v1.pdf",
        "2505.11820v2.pdf",
        "583127822_872908961738101_8157213289131333559_n.pdf",
        "2505.10554v2.pdf"
      ],
      "by_paper": {
        "2505.19147v3.pdf": 1,
        "2505.09388v1.pdf": 5,
        "2505.11820v2.pdf": 2,
        "583127822_872908961738101_8157213289131333559_n.pdf": 1,
        "2505.10554v2.pdf": 1
      },
      "by_region_type": {
        "text": 7,
        "title": 2,
        "list": 1
      }
    },
    "cached_at": "2026-02-11T00:33:48.633014"
  },
  "How does the DiT transformer architecture work?": {
    "question": "How does the DiT transformer architecture work?",
    "answer": "The DiT transformer architecture is mentioned in several papers, but the most detailed explanation is found in [Evidence 3], which discusses the Hunyuan Foundation Model. According to this paper, the DiT model is a transformer architecture with 8.3 billion parameters, designed for multi-task learning. However, there is no further explanation of how the DiT architecture works in this paper.\n\nIn [Evidence 7], the DiT-2024 model is tested with the setting cache cy-, but there is no information provided on how the DiT architecture works.\n\nIt's worth noting that the DiT architecture is not explicitly explained in any of the provided evidence passages. However, [Evidence 3] mentions that the HunyuanVideo 1.5 model utilizes a video network to enhance visual quality, but this is not related to the DiT architecture.\n\nTherefore, the provided evidence does not contain sufficient information to answer this question.\n\n---\n**Evidence:**\n- Evidence 3 (from Foundation Models): Describes the DiT model as a transformer architecture with 8.3 billion parameters, designed for multi-task learning.\n- Evidence 7 (from Computer Vision): Tests the DiT-2024 model with the setting cache cy-, but does not provide information on how the DiT architecture works.\n\n**Sources:**\n- [Hunyuan Foundation Model] (Topic: Foundation Models) - Page 4\n- [Computer Vision and Image Processing] (Topic: Computer Vision) - Page 17",
    "evidence": [
      {
        "text": "requirements, we have made two major designs based on the Transformer architecture: some intermediate layers and then append the classifier to each as the sub-models; (b) we values are only produced by the first sub-model (the first few bottom layers), which are",
        "source": {
          "paper": "2505.11820v2.pdf",
          "page": 18,
          "region_type": "text",
          "region_id": "page18_region7"
        },
        "score": 0.358,
        "citation": "2505.11820v2.pdf, Page 18"
      },
      {
        "text": "AR Transformer+FM+Flow-VAE 0.901 0.782 2.231 0.709",
        "source": {
          "paper": "2505.07916v1.pdf",
          "page": 13,
          "region_type": "text",
          "region_id": "page13_region8"
        },
        "score": 0.353,
        "citation": "2505.07916v1.pdf, Page 13"
      },
      {
        "text": "transformer (DiT) model with 8.3 billion parameters, designed for multi-task learning. The hyperparameters of HunyuanVideo 1.5 are detailed in Subsequently, a video network is utilized to further enhance visual quality. Comprehensive elaborations",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 4,
          "region_type": "text",
          "region_id": "page4_region6"
        },
        "score": 0.35,
        "citation": "2511.18870v2.pdf, Page 4"
      },
      {
        "text": "T2V generation. However, these DPO frameworks mainly improve",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 10,
          "region_type": "title",
          "region_id": "page10_region7"
        },
        "score": 0.275,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 10"
      },
      {
        "text": "Strong-to-Weak Distillation Figure 1: Post-training pipeline of the Qwen3 series models. 14B/8B/4 Models Models 7B/0.6B",
        "source": {
          "paper": "2505.09388v1.pdf",
          "page": 9,
          "region_type": "text",
          "region_id": "page9_region1"
        },
        "score": 0.271,
        "citation": "2505.09388v1.pdf, Page 9"
      },
      {
        "text": "can formulate this as: z = . + 0 © €, where €e ~ N(0,1), 0 ~ N(0,C,). is a mirror-symmetric encoder-decoder structure. The encoder employs a hierarchical stages of modified Transformer blocks [VSP* 17] (using 1D depth-wise causal convo- of self-attention module) for efficient streaming processing. Six downsampling layers cumulative 3200X downsampling rate from a 24kHz input, yielding 7.5 tokens/frames Each encoder/decoder component has approximately 340M parameters. The training follows the DAC [KSL* 23], including its discriminator and loss designs. Tokenizer mirrors the hierarchical architecture of the Acoustic Tokenizer’s encoder, but components, as its objective is deterministic content-centric feature extraction. The is the training objective, which uses Automatic Speech Recognition (ASR) as the During training, its output is decoded by several Transformer decoder layers to predict",
        "source": {
          "paper": "2508.19205v1.pdf",
          "page": 3,
          "region_type": "text",
          "region_id": "page3_region0"
        },
        "score": 0.263,
        "citation": "2508.19205v1.pdf, Page 3"
      },
      {
        "text": "tested the DiT- 2024) with the setting cache cy-",
        "source": {
          "paper": "2505.19147v3.pdf",
          "page": 17,
          "region_type": "text",
          "region_id": "page17_region2"
        },
        "score": 0.244,
        "citation": "2505.19147v3.pdf, Page 17"
      },
      {
        "text": "above designs, Transformer can provide multiple capability across different models flexible deployment for various applications. However, in attention module, each preserves its unique set of keys and values, and cannot bridge the connections between scales. For example, when transitioning from a small language model (SLM) to a LLM it typically requires re-computing all keys and values for the preceding content. To",
        "source": {
          "paper": "2505.11820v2.pdf",
          "page": 6,
          "region_type": "text",
          "region_id": "page6_region3"
        },
        "score": 0.342,
        "citation": "2505.11820v2.pdf, Page 6"
      },
      {
        "text": "speedup, and allowing us to seamlessly switch LMs at different scales from CoLM for To the best of our knowledge, this is the first work to fulfill this functionality, and we term with KV sharing as CoLM-Air. More detailed implementation are in Appendix A.6.2. Function above designs of each component, the output feature 2 € R? of Transformer network to the CoR form £(a,). And finally, we apply a classification layer W € R?*Y, to dimension into the vocab size. Generally, we can adopt the cross entropy loss as the function for CoLM training. However, to enable multi-scale prediction, we also need to classification heads for each scale. Motivated by MRL [21], we propose a multi-chain loss to compute each scale as: Loss; = £(W'x<j), where W' € (Dx) XV ig W, and CL is the cross entropy function. Although the computations of different logits can be shared without any additional cost, calculating multiple cross-entropy losses will some computational overhead during the training. Due to resource limitations, we apply loss at the pre-training stage to accelerate convergence, and then fine-tune the model by proposed multi-chain cross-entropy loss.",
        "source": {
          "paper": "2505.11820v2.pdf",
          "page": 6,
          "region_type": "text",
          "region_id": "page6_region1"
        },
        "score": 0.287,
        "citation": "2505.11820v2.pdf, Page 6"
      },
      {
        "text": "Shazeer. GLU variants improve transformer. CoRR, abs/2002.05202, 2020. Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR, abs/1607.06450, 2016. Zhang and Rico Sennrich. Root mean square layer normalization. In Advances in Neural Processing Systems 32: Annual Conference on Neural Information Processing 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 12360- 2019. Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sanghai. GQA: training generalized multi-query transformer models from multi-head In Proceedings of the 2023 Conference on Empirical Methods in Natural Language EMNLP 2023, Singapore, December 6-10, 2023, pages 4895-4901, 2023. Kusupati, Gantavya Bhatt, Aniket Rege, Matthew Wallingford, Aditya Sinha, Vivek William Howard-Snyder, Kaifeng Chen, Sham M. Kakade, Prateek Jain, and Ali Matryoshka representation learning. In Advances in Neural Information Processing 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS New Orleans, LA, USA, November 28 - December 9, 2022, 2022. Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R_ Steeves, Hestness, and Nolan Dey. SlimPajama: A 627B_ token cleaned deduplicated version of RedPajama. https: //cerebras.ai/blog/ slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama, 2023. Liang, Tianyu Liu, Less Wright, Will Constable, Andrew Gu, Chien-Chin Huang, Iris Wei Feng, Howard Huang, Junjie Wang, Sanket Purandare, Gokul Nadathur, and Stratos Torchtitan: One-stop pytorch native solution for production ready LLM pre-training. abs/2410.06511, 2024. P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May Conference Track Proceedings, 2015.",
        "source": {
          "paper": "2505.11820v2.pdf",
          "page": 11,
          "region_type": "list",
          "region_id": "page11_region0"
        },
        "score": 0.281,
        "citation": "2505.11820v2.pdf, Page 11"
      }
    ],
    "sources": [
      "MiniMax-Speech: Autoregressive Text-to-Speech Model (Topic: Text-to-Speech (TTS) Generation) - Pages 13",
      "Research Paper on Machine Learning (Topic: Machine Learning) - Pages 9",
      "Natural Language Processing Study (Topic: Natural Language Processing) - Pages 6, 11, 18",
      "Computer Vision and Image Processing (Topic: Computer Vision) - Pages 17",
      "AI Model Optimization (Topic: Model Optimization) - Pages 3",
      "Hunyuan Foundation Model (Topic: Foundation Models) - Pages 4",
      "Text-to-Video Generation with Physical Consistency (Topic: Video Generation) - Pages 10"
    ],
    "has_evidence": true,
    "retrieval_stats": {
      "total_chunks": 10,
      "papers_searched": [
        "2505.11820v2.pdf",
        "2505.07916v1.pdf",
        "2511.18870v2.pdf",
        "605562216_1287553579874010_8068650067268945856_n.pdf",
        "2505.09388v1.pdf",
        "2508.19205v1.pdf",
        "2505.19147v3.pdf"
      ],
      "by_paper": {
        "2505.11820v2.pdf": 4,
        "2505.07916v1.pdf": 1,
        "2511.18870v2.pdf": 1,
        "605562216_1287553579874010_8068650067268945856_n.pdf": 1,
        "2505.09388v1.pdf": 1,
        "2508.19205v1.pdf": 1,
        "2505.19147v3.pdf": 1
      },
      "by_region_type": {
        "text": 8,
        "title": 1,
        "list": 1
      }
    },
    "cached_at": "2026-02-11T00:33:49.675131"
  },
  "What is the role of the Video Super-Resolution Network?": {
    "question": "What is the role of the Video Super-Resolution Network?",
    "answer": "The Video Super-Resolution Network plays a crucial role in enhancing the final visual fidelity of the video sequences produced by the Hunyuan Foundation Model. According to [Evidence 1], the second stage of the model utilizes a dedicated Video Super-Resolution Network to upscale 1080p, significantly enhancing the final visual fidelity.\n\nThis suggests that the primary function of the Video Super-Resolution Network is to improve the visual quality of the video output, making it more detailed and refined. The network's ability to upscale 1080p indicates that it is designed to handle high-resolution video processing, which is essential for producing high-fidelity video sequences.\n\nIt's worth noting that the Hunyuan Foundation Model's overall goal is to improve video perception quality, as stated in [Evidence 6]. The Video Super-Resolution Network is a key component in achieving this goal, as it enhances visual quality, motion stability, and overall temporal coherence, as mentioned in [Evidence 7].\n\nIn summary, the Video Super-Resolution Network is a critical component of the Hunyuan Foundation Model, responsible for enhancing the visual fidelity of the video output.\n\n---\n**Evidence:**\n- Evidence 1 (from Foundation Models): The second stage utilizes a dedicated Video Super-Resolution Network to upscale 1080p, significantly enhancing the final visual fidelity.\n- Evidence 6 (from Foundation Models): The Hunyuan Foundation Model aims to improve video perception quality.\n- Evidence 7 (from Foundation Models): The Video Super-Resolution Network enhances visual quality, motion stability, and overall temporal coherence.\n\n**Sources:**\n- Hunyuan Foundation Model (Topic: Foundation Models) - Pages 1, 4, 8\n- Computer Vision and Image Processing (Topic: Computer Vision) - Pages 2, 10, 13",
    "evidence": [
      {
        "text": "producing initial video sequences with resolutions from 480p to 720p and durations of 5 The second stage utilizes a dedicated Video Super-Resolution Network to upscale 1080p, significantly enhancing the final visual fidelity. The main contributions of this summarized as follows:",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 1,
          "region_type": "text",
          "region_id": "page1_region1"
        },
        "score": 0.699,
        "citation": "2511.18870v2.pdf, Page 1"
      },
      {
        "text": "Scardapane, and and effective compression. on Empiri- Processing, pages Association for Zang, Yuhang Songyang Zhang, Yining Li, and 1 A pioneer- handling resolutions in Neural Infor- 37:42566-42592. Hongyuan Zhu, of embodied ai: [EEE Transac- Computational Intelli- Chen, Xizhou Zhu, Hongsheng Li, Jifeng Vision-rwkv: Effi- with rwkv-like arXiv:2403.02308. Hoefler, and post-training transformers. Conference on Luo, Lei Li, Wang, Chenyu Zhang, and | oth- comprehensive Ilms in video Zhongmin Wang, driving secu- IEEE Internet of Laurence Gold- Jason Phang, Nabeshima, and 1 dataset of di- arXiv preprint Cheng, and diffusion transformer Proceedings of the Vision and Pat- Minjia Zhang, Model tells you",
        "source": {
          "paper": "2505.19147v3.pdf",
          "page": 10,
          "region_type": "list",
          "region_id": "page10_region0"
        },
        "score": 0.421,
        "citation": "2505.19147v3.pdf, Page 10"
      },
      {
        "text": "output distribution, enhancing the information expression ability of the encoder.",
        "source": {
          "paper": "2505.07916v1.pdf",
          "page": 6,
          "region_type": "text",
          "region_id": "page6_region3"
        },
        "score": 0.415,
        "citation": "2505.07916v1.pdf, Page 6"
      },
      {
        "text": "arXiv:2501.08453, 2025. Peng Gao, Le Zhuo, Dongyang Liu, Ruoyi Du, Xu Luo, Longtian Qiu, Yuhang Zhang, Chen Lin, Rongjie Huang, Shijie Geng, et al. Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers. arXiv preprint arXiv:2405.05945, 2024. Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Fei-Fei Li, Irfan Essa, Lu Jiang, and José Lezama. Photorealistic video generation with diffusion models. In ECCV, 2024. Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. arXiv preprint arXiv:2211.13221, 2022. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. NeurIPS, 2022. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In JCLR, 2022. Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video generative modeling. In ICLR, 2025. Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video generative modeling. In JCLR, 2025. Xuan Ju, Tianyu Wang, Yugian Zhou, He Zhang, Qing Liu, Nanxuan Zhao, Zhifei Zhang, Yijun Li, Yuanhao Cai, Shaoteng Liu, et al.",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 12,
          "region_type": "list",
          "region_id": "page12_region0"
        },
        "score": 0.352,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 12"
      },
      {
        "text": "and inference. In our CoM method, we can retain all keys and values at each layer, inference at multiple scales. Generally speaking, to abstract high-level features in neural usually matters more than width and and preserving invariant depth (i.e., layers) for 18",
        "source": {
          "paper": "2505.11820v2.pdf",
          "page": 18,
          "region_type": "text",
          "region_id": "page18_region0"
        },
        "score": 0.31,
        "citation": "2505.11820v2.pdf, Page 18"
      },
      {
        "text": "on overall video perception quality.",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 8,
          "region_type": "title",
          "region_id": "page8_region11"
        },
        "score": 0.475,
        "citation": "2511.18870v2.pdf, Page 8"
      },
      {
        "text": "enhances visual quality, motion stability, and overall temporal coherence.",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 8,
          "region_type": "text",
          "region_id": "page8_region5"
        },
        "score": 0.47,
        "citation": "2511.18870v2.pdf, Page 8"
      },
      {
        "text": "Xuyang Liu, Siteng Huang, Liangtao Shi, Xu, Yi Xin, Quanjun Yin, and Xiaohong Liu. Sparse-tuning: Adapting vision transform- with efficient fine-tuning and inference. arXiv arXiv:2405.14700. Liu, Yiyu Wang, Junpeng Ma, and Linfeng 2025a. Video compression commander: Plug- inference acceleration for video large lan- models. arXiv preprint arXiv:2505.14454. Liu, Ziming Wang, Yuhang Han, Yingyao Jiale Yuan, Jun Song, Bo Zheng, Linfeng Siteng Huang, and Honggang Chen. 2025b. compression commander: Plug-and-play in- acceleration for high-resolution large vision- models. arXiv preprint arXiv:2501.05179. Haodong Duan, Yuanhan Zhang, Bo Li, Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Conghui He, Ziwei Liu, and | others. 2025c. Is your multi-modal model an all-around In European Conference on Computer Vi- pages 216-233. Springer. Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zhang, Stephen Lin, and Baining Guo. 2021. transformer: Hierarchical vision transformer shifted windows. In Proceedings of the International Conference on Computer Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, Trevor Darrell. 2019. Rethinking the value of pruning. In Proceedings of the International on Learning Representations. Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai- Chang, Song-Chun Zhu, Oyvind Tafjord, Peter and Ashwin Kalyan. 2022. Learn to explain: reasoning via thought chains for science answering. Advances in Neural Information Systems, 35:2507-2521. Mahmud, Burhaneddin Yaman, Chun-Hao Liu, Diana Marculescu. 2024. Papr: Training-free patch pruning with lightweight convnets faster inference. In Proceedings of the Euro- Conference on Computer Vision, pages 110- Springer. 2024. Aime2024. https: //huggingface.co/datasets/Maxwell-Jia/ 2025. The llama 4 herd: The be- of a new era of natively multimodal innovation. https: //ai.meta.com/blog/ llama-4-multimodal-intelligence/. 2023. GPT-4 technical report. arXiv preprint arXiv:2303.08774. 13 preprint arXiv:2412.07626.",
        "source": {
          "paper": "2505.19147v3.pdf",
          "page": 13,
          "region_type": "list",
          "region_id": "page13_region0"
        },
        "score": 0.391,
        "citation": "2505.19147v3.pdf, Page 13"
      },
      {
        "text": "create higher-resolution images (Labs, and hour-long videos (Brooks et al., 2024), requiring more tokens and causing substantial computational overhead. Consequently, by late the primary bottleneck has clearly shifted to quadratic cost of the attention mechanism over extremely long context sequences. efficiency optimization. ¢ Unified Formulation of Model Efficiency: We establish a comprehensive mathematical formu- lation that unifies architectural design, model- centric compression, and data-centric compres- sion within a single expression. « Systematic Review of Data-centric Compres-",
        "source": {
          "paper": "2505.19147v3.pdf",
          "page": 2,
          "region_type": "text",
          "region_id": "page2_region3"
        },
        "score": 0.381,
        "citation": "2505.19147v3.pdf, Page 2"
      },
      {
        "text": "transformer (DiT) model with 8.3 billion parameters, designed for multi-task learning. The hyperparameters of HunyuanVideo 1.5 are detailed in Subsequently, a video network is utilized to further enhance visual quality. Comprehensive elaborations",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 4,
          "region_type": "text",
          "region_id": "page4_region6"
        },
        "score": 0.378,
        "citation": "2511.18870v2.pdf, Page 4"
      }
    ],
    "sources": [
      "MiniMax-Speech: Autoregressive Text-to-Speech Model (Topic: Text-to-Speech (TTS) Generation) - Pages 6",
      "Natural Language Processing Study (Topic: Natural Language Processing) - Pages 18",
      "Computer Vision and Image Processing (Topic: Computer Vision) - Pages 2, 10, 13",
      "Hunyuan Foundation Model (Topic: Foundation Models) - Pages 1, 4, 8",
      "Text-to-Video Generation with Physical Consistency (Topic: Video Generation) - Pages 12"
    ],
    "has_evidence": true,
    "retrieval_stats": {
      "total_chunks": 10,
      "papers_searched": [
        "2511.18870v2.pdf",
        "2505.19147v3.pdf",
        "2505.07916v1.pdf",
        "605562216_1287553579874010_8068650067268945856_n.pdf",
        "2505.11820v2.pdf"
      ],
      "by_paper": {
        "2511.18870v2.pdf": 4,
        "2505.19147v3.pdf": 3,
        "2505.07916v1.pdf": 1,
        "605562216_1287553579874010_8068650067268945856_n.pdf": 1,
        "2505.11820v2.pdf": 1
      },
      "by_region_type": {
        "text": 6,
        "list": 3,
        "title": 1
      }
    },
    "cached_at": "2026-02-11T00:33:55.379851"
  },
  "How are spatial resolution and temporal length scaled?": {
    "question": "How are spatial resolution and temporal length scaled?",
    "answer": "The provided evidence does not contain sufficient information to answer the question about how spatial resolution and temporal length are scaled.\n\n---\n**Evidence:**\n- Evidence 1 (from Foundation Models): Describes a two-stage process for video generation, including a video super-resolution network to upscale 1080p, but does not mention scaling of spatial resolution or temporal length.\n- Evidence 2 (from AI Research): Discusses the need for further empirical investigation to understand scaling dynamics, but does not provide specific information on spatial resolution or temporal length.\n- Evidence 3 (from Natural Language Processing Study): Presents a table with various model sizes and their corresponding parameters, but this is unrelated to spatial resolution or temporal length.\n- Evidence 4 (from Computer Vision and Image Processing): Discusses methods for reducing bias in spatial uniformity compression, but does not address scaling of spatial resolution or temporal length.\n- Evidence 5 (from Text-to-Speech Generation): Describes an autoregressive text-to-speech model, but this is unrelated to spatial resolution or temporal length.\n- Evidence 6 (from Video Generation): Lists various papers on video generation, but does not provide information on scaling of spatial resolution or temporal length.\n- Evidence 7 (from Foundation Models): Discusses the impact of the Hunyuan Foundation Model on overall video perception quality, but does not address scaling of spatial resolution or temporal length.\n- Evidence 8 (from Foundation Models): Presents training data and resolution for the Hunyuan Foundation Model, but does not provide information on scaling of spatial resolution or temporal length.\n- Evidence 9 (from Foundation Models): Describes the benefits of the Hunyuan Foundation Model, including enhanced visual quality and motion stability, but does not address scaling of spatial resolution or temporal length.\n- Evidence 10 (from Foundation Models): Presents a table with video generation results, including spatial resolution and temporal length, but does not provide information on how these are scaled.\n\n**Sources:**\n- Hunyuan Foundation Model (Topic: Foundation Models) - Pages referenced: 1, 7, 8, 11\n- AI Research and Development (Topic: AI Research) - Page referenced: 10\n- Natural Language Processing Study (Topic: Natural Language Processing) - Page referenced: 17\n- Computer Vision and Image Processing (Topic: Computer Vision) - Page referenced: 17\n- MiniMax-Speech: Autoregressive Text-to-Speech Model (Topic: Text-to-Speech Generation) - Page referenced: 6\n- Text-to-Video Generation with Physical Consistency (Topic: Video Generation) - Page referenced: 12",
    "evidence": [
      {
        "text": "producing initial video sequences with resolutions from 480p to 720p and durations of 5 The second stage utilizes a dedicated Video Super-Resolution Network to upscale 1080p, significantly enhancing the final visual fidelity. The main contributions of this summarized as follows:",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 1,
          "region_type": "text",
          "region_id": "page1_region1"
        },
        "score": 0.481,
        "citation": "2511.18870v2.pdf, Page 1"
      },
      {
        "text": "differences between the models involved. Further empirical investigation is needed to better understand scaling dynamics.",
        "source": {
          "paper": "583127822_872908961738101_8157213289131333559_n.pdf",
          "page": 10,
          "region_type": "text",
          "region_id": "page10_region5"
        },
        "score": 0.363,
        "citation": "583127822_872908961738101_8157213289131333559_n.pdf, Page 10"
      },
      {
        "text": "Length L D(Dx, ) 128K 256K 512K 1M 32 4096 (409) 36.0(21.6) 134.0(49.5) 512.0(106.0) 1920 (186.0)",
        "source": {
          "paper": "2505.11820v2.pdf",
          "page": 17,
          "region_type": "text",
          "region_id": "page17_region1"
        },
        "score": 0.334,
        "citation": "2505.11820v2.pdf, Page 17"
      },
      {
        "text": "dropping and methods. We of spatial uni- bias (Sec. 4) in This also high- bias in atten- spatial uniformity compression. We evalu- al., 2024a) on et al., 2024c), MLVU (Zhou",
        "source": {
          "paper": "2505.19147v3.pdf",
          "page": 17,
          "region_type": "text",
          "region_id": "page17_region6"
        },
        "score": 0.319,
        "citation": "2505.19147v3.pdf, Page 17"
      },
      {
        "text": "output distribution, enhancing the information expression ability of the encoder.",
        "source": {
          "paper": "2505.07916v1.pdf",
          "page": 6,
          "region_type": "text",
          "region_id": "page6_region3"
        },
        "score": 0.3,
        "citation": "2505.07916v1.pdf, Page 6"
      },
      {
        "text": "arXiv:2501.08453, 2025. Peng Gao, Le Zhuo, Dongyang Liu, Ruoyi Du, Xu Luo, Longtian Qiu, Yuhang Zhang, Chen Lin, Rongjie Huang, Shijie Geng, et al. Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers. arXiv preprint arXiv:2405.05945, 2024. Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Fei-Fei Li, Irfan Essa, Lu Jiang, and José Lezama. Photorealistic video generation with diffusion models. In ECCV, 2024. Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. arXiv preprint arXiv:2211.13221, 2022. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. NeurIPS, 2022. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In JCLR, 2022. Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video generative modeling. In ICLR, 2025. Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video generative modeling. In JCLR, 2025. Xuan Ju, Tianyu Wang, Yugian Zhou, He Zhang, Qing Liu, Nanxuan Zhao, Zhifei Zhang, Yijun Li, Yuanhao Cai, Shaoteng Liu, et al.",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 12,
          "region_type": "list",
          "region_id": "page12_region0"
        },
        "score": 0.286,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 12"
      },
      {
        "text": "on overall video perception quality.",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 8,
          "region_type": "title",
          "region_id": "page8_region11"
        },
        "score": 0.345,
        "citation": "2511.18870v2.pdf, Page 8"
      },
      {
        "text": "Stage Training Resolution Data Volumn Task Pretrain 256p 5 billion T21 Pretrain 512p 1 billion T21",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 7,
          "region_type": "text",
          "region_id": "page7_region2"
        },
        "score": 0.321,
        "citation": "2511.18870v2.pdf, Page 7"
      },
      {
        "text": "enhances visual quality, motion stability, and overall temporal coherence.",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 8,
          "region_type": "text",
          "region_id": "page8_region5"
        },
        "score": 0.319,
        "citation": "2511.18870v2.pdf, Page 8"
      },
      {
        "text": "720p (720 x 1280) 241 v 58.39 1.1679",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 11,
          "region_type": "text",
          "region_id": "page11_region6"
        },
        "score": 0.319,
        "citation": "2511.18870v2.pdf, Page 11"
      }
    ],
    "sources": [
      "MiniMax-Speech: Autoregressive Text-to-Speech Model (Topic: Text-to-Speech (TTS) Generation) - Pages 6",
      "Natural Language Processing Study (Topic: Natural Language Processing) - Pages 17",
      "Computer Vision and Image Processing (Topic: Computer Vision) - Pages 17",
      "Hunyuan Foundation Model (Topic: Foundation Models) - Pages 1, 7, 8, 11",
      "AI Research and Development (Topic: AI Research) - Pages 10",
      "Text-to-Video Generation with Physical Consistency (Topic: Video Generation) - Pages 12"
    ],
    "has_evidence": true,
    "retrieval_stats": {
      "total_chunks": 10,
      "papers_searched": [
        "2511.18870v2.pdf",
        "583127822_872908961738101_8157213289131333559_n.pdf",
        "2505.11820v2.pdf",
        "2505.19147v3.pdf",
        "2505.07916v1.pdf",
        "605562216_1287553579874010_8068650067268945856_n.pdf"
      ],
      "by_paper": {
        "2511.18870v2.pdf": 5,
        "583127822_872908961738101_8157213289131333559_n.pdf": 1,
        "2505.11820v2.pdf": 1,
        "2505.19147v3.pdf": 1,
        "2505.07916v1.pdf": 1,
        "605562216_1287553579874010_8068650067268945856_n.pdf": 1
      },
      "by_region_type": {
        "text": 8,
        "list": 1,
        "title": 1
      }
    },
    "cached_at": "2026-02-11T00:34:02.882158"
  },
  "What pre-training stages are used for the foundation model?": {
    "question": "What pre-training stages are used for the foundation model?",
    "answer": "The foundation model's pre-training stages are not explicitly mentioned in the provided evidence. However, we can infer some information about pre-training from other papers.\n\nAccording to [Evidence 1], the pre-training data is mentioned, but the details of the pre-training process are not specified. [Evidence 6] mentions that pre-training is used, but it does not provide any information about the stages involved.\n\nOn the other hand, [Evidence 8] discusses a model that employs a Large Language Model (LLM) as its core sequence model, integrated with audio encoding and diffusion-based decoding modules. This suggests that the model undergoes some form of pre-training, but the specific stages are not mentioned.\n\nIt's worth noting that [Evidence 5] is the title of a paper, but it does not provide any information about the pre-training stages of the Hunyuan Foundation Model.\n\nTherefore, the provided evidence does not contain sufficient information to answer the question about the pre-training stages used for the foundation model.\n\n---\n**Evidence:**\n- Evidence 1 (from Machine Learning): Discusses pre-training data, but does not specify the stages involved.\n- Evidence 6 (from Natural Language Processing Study): Mentions pre-training, but does not provide information about the stages.\n- Evidence 8 (from AI Model Optimization): Discusses a model that employs a Large Language Model (LLM) as its core sequence model, integrated with audio encoding and diffusion-based decoding modules, but does not specify the pre-training stages.\n- Evidence 5 (from Hunyuan Foundation Model): Is the title of a paper, but does not provide any information about the pre-training stages.\n\n**Sources:**\n- \"Doremi: Optimizing data mixtures speeds up language model pretraining\" (Topic: Machine Learning) - Pages 69798-69818\n- \"Effective long-context scaling of foundation models\" (Topic: Foundation Models) - Page 1\n- \"Qwen2 technical report\" (Topic: AI Research) - Page 2407.10671\n- \"Qwen2.5 technical report\" (Topic: AI Research) - Page 2412.15115\n- \"Qwen2.5-Math technical report: Toward mathematical expert model self-improvement\" (Topic: AI Research) - Page 2409.12122\n- \"Berkeley function calling leaderboard\" (Topic: AI Research) - Page 2024\n- \"Qwen2 technical report\" (Topic: AI Research) - Page 2407.10671\n- \"Qwen2.5 technical report\" (Topic: AI Research) - Page 2412.15115\n- \"Qwen2.5-Math technical report: Toward mathematical expert model self-improvement\" (Topic: AI Research) - Page 2409.12122\n- \"Berkeley function calling leaderboard\" (Topic: AI Research) - Page 2024\n- \"Hunyuan Foundation Model\" (Topic: Foundation Models) - Page 1\n- \"Doremi: Optimizing data mixtures speeds up language model pretraining\" (Topic: Machine Learning) - Pages 69798-69818\n- \"Effective long-context scaling of foundation models\" (Topic: Foundation Models) - Page 1\n- \"Qwen2 technical report\" (Topic: AI Research) - Page 2407.10671\n- \"Qwen2.5 technical report\" (Topic: AI Research) - Page 2412.15115\n- \"Qwen2.5-Math technical report: Toward mathematical expert model self-improvement\" (Topic: AI Research) - Page 2409.12122\n- \"Berkeley function calling leaderboard\" (Topic: AI Research) - Page 2024",
    "evidence": [
      {
        "text": "of our pretraining data, the details of our pretraining",
        "source": {
          "paper": "2505.09388v1.pdf",
          "page": 3,
          "region_type": "text",
          "region_id": "page3_region12"
        },
        "score": 0.575,
        "citation": "2505.09388v1.pdf, Page 3"
      },
      {
        "text": "an development leaking any in the model training practice: In this work, we have only tested the souping of the ’ final’ posttrained checkpoints. As other works have shown, the models can be souped after pre-training Li et al. (2025), after Wortsman et al. (2022), with adapters’. We do not recommend souping of the checkpoints from different as well as souping unaligned or uncensored models with the aligned ones to avoid the inheritance of",
        "source": {
          "paper": "583127822_872908961738101_8157213289131333559_n.pdf",
          "page": 10,
          "region_type": "text",
          "region_id": "page10_region3"
        },
        "score": 0.499,
        "citation": "583127822_872908961738101_8157213289131333559_n.pdf, Page 10"
      },
      {
        "text": "With this schedule, the 7B model converges by Level at higher levels, so we restrict training to Levels 1-2.",
        "source": {
          "paper": "2505.10554v2.pdf",
          "page": 6,
          "region_type": "title",
          "region_id": "page6_region8"
        },
        "score": 0.445,
        "citation": "2505.10554v2.pdf, Page 6"
      },
      {
        "text": "param- models, these cri- main approaches: auxiliary net- X + {s:}7_), A@ beyond the methods in- (Rao et al., 2024d; Kim et al., training to learn {s,}_,, and (ii) et al., 2024; employ pre-trained : KX + utilize parameter- without introduc- approaches can be computation meth- al., 2025; Chen Ge et al., 2024) calculations for to- such as using where a; repre- and (ii) ex- et al., 2023a;",
        "source": {
          "paper": "2505.19147v3.pdf",
          "page": 4,
          "region_type": "text",
          "region_id": "page4_region9"
        },
        "score": 0.443,
        "citation": "2505.19147v3.pdf, Page 4"
      },
      {
        "text": "Hunyuan Foundation Model Team",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 1,
          "region_type": "title",
          "region_id": "page1_region3"
        },
        "score": 0.419,
        "citation": "2511.18870v2.pdf, Page 1"
      },
      {
        "text": "used in pretraining.",
        "source": {
          "paper": "2505.11820v2.pdf",
          "page": 14,
          "region_type": "text",
          "region_id": "page14_region7"
        },
        "score": 0.419,
        "citation": "2505.11820v2.pdf, Page 14"
      },
      {
        "text": "is responsible for updating the model weights, while the rollout captures used by the trainer. For single-turn RL tasks, such as mathematical reasoning, the",
        "source": {
          "paper": "2508.03680v1.pdf",
          "page": 10,
          "region_type": "title",
          "region_id": "page10_region7"
        },
        "score": 0.395,
        "citation": "2508.03680v1.pdf, Page 10"
      },
      {
        "text": "after pre-training. employs a Large Language Model (LLM) as its core sequence model, integrated with audio encoding and diffusion-based decoding modules to achieve scalable, high-fidelity speech synthesis. The overall inference architecture is depicted in Figure 2.",
        "source": {
          "paper": "2508.19205v1.pdf",
          "page": 3,
          "region_type": "text",
          "region_id": "page3_region1"
        },
        "score": 0.354,
        "citation": "2508.19205v1.pdf, Page 3"
      },
      {
        "text": "Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy S Liang, Quoc V Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up language model pretraining. Advances in Neural Information Processing Systems, 36:69798-69818, 2023. Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, Hao Ma. Effective long-context scaling of foundation models. CoRR, abs/2309.16039, 2023. Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G. Patil, lon Stoica, and Joseph E. Gonzalez. Berkeley function calling leaderboard. https: //gorilla.cs.berkeley.edu/blogs/8_ber keley_function_calling 2024. Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report. CoRR, abs/2407.10671, 2024a. Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024b. Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Zhou, Junyang Lin, et al. Qwen2.5-Math technical report: Toward mathematical expert model self-improvement. CoRR, abs/2409.12122, 2024c.",
        "source": {
          "paper": "2505.09388v1.pdf",
          "page": 35,
          "region_type": "list",
          "region_id": "page35_region0"
        },
        "score": 0.416,
        "citation": "2505.09388v1.pdf, Page 35"
      },
      {
        "text": "chain-of-model across the depth (i.e., layers) of the language model. Here, when applying model depth, we also require it to meet two characteristics to ensure the alignment with our current version (i.e., CoLM):",
        "source": {
          "paper": "2505.11820v2.pdf",
          "page": 18,
          "region_type": "text",
          "region_id": "page18_region2"
        },
        "score": 0.39,
        "citation": "2505.11820v2.pdf, Page 18"
      }
    ],
    "sources": [
      "Research Paper on Machine Learning (Topic: Machine Learning) - Pages 3, 35",
      "Multi-Modal AI Research (Topic: Multi-Modal Learning) - Pages 6",
      "Natural Language Processing Study (Topic: Natural Language Processing) - Pages 14, 18",
      "Computer Vision and Image Processing (Topic: Computer Vision) - Pages 4",
      "Deep Learning Architectures (Topic: Deep Learning) - Pages 10",
      "AI Model Optimization (Topic: Model Optimization) - Pages 3",
      "Hunyuan Foundation Model (Topic: Foundation Models) - Pages 1",
      "AI Research and Development (Topic: AI Research) - Pages 10"
    ],
    "has_evidence": true,
    "retrieval_stats": {
      "total_chunks": 10,
      "papers_searched": [
        "2505.09388v1.pdf",
        "583127822_872908961738101_8157213289131333559_n.pdf",
        "2505.10554v2.pdf",
        "2505.19147v3.pdf",
        "2511.18870v2.pdf",
        "2505.11820v2.pdf",
        "2508.03680v1.pdf",
        "2508.19205v1.pdf"
      ],
      "by_paper": {
        "2505.09388v1.pdf": 2,
        "583127822_872908961738101_8157213289131333559_n.pdf": 1,
        "2505.10554v2.pdf": 1,
        "2505.19147v3.pdf": 1,
        "2511.18870v2.pdf": 1,
        "2505.11820v2.pdf": 2,
        "2508.03680v1.pdf": 1,
        "2508.19205v1.pdf": 1
      },
      "by_region_type": {
        "text": 6,
        "title": 3,
        "list": 1
      }
    },
    "cached_at": "2026-02-11T00:34:16.128665"
  },
  "How is training data quality ensured in video generation?": {
    "question": "How is training data quality ensured in video generation?",
    "answer": "To ensure training data quality in video generation, the Hunyuan Foundation Model employs a multi-stage filtering process [Evidence 1]. Initially, videos are filtered based on their aesthetic scores, discarding those with low scores [Evidence 1]. This is followed by a series of filtering stages, including the removal of corrupted files and deduplication of raw video data [Evidence 8]. The model also utilizes a high-quality video dataset, sourced from various channels, to ensure diversity in content, filming techniques, camera movements, styles, and more [Evidence 8]. Furthermore, the model's training data is segmented into consistent clips ranging from 2 to 10 seconds to optimize training efficiency [Evidence 8]. Additionally, the model's training pipeline includes a comprehensive post-training stage to further enhance generative quality and task alignment with human preferences [Evidence 10].\n\nThe Hunyuan Foundation Model also utilizes Direct Preference Optimization (DPO) to optimize the model's performance on fine-grained motion quality [Evidence 6]. This involves an offline optimization stage using DPO, where a balanced prompt set is used to generate video candidates per prompt, creating non-repetitive pairs [Evidence 6]. The model also employs Reinforcement Learning from Human Feedback (RLHF) strategies tailored for T2V tasks, mainly aimed at correcting artifacts and enhancing motion quality [Evidence 7].\n\nIn terms of data acquisition, the model sources raw videos from a variety of channels, ensuring coverage across diverse content, filming techniques, camera movements, styles, and more [Evidence 8]. The model also utilizes PySceneDetect to segment raw videos into consistent clips ranging from 2 to 10 seconds [Evidence 8].\n\nOverall, the Hunyuan Foundation Model's training data quality is ensured through a combination of filtering processes, diverse data sourcing, and optimization techniques.\n\n---\n**Evidence:**\n- Evidence 1 (from Foundation Models): Filtering process to discard clips with poor visual quality and low aesthetic scores.\n- Evidence 2 (from Computer Vision): Not relevant to the question.\n- Evidence 3 (from Video Generation): Not relevant to the question.\n- Evidence 4 (from Foundation Models): Not relevant to the question.\n- Evidence 5 (from Foundation Models): Overview of the Hunyuan Foundation Model's capabilities.\n- Evidence 6 (from Foundation Models): Direct Preference Optimization (DPO) for fine-grained motion quality.\n- Evidence 7 (from Foundation Models): Reinforcement Learning from Human Feedback (RLHF) strategies for T2V tasks.\n- Evidence 8 (from Foundation Models): Data acquisition and filtering processes.\n- Evidence 9 (from Foundation Models): Training optimization and multimodal understanding.\n- Evidence 10 (from Foundation Models): Comprehensive post-training pipeline for generative quality and task alignment.\n\n**Sources:**\n- \"Hunyuan Foundation Model\" (Topic: Foundation Models) - Pages 1-10\n- \"Computer Vision and Image Processing\" (Topic: Computer Vision) - Page 10\n- \"Text-to-Video Generation with Physical Consistency\" (Topic: Video Generation) - Page 13\n- \"Hunyuan Foundation Model\" (Topic: Foundation Models) - Pages 1-10",
    "evidence": [
      {
        "text": "videos across four dimensions: sharpness, detail retention, noise & artifacts, and This allows us to discard clips with poor visual quality. Filtering: We applied an aesthetic scoring operator based on to evaluate the of the videos, filtering out those with low aesthetic scores. these filtering stages, approximately 800 million high-quality video segments play a crucial role in video generation, significantly influencing precise",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 3,
          "region_type": "list",
          "region_id": "page3_region2"
        },
        "score": 0.547,
        "citation": "2511.18870v2.pdf, Page 3"
      },
      {
        "text": "Scardapane, and and effective compression. on Empiri- Processing, pages Association for Zang, Yuhang Songyang Zhang, Yining Li, and 1 A pioneer- handling resolutions in Neural Infor- 37:42566-42592. Hongyuan Zhu, of embodied ai: [EEE Transac- Computational Intelli- Chen, Xizhou Zhu, Hongsheng Li, Jifeng Vision-rwkv: Effi- with rwkv-like arXiv:2403.02308. Hoefler, and post-training transformers. Conference on Luo, Lei Li, Wang, Chenyu Zhang, and | oth- comprehensive Ilms in video Zhongmin Wang, driving secu- IEEE Internet of Laurence Gold- Jason Phang, Nabeshima, and 1 dataset of di- arXiv preprint Cheng, and diffusion transformer Proceedings of the Vision and Pat- Minjia Zhang, Model tells you",
        "source": {
          "paper": "2505.19147v3.pdf",
          "page": 10,
          "region_type": "list",
          "region_id": "page10_region0"
        },
        "score": 0.493,
        "citation": "2505.19147v3.pdf, Page 10"
      },
      {
        "text": "CVPR, 2024. Fanqing Meng, Jiaqi Liao, Xinyu Tan, Wenqi Shao, Quanfeng Lu, Kaipeng Zhang, Yu Cheng, Dianqi Li, Yu Qiao, and Ping Luo. Towards world simulator: Crafting physical commonsense-based benchmark for video generation. In ICML, 2025. Sanghyeon Na, Yonggyu Kim, and Hyunjoon Lee. Boost your human image generation model via direct preference optimization. In CVPR, 2025. Sharan Narang, Gregory Diamos, Erich Elsen, Paulius Micikevicius, Jonah Alben, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. In ICLR, 2018. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019. William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. Pika. Pika art. https://pika.art/. Accessed: 2025-05-04. Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et al. Movie gen: A cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. In NeurIPS, 2023. Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In EMNLP, 2019. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseeckmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Dani Valevski, Yaniv Leviathan, Moab Arar, and Shlomi Fruchter. Diffusion models are real-time game engines. In ICLR, 2025. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need.",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 13,
          "region_type": "list",
          "region_id": "page13_region0"
        },
        "score": 0.491,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 13"
      },
      {
        "text": "on overall video perception quality.",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 8,
          "region_type": "title",
          "region_id": "page8_region11"
        },
        "score": 0.545,
        "citation": "2511.18870v2.pdf, Page 8"
      },
      {
        "text": "and proficient model establishes a new state-of-the-art among generation models. By releasing the code and model weights, community with a high-performance foundation that lowers the creation and research, making advanced video generation ac- broader audience. All open-source assets are publicly available at //github.com/Tencent-Hunyuan/HunyuanVideo-1.5 remarkable advancements in video generation models, with closed- Kling2.5 [I], Veo3.1 [2], and Sora2 achieving state-of-the-art (SOTA) open-source domain, models such as HunyuanVideo [4], StepVideo [5], and as notable contenders. However, despite these strides, most SOTA models accessibility and community-driven innovation. Wan2.2 [6] employs a (MoE) architecture that leverages two 14-billion-parameter expert models Although this MoE architecture enhances visual fidelity by leveraging different denoising stages, it inherently introduces computational inefficiencies, multiple large parameter sets (totaling 27B parameters with 14B activated), demands. While Wan2.2 [6] has attempted to address this with a more variant, which utilizes a high-compression 3D VAE to reduce memory model still exhibits limitations. Specifically, its generative capabilities, motion stability across frames and achieving the nuanced aesthetic quality applications, continue to fall short of practical demands. This situation",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 1,
          "region_type": "text",
          "region_id": "page1_region2"
        },
        "score": 0.539,
        "citation": "2511.18870v2.pdf, Page 1"
      },
      {
        "text": "models struggle to effectively differentiate fine-grained motion quality. Therefore, an offline optimization stage using Direct Preference Optimization (DPO). For a balanced prompt set (from LLMs-generated prompts and training video covering diverse dimensions (motion, scene, subject, etc.). Using a selected high-quality we generate N video candidates per prompt to create non-repetitive pairs. These pairs",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 7,
          "region_type": "text",
          "region_id": "page7_region6"
        },
        "score": 0.518,
        "citation": "2511.18870v2.pdf, Page 7"
      },
      {
        "text": "checkpoint, enabling upon a high-quality SFT, we utilize rigorously selected clips per task, filtered strictly considering aesthetic and motion smoothness. Through CT and SFT, we observe evident improvements in visual quality, aesthetic appeal, and temporal consistency for both generation tasks. employ Reinforcement Learning from Human Feedback (RLHF) strategies tailored for T2V tasks, both mainly aimed at correcting artifacts and enhancing motion quality. task, we apply online reinforcement learning (RL) during post-training to correct",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 7,
          "region_type": "text",
          "region_id": "page7_region1"
        },
        "score": 0.501,
        "citation": "2511.18870v2.pdf, Page 7"
      },
      {
        "text": "significantly expanded the data volume while refining the filtering mechanisms to enhance specifics of these processes are detailed below. To continuously improve the model’s performance, we prioritized diversity during data acquisition. We sourced raw videos from a variety of channels, ensuring coverage across diverse content, filming techniques, camera movements, styles, and basic deduplication and the removal of corrupted files, we obtained more than 10 of raw video data. high variance in raw video lengths and optimize training efficiency, we segmented consistent clips ranging from 2 to 10 seconds. Specifically, we utilized PySceneDe-",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 2,
          "region_type": "text",
          "region_id": "page2_region1"
        },
        "score": 0.499,
        "citation": "2511.18870v2.pdf, Page 2"
      },
      {
        "text": "parameter count while fully leveraging the potential of the model, yielding comparable to state-of-the-art video generation models. Super-Resolution Enhancement: We develop an efficient few-step super-resolution upscales outputs to 1080p. It enhances sharpness while correcting distortions, details and overall visual texture. Optimization: We introduce a novel Selective and Sliding Tile Attention mechanism that dynamically prunes redundant spatiotemporal tokens. This signifi- computational overhead for long video sequences and accelerates inference, end-to-end speedup of 1.87x in 10-second 720p video synthesis compared to [7]. Multimodal Understanding: Our framework utilizes a large multimodal model bilingual (Chinese-English) understanding, combined with ByT5 for dedicated to enhance text generation accuracy in videos. Additionally, detailed captions are generated for both images and videos. Training Optimization: We demonstrate that the Muon optimizer [8] signifi- convergence in video generation model training, while the multi-phase training strategy—spanning from pre-training to post-training stages—enhances aesthetic quality, and alignment with human preferences, thereby enabling of professional-grade content. comprehensive pipeline, beginning with meticulous data preparation—including captioning, as detailed in Section|2| Section [3]introduces the core architecture such as the unified diffusion transformer, the video super-resolution network, sparse attention. Section |4|describes training strategies for text-to-video and covering both pre-training and post-training methodologies. Finally, Section[5| evaluation of the model’s performance against state-of-the-art models using qualitative benchmarks. and Filtering comprises both image and video data. For image data, we adopted the acquisition outlined in [9], curating 5 billion images from a pool of over 10 billion for",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 2,
          "region_type": "list",
          "region_id": "page2_region4"
        },
        "score": 0.494,
        "citation": "2511.18870v2.pdf, Page 2"
      },
      {
        "text": "video durations ranging from 2 to 10 seconds. This structured scaling of spatiotemporal facilitates stable convergence and strengthens the model’s capacity for detailed, coherent Similar to prior studies [18], we observe that flow matching-based training is sensitive to the shift hyper-parameter when video token lengths vary across stages. To training stability under such varying sequence conditions, we carefully design a series of strategies that adapt to different token lengths during the progressive training process. comprehensive post-training pipeline to further enhance the generative quality and task-",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 6,
          "region_type": "text",
          "region_id": "page6_region1"
        },
        "score": 0.488,
        "citation": "2511.18870v2.pdf, Page 6"
      }
    ],
    "sources": [
      "Computer Vision and Image Processing (Topic: Computer Vision) - Pages 10",
      "Hunyuan Foundation Model (Topic: Foundation Models) - Pages 1, 2, 3, 6, 7, 8",
      "Text-to-Video Generation with Physical Consistency (Topic: Video Generation) - Pages 13"
    ],
    "has_evidence": true,
    "retrieval_stats": {
      "total_chunks": 10,
      "papers_searched": [
        "2511.18870v2.pdf",
        "2505.19147v3.pdf",
        "605562216_1287553579874010_8068650067268945856_n.pdf"
      ],
      "by_paper": {
        "2511.18870v2.pdf": 8,
        "2505.19147v3.pdf": 1,
        "605562216_1287553579874010_8068650067268945856_n.pdf": 1
      },
      "by_region_type": {
        "list": 4,
        "title": 1,
        "text": 5
      }
    },
    "cached_at": "2026-02-11T00:35:49.798565"
  },
  "What filtering mechanisms are applied to raw video data?": {
    "question": "What filtering mechanisms are applied to raw video data?",
    "answer": "The Hunyuan Foundation Model applies two filtering mechanisms to raw video data. Firstly, it uses an aesthetic scoring operator to evaluate the quality of the videos, filtering out those with low aesthetic scores [Evidence 1]. This is done to discard clips with poor visual quality. Secondly, it filters out corrupted files and removes duplicates, obtaining more than 10 million raw video data [Evidence 7].\n\nAdditionally, the model segments consistent clips ranging from 2 to 10 seconds to optimize training efficiency [Evidence 7]. This suggests that the model uses a combination of quality evaluation, corruption removal, and clip segmentation to filter and prepare the raw video data for training.\n\n---\n**Evidence:**\n- Evidence 1 (from Foundation Models): The Hunyuan Foundation Model uses an aesthetic scoring operator to evaluate the quality of videos and filter out those with low aesthetic scores.\n- Evidence 7 (from Foundation Models): The model filters out corrupted files and removes duplicates, obtaining more than 10 million raw video data, and segments consistent clips ranging from 2 to 10 seconds to optimize training efficiency.\n\n**Sources:**\n- \"Hunyuan Foundation Model\" (Topic: Foundation Models) - Pages 3, 7, 8, 9\n- \"MiniMax-Speech: Autoregressive Text-to-Speech Model\" (Topic: Text-to-Speech (TTS) Generation) - Pages 3, 6\n- \"Text-to-Video Generation with Physical Consistency\" (Topic: Video Generation) - Page 4\n- \"AI Model Optimization\" (Topic: Model Optimization) - Page 1\n- \"Hunyuan Foundation Model\" (Topic: Foundation Models) - Pages 1, 2, 8, 9",
    "evidence": [
      {
        "text": "videos across four dimensions: sharpness, detail retention, noise & artifacts, and This allows us to discard clips with poor visual quality. Filtering: We applied an aesthetic scoring operator based on to evaluate the of the videos, filtering out those with low aesthetic scores. these filtering stages, approximately 800 million high-quality video segments play a crucial role in video generation, significantly influencing precise",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 3,
          "region_type": "list",
          "region_id": "page3_region2"
        },
        "score": 0.456,
        "citation": "2511.18870v2.pdf, Page 3"
      },
      {
        "text": "on its robust zero-shot capabilities. As shown in Figure 1, it primarily comprises three a tokenizer, an autoregressive Transformer, and a latent flow matching model, which of flow matching module and Flow-VAE module. The text tokenizer utilizes Byte Pair (BPE), while the audio tokenizer employs Encoder-VQ-Decoder architecture (Betker, 2023; Oord et al., 2017) quantization on mel-spectrograms with a rate of 25 tokens per second",
        "source": {
          "paper": "2505.07916v1.pdf",
          "page": 3,
          "region_type": "list",
          "region_id": "page3_region1"
        },
        "score": 0.365,
        "citation": "2505.07916v1.pdf, Page 3"
      },
      {
        "text": "Scardapane, and and effective compression. on Empiri- Processing, pages Association for Zang, Yuhang Songyang Zhang, Yining Li, and 1 A pioneer- handling resolutions in Neural Infor- 37:42566-42592. Hongyuan Zhu, of embodied ai: [EEE Transac- Computational Intelli- Chen, Xizhou Zhu, Hongsheng Li, Jifeng Vision-rwkv: Effi- with rwkv-like arXiv:2403.02308. Hoefler, and post-training transformers. Conference on Luo, Lei Li, Wang, Chenyu Zhang, and | oth- comprehensive Ilms in video Zhongmin Wang, driving secu- IEEE Internet of Laurence Gold- Jason Phang, Nabeshima, and 1 dataset of di- arXiv preprint Cheng, and diffusion transformer Proceedings of the Vision and Pat- Minjia Zhang, Model tells you",
        "source": {
          "paper": "2505.19147v3.pdf",
          "page": 10,
          "region_type": "list",
          "region_id": "page10_region0"
        },
        "score": 0.345,
        "citation": "2505.19147v3.pdf, Page 10"
      },
      {
        "text": "the real video as the winning case xf’ because it always follows physical laws and a set of generated videos as",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 4,
          "region_type": "text",
          "region_id": "page4_region3"
        },
        "score": 0.327,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 4"
      },
      {
        "text": "preserves audio fidelity",
        "source": {
          "paper": "2508.19205v1.pdf",
          "page": 1,
          "region_type": "text",
          "region_id": "page1_region3"
        },
        "score": 0.29,
        "citation": "2508.19205v1.pdf, Page 1"
      },
      {
        "text": "on overall video perception quality.",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 8,
          "region_type": "title",
          "region_id": "page8_region11"
        },
        "score": 0.434,
        "citation": "2511.18870v2.pdf, Page 8"
      },
      {
        "text": "significantly expanded the data volume while refining the filtering mechanisms to enhance specifics of these processes are detailed below. To continuously improve the model’s performance, we prioritized diversity during data acquisition. We sourced raw videos from a variety of channels, ensuring coverage across diverse content, filming techniques, camera movements, styles, and basic deduplication and the removal of corrupted files, we obtained more than 10 of raw video data. high variance in raw video lengths and optimize training efficiency, we segmented consistent clips ranging from 2 to 10 seconds. Specifically, we utilized PySceneDe-",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 2,
          "region_type": "text",
          "region_id": "page2_region1"
        },
        "score": 0.37,
        "citation": "2511.18870v2.pdf, Page 2"
      },
      {
        "text": "output distribution, enhancing the information expression ability of the encoder.",
        "source": {
          "paper": "2505.07916v1.pdf",
          "page": 6,
          "region_type": "text",
          "region_id": "page6_region3"
        },
        "score": 0.334,
        "citation": "2505.07916v1.pdf, Page 6"
      },
      {
        "text": "producing initial video sequences with resolutions from 480p to 720p and durations of 5 The second stage utilizes a dedicated Video Super-Resolution Network to upscale 1080p, significantly enhancing the final visual fidelity. The main contributions of this summarized as follows:",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 1,
          "region_type": "text",
          "region_id": "page1_region1"
        },
        "score": 0.334,
        "citation": "2511.18870v2.pdf, Page 1"
      },
      {
        "text": "and proficient model establishes a new state-of-the-art among generation models. By releasing the code and model weights, community with a high-performance foundation that lowers the creation and research, making advanced video generation ac- broader audience. All open-source assets are publicly available at //github.com/Tencent-Hunyuan/HunyuanVideo-1.5 remarkable advancements in video generation models, with closed- Kling2.5 [I], Veo3.1 [2], and Sora2 achieving state-of-the-art (SOTA) open-source domain, models such as HunyuanVideo [4], StepVideo [5], and as notable contenders. However, despite these strides, most SOTA models accessibility and community-driven innovation. Wan2.2 [6] employs a (MoE) architecture that leverages two 14-billion-parameter expert models Although this MoE architecture enhances visual fidelity by leveraging different denoising stages, it inherently introduces computational inefficiencies, multiple large parameter sets (totaling 27B parameters with 14B activated), demands. While Wan2.2 [6] has attempted to address this with a more variant, which utilizes a high-compression 3D VAE to reduce memory model still exhibits limitations. Specifically, its generative capabilities, motion stability across frames and achieving the nuanced aesthetic quality applications, continue to fall short of practical demands. This situation",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 1,
          "region_type": "text",
          "region_id": "page1_region2"
        },
        "score": 0.313,
        "citation": "2511.18870v2.pdf, Page 1"
      }
    ],
    "sources": [
      "MiniMax-Speech: Autoregressive Text-to-Speech Model (Topic: Text-to-Speech (TTS) Generation) - Pages 3, 6",
      "Computer Vision and Image Processing (Topic: Computer Vision) - Pages 10",
      "AI Model Optimization (Topic: Model Optimization) - Pages 1",
      "Hunyuan Foundation Model (Topic: Foundation Models) - Pages 1, 2, 3, 8",
      "Text-to-Video Generation with Physical Consistency (Topic: Video Generation) - Pages 4"
    ],
    "has_evidence": true,
    "retrieval_stats": {
      "total_chunks": 10,
      "papers_searched": [
        "2511.18870v2.pdf",
        "2505.07916v1.pdf",
        "2505.19147v3.pdf",
        "605562216_1287553579874010_8068650067268945856_n.pdf",
        "2508.19205v1.pdf"
      ],
      "by_paper": {
        "2511.18870v2.pdf": 5,
        "2505.07916v1.pdf": 2,
        "2505.19147v3.pdf": 1,
        "605562216_1287553579874010_8068650067268945856_n.pdf": 1,
        "2508.19205v1.pdf": 1
      },
      "by_region_type": {
        "list": 3,
        "text": 6,
        "title": 1
      }
    },
    "cached_at": "2026-02-11T00:35:56.474189"
  },
  "How are aesthetic scores used to evaluate videos?": {
    "question": "How are aesthetic scores used to evaluate videos?",
    "answer": "The Hunyuan Foundation Model uses aesthetic scores to evaluate videos by applying an aesthetic scoring operator based on the visual quality of the videos, filtering out those with low aesthetic scores [Evidence 1]. This filtering stage is crucial in video generation, as it allows the model to discard clips with poor visual quality and focus on high-quality video segments. The aesthetic scores are used to evaluate the videos across four dimensions: sharpness, detail retention, noise & artifacts, and aesthetic quality [Evidence 1].\n\nThe aesthetic scores are calculated to evaluate the visual quality of the videos, which is a key aspect of video generation. The model uses a comprehensive rating system that considers five key dimensions: text-video consistency, aesthetic quality of individual quality, structural stability, and motion effects [Evidence 7]. This rating system is used to assess the overall video perception quality [Evidence 6].\n\nIn the Hunyuan Foundation Model, the aesthetic scores are used in conjunction with other evaluation metrics, such as text alignment, visual quality, and motion dynamics, to evaluate the videos [Evidence 8]. The model uses a hybrid offline-then-online approach to address the increased complexity of T2V motion artifacts, and the aesthetic scores play a crucial role in this process.\n\nOverall, the aesthetic scores are a critical component of the Hunyuan Foundation Model's evaluation process, allowing the model to focus on high-quality video segments and improve the overall visual quality of the generated videos.\n\n---\n**Evidence:**\n- Evidence 1 (from Foundation Models): The Hunyuan Foundation Model uses aesthetic scores to evaluate videos by applying an aesthetic scoring operator based on the visual quality of the videos, filtering out those with low aesthetic scores.\n- Evidence 7 (from Foundation Models): The aesthetic scores are used to evaluate the videos across four dimensions: text-video consistency, aesthetic quality of individual quality, structural stability, and motion effects.\n- Evidence 6 (from Foundation Models): The aesthetic scores are used to evaluate the videos across four dimensions: text-video consistency, aesthetic quality of individual quality, structural stability, and motion effects.\n- Evidence 8 (from Foundation Models): The aesthetic scores are used in conjunction with other evaluation metrics, such as text alignment, visual quality, and motion dynamics, to evaluate the videos.\n\n**Sources:**\n- \"Hunyuan Foundation Model\" (Topic: Foundation Models) - Pages 3, 7, 8\n- \"Hunyuan Foundation Model\" (Topic: Foundation Models) - Pages 7, 8\n- \"Hunyuan Foundation Model\" (Topic: Foundation Models) - Page 3\n- \"Hunyuan Foundation Model\" (Topic: Foundation Models) - Page 7",
    "evidence": [
      {
        "text": "videos across four dimensions: sharpness, detail retention, noise & artifacts, and This allows us to discard clips with poor visual quality. Filtering: We applied an aesthetic scoring operator based on to evaluate the of the videos, filtering out those with low aesthetic scores. these filtering stages, approximately 800 million high-quality video segments play a crucial role in video generation, significantly influencing precise",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 3,
          "region_type": "list",
          "region_id": "page3_region2"
        },
        "score": 0.679,
        "citation": "2511.18870v2.pdf, Page 3"
      },
      {
        "text": "evaluation, we recruited 24 human annotators to provide Mean Opinion Scores (MOS) dimensions: Realism (how natural and human-like the speech sounds, including prosody, the smoothness of speaker turns), Richness (the expressiveness of the speech in terms emotion, including variation and adaptation to context), and Preference (overall listener",
        "source": {
          "paper": "2508.19205v1.pdf",
          "page": 4,
          "region_type": "text",
          "region_id": "page4_region11"
        },
        "score": 0.402,
        "citation": "2508.19205v1.pdf, Page 4"
      },
      {
        "text": "To facilitate this, we have each with customized scoring",
        "source": {
          "paper": "2505.09388v1.pdf",
          "page": 12,
          "region_type": "title",
          "region_id": "page12_region8"
        },
        "score": 0.383,
        "citation": "2505.09388v1.pdf, Page 12"
      },
      {
        "text": "reports the human evaluators than all SOTA methods.",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 9,
          "region_type": "text",
          "region_id": "page9_region10"
        },
        "score": 0.369,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 9"
      },
      {
        "text": "across multiple dimensions. evaluation. Our main analysis",
        "source": {
          "paper": "2505.07916v1.pdf",
          "page": 7,
          "region_type": "title",
          "region_id": "page7_region7"
        },
        "score": 0.321,
        "citation": "2505.07916v1.pdf, Page 7"
      },
      {
        "text": "on overall video perception quality.",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 8,
          "region_type": "title",
          "region_id": "page8_region11"
        },
        "score": 0.543,
        "citation": "2511.18870v2.pdf, Page 8"
      },
      {
        "text": "TableB]and Table/4] we assess text-to-video generation using a comprehensive rating that considers five key dimensions: text-video consistency, aesthetic quality of individual quality, structural stability, and motion effects. For image-to-video generation, the encompasses image-video consistency, instruction responsiveness, visual quality, structural motion effects.",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 8,
          "region_type": "text",
          "region_id": "page8_region0"
        },
        "score": 0.428,
        "citation": "2511.18870v2.pdf, Page 8"
      },
      {
        "text": "motion artifacts. Our approach begins with a curated prompt set spanning 100+ constructed from high-aesthetic images. Candidate prompts are first generated via a model (VLM), then manually verified to ensure strict text-image consistency. We VLM-based reward model to evaluate videos across four key dimensions: text alignment, visual quality, and motion dynamics. During RL training, we employ mixed strategies by varying both random seeds and CFG scales, and adopt a hybrid ODE-SDE [19] to enrich exploration while maintaining sampling quality. This RLHF process improvements across all evaluation metrics, with particularly notable gains in motion this, our T2V alignment strategy uses a more comprehensive hybrid offline-then-online designed to address the increased complexity of T2V motion artifacts. We found that",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 7,
          "region_type": "text",
          "region_id": "page7_region5"
        },
        "score": 0.405,
        "citation": "2511.18870v2.pdf, Page 7"
      },
      {
        "text": "models struggle to effectively differentiate fine-grained motion quality. Therefore, an offline optimization stage using Direct Preference Optimization (DPO). For a balanced prompt set (from LLMs-generated prompts and training video covering diverse dimensions (motion, scene, subject, etc.). Using a selected high-quality we generate N video candidates per prompt to create non-repetitive pairs. These pairs",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 7,
          "region_type": "text",
          "region_id": "page7_region6"
        },
        "score": 0.405,
        "citation": "2511.18870v2.pdf, Page 7"
      },
      {
        "text": "checkpoint, enabling upon a high-quality SFT, we utilize rigorously selected clips per task, filtered strictly considering aesthetic and motion smoothness. Through CT and SFT, we observe evident improvements in visual quality, aesthetic appeal, and temporal consistency for both generation tasks. employ Reinforcement Learning from Human Feedback (RLHF) strategies tailored for T2V tasks, both mainly aimed at correcting artifacts and enhancing motion quality. task, we apply online reinforcement learning (RL) during post-training to correct",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 7,
          "region_type": "text",
          "region_id": "page7_region1"
        },
        "score": 0.393,
        "citation": "2511.18870v2.pdf, Page 7"
      }
    ],
    "sources": [
      "MiniMax-Speech: Autoregressive Text-to-Speech Model (Topic: Text-to-Speech (TTS) Generation) - Pages 7",
      "Research Paper on Machine Learning (Topic: Machine Learning) - Pages 12",
      "AI Model Optimization (Topic: Model Optimization) - Pages 4",
      "Hunyuan Foundation Model (Topic: Foundation Models) - Pages 3, 7, 8",
      "Text-to-Video Generation with Physical Consistency (Topic: Video Generation) - Pages 9"
    ],
    "has_evidence": true,
    "retrieval_stats": {
      "total_chunks": 10,
      "papers_searched": [
        "2511.18870v2.pdf",
        "2508.19205v1.pdf",
        "2505.09388v1.pdf",
        "605562216_1287553579874010_8068650067268945856_n.pdf",
        "2505.07916v1.pdf"
      ],
      "by_paper": {
        "2511.18870v2.pdf": 6,
        "2508.19205v1.pdf": 1,
        "2505.09388v1.pdf": 1,
        "605562216_1287553579874010_8068650067268945856_n.pdf": 1,
        "2505.07916v1.pdf": 1
      },
      "by_region_type": {
        "list": 1,
        "text": 6,
        "title": 3
      }
    },
    "cached_at": "2026-02-11T00:36:07.485620"
  },
  "What dimensions are used to assess video quality?": {
    "question": "What dimensions are used to assess video quality?",
    "answer": "The dimensions used to assess video quality are not explicitly stated in the provided evidence passages. However, we can infer some information from the papers.\n\nAccording to [Evidence 1], the Hunyuan Foundation Model is related to overall video perception quality, but it doesn't specify the dimensions used to assess it.\n\nIn [Evidence 2], the Text-to-Video Generation with Physical Consistency paper mentions evaluating video representative using a physics-aware VLM, VideoCon-Physics, which outputs a semantics adherence score and a physics commonsense score. However, it doesn't explicitly state the dimensions used to assess video quality.\n\nIn [Evidence 8], the same paper mentions using the same metrics of VideoPhy2 and PhyGenBench to evaluate the T2V generation models, but it doesn't provide details on the dimensions used.\n\nIn [Evidence 9], the paper compares the performance of different models, but it doesn't mention the dimensions used to assess video quality.\n\nHowever, [Evidence 5] and [Evidence 6] from the Hunyuan Foundation Model paper provide some information on video resolution, which might be related to video quality. [Evidence 5] mentions a 720p resolution (720 x 1280) and [Evidence 6] mentions a 480p resolution (480 x 848).\n\nAdditionally, [Evidence 7] from the same paper mentions producing initial video sequences with resolutions from 480p to 720p, which suggests that video resolution is an important aspect of video quality.\n\nIn summary, while the provided evidence doesn't explicitly state the dimensions used to assess video quality, it suggests that video resolution might be an important factor.\n\n---\n**Evidence:**\n- Evidence 1 (from Foundation Models): Provides context for video perception quality but doesn't specify dimensions.\n- Evidence 2 (from Video Generation): Mentions evaluating video representative using a physics-aware VLM, but doesn't provide details on dimensions.\n- Evidence 5 (from Foundation Models): Mentions a 720p resolution (720 x 1280).\n- Evidence 6 (from Foundation Models): Mentions a 480p resolution (480 x 848).\n- Evidence 7 (from Foundation Models): Produces initial video sequences with resolutions from 480p to 720p.\n\n**Sources:**\n- \"Hunyuan Foundation Model\" (Topic: Foundation Models) - Pages 8, 11, 1, 10\n- \"Text-to-Video Generation with Physical Consistency\" (Topic: Video Generation) - Pages 3, 8, 9",
    "evidence": [
      {
        "text": "on overall video perception quality.",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 8,
          "region_type": "title",
          "region_id": "page8_region11"
        },
        "score": 0.671,
        "citation": "2511.18870v2.pdf, Page 8"
      },
      {
        "text": "category as its representatives. Then we employ a physics-aware VLM, VideoCon-Physics [3], to evaluate video representative. VideoCon-Physics outputs a semantics adherence score and a physics commonsense € [0,1] to measure the physics plaucibility of the video. We average the two scores to obtain the overall for each video. Then we compute the mean score of all representatives within each category to obtain the score. In this way, we derive the score distribution histogram Ss. Then we sample the data according",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 3,
          "region_type": "text",
          "region_id": "page3_region2"
        },
        "score": 0.435,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 3"
      },
      {
        "text": "Length L D(Dx, ) 128K 256K 512K 1M 32 4096 (409) 36.0(21.6) 134.0(49.5) 512.0(106.0) 1920 (186.0)",
        "source": {
          "paper": "2505.11820v2.pdf",
          "page": 17,
          "region_type": "text",
          "region_id": "page17_region1"
        },
        "score": 0.392,
        "citation": "2505.11820v2.pdf, Page 17"
      },
      {
        "text": "output distribution, enhancing the information expression ability of the encoder.",
        "source": {
          "paper": "2505.07916v1.pdf",
          "page": 6,
          "region_type": "text",
          "region_id": "page6_region3"
        },
        "score": 0.351,
        "citation": "2505.07916v1.pdf, Page 6"
      },
      {
        "text": "720p (720 x 1280) 241 v 58.39 1.1679",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 11,
          "region_type": "text",
          "region_id": "page11_region6"
        },
        "score": 0.525,
        "citation": "2511.18870v2.pdf, Page 11"
      },
      {
        "text": "480p (480 x 848) 121 x 0.9064 + 0.0062 480p (480 x 848) 241 x 1.7015 + 0.0203",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 11,
          "region_type": "text",
          "region_id": "page11_region5"
        },
        "score": 0.414,
        "citation": "2511.18870v2.pdf, Page 11"
      },
      {
        "text": "producing initial video sequences with resolutions from 480p to 720p and durations of 5 The second stage utilizes a dedicated Video Super-Resolution Network to upscale 1080p, significantly enhancing the final visual fidelity. The main contributions of this summarized as follows:",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 1,
          "region_type": "text",
          "region_id": "page1_region1"
        },
        "score": 0.413,
        "citation": "2511.18870v2.pdf, Page 1"
      },
      {
        "text": "480x832. We set tr = 3, N = 100, Qmin = 0.5, ky = 2.0, b, = 0.4, A = 0.6, ka = 5.0, ba = 0.5. The rank of LoRA to 48. We follow the same metrics of VideoPhy2 and PhyGenBench to evaluate the T2V generation models. Comparison with State-of-the-Art Methods",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 8,
          "region_type": "text",
          "region_id": "page8_region5"
        },
        "score": 0.346,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 8"
      },
      {
        "text": "higher than Sora2 and Veo3. (ii) Compared to the SOTA DPO algorithm for video generation",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 9,
          "region_type": "text",
          "region_id": "page9_region8"
        },
        "score": 0.33,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 9"
      },
      {
        "text": "HunyuanVideo 1.5 720P 12V model and its competitors",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 10,
          "region_type": "text",
          "region_id": "page10_region9"
        },
        "score": 0.327,
        "citation": "2511.18870v2.pdf, Page 10"
      }
    ],
    "sources": [
      "MiniMax-Speech: Autoregressive Text-to-Speech Model (Topic: Text-to-Speech (TTS) Generation) - Pages 6",
      "Natural Language Processing Study (Topic: Natural Language Processing) - Pages 17",
      "Hunyuan Foundation Model (Topic: Foundation Models) - Pages 1, 8, 10, 11",
      "Text-to-Video Generation with Physical Consistency (Topic: Video Generation) - Pages 3, 8, 9"
    ],
    "has_evidence": true,
    "retrieval_stats": {
      "total_chunks": 10,
      "papers_searched": [
        "2511.18870v2.pdf",
        "605562216_1287553579874010_8068650067268945856_n.pdf",
        "2505.11820v2.pdf",
        "2505.07916v1.pdf"
      ],
      "by_paper": {
        "2511.18870v2.pdf": 5,
        "605562216_1287553579874010_8068650067268945856_n.pdf": 3,
        "2505.11820v2.pdf": 1,
        "2505.07916v1.pdf": 1
      },
      "by_region_type": {
        "title": 1,
        "text": 9
      }
    },
    "cached_at": "2026-02-11T00:36:56.370935"
  },
  "How much video data is used for training?": {
    "question": "How much video data is used for training?",
    "answer": "According to the Hunyuan Foundation Model paper [Evidence 2], the team sourced raw videos from a variety of channels, ensuring coverage across diverse content, filming techniques, camera movements, styles, and basic deduplication and the removal of corrupted files, they obtained more than 10 of raw video data. This suggests that the team used a significant amount of video data for training, with at least 10 times the amount of raw video data.\n\nAdditionally, the same paper [Evidence 7] mentions that the video durations used for training ranged from 2 to 10 seconds, which is a structured scaling of spatiotemporal information that facilitates stable convergence and strengthens the model's capacity for detailed, coherent video generation.\n\nThe Text-to-Video Generation with Physical Consistency paper [Evidence 3] also mentions that the scarcity of training data with rich physics interactions and phenomena is a problem, and that the team proposes a Physics-Augmented video data construction Pipeline, PhyAugPipe, to collect a large-scale training dataset.\n\nHowever, the exact amount of video data used for training is not explicitly stated in any of the provided evidence passages. Therefore, while we can infer that a significant amount of video data is used for training, the exact quantity is not specified.\n\n---\n**Evidence:**\n- Evidence 2 (from Hunyuan Foundation Model): The team sourced raw videos from a variety of channels, ensuring coverage across diverse content, filming techniques, camera movements, styles, and basic deduplication and the removal of corrupted files, they obtained more than 10 of raw video data.\n- Evidence 7 (from Hunyuan Foundation Model): Video durations used for training ranged from 2 to 10 seconds.\n- Evidence 3 (from Text-to-Video Generation with Physical Consistency): The scarcity of training data with rich physics interactions and phenomena is a problem, and that the team proposes a Physics-Augmented video data construction Pipeline, PhyAugPipe, to collect a large-scale training dataset.\n\n**Sources:**\n- Hunyuan Foundation Model (Topic: Foundation Models) - Pages 2, 6, 8, 13\n- Text-to-Video Generation with Physical Consistency (Topic: Video Generation) - Pages 1, 2, 11",
    "evidence": [
      {
        "text": "of our pretraining data, the details of our pretraining",
        "source": {
          "paper": "2505.09388v1.pdf",
          "page": 3,
          "region_type": "text",
          "region_id": "page3_region12"
        },
        "score": 0.473,
        "citation": "2505.09388v1.pdf, Page 3"
      },
      {
        "text": "significantly expanded the data volume while refining the filtering mechanisms to enhance specifics of these processes are detailed below. To continuously improve the model’s performance, we prioritized diversity during data acquisition. We sourced raw videos from a variety of channels, ensuring coverage across diverse content, filming techniques, camera movements, styles, and basic deduplication and the removal of corrupted files, we obtained more than 10 of raw video data. high variance in raw video lengths and optimize training efficiency, we segmented consistent clips ranging from 2 to 10 seconds. Specifically, we utilized PySceneDe-",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 2,
          "region_type": "text",
          "region_id": "page2_region1"
        },
        "score": 0.454,
        "citation": "2511.18870v2.pdf, Page 2"
      },
      {
        "text": "The scarcity of training data with rich physics interactions and phenomena is also a problem. In paper, we first introduce a Physics-Augmented video data construction Pipeline, PhyAugPipe, that a vision-language model (VLM) with chain-of-thought reasoning to collect a large-scale training",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 1,
          "region_type": "text",
          "region_id": "page1_region8"
        },
        "score": 0.427,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 1"
      },
      {
        "text": "Scardapane, and and effective compression. on Empiri- Processing, pages Association for Zang, Yuhang Songyang Zhang, Yining Li, and 1 A pioneer- handling resolutions in Neural Infor- 37:42566-42592. Hongyuan Zhu, of embodied ai: [EEE Transac- Computational Intelli- Chen, Xizhou Zhu, Hongsheng Li, Jifeng Vision-rwkv: Effi- with rwkv-like arXiv:2403.02308. Hoefler, and post-training transformers. Conference on Luo, Lei Li, Wang, Chenyu Zhang, and | oth- comprehensive Ilms in video Zhongmin Wang, driving secu- IEEE Internet of Laurence Gold- Jason Phang, Nabeshima, and 1 dataset of di- arXiv preprint Cheng, and diffusion transformer Proceedings of the Vision and Pat- Minjia Zhang, Model tells you",
        "source": {
          "paper": "2505.19147v3.pdf",
          "page": 10,
          "region_type": "list",
          "region_id": "page10_region0"
        },
        "score": 0.424,
        "citation": "2505.19147v3.pdf, Page 10"
      },
      {
        "text": "on overall video perception quality.",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 8,
          "region_type": "title",
          "region_id": "page8_region11"
        },
        "score": 0.415,
        "citation": "2511.18870v2.pdf, Page 8"
      },
      {
        "text": "Bra suc of SFT. However, directly using DPO may encounter three problems: (i) Lack of training data pairs comprehensively capture physical activities, interactions, and phenomena. (ii) The supervision is mainly based ley-Terry (BT) probabilistic model, which only compares pairwise data samples. Such binary win/lose lacks the guidance of physics-aware rewarding and struggles to capture inherently holistic global h as physical plausibility and motion smoothness. (iii) Vanilla DPO duplicates the training reference, which consumes substantial GPU memory and significantly decreases compu preference model as a ational efficiency. address these issues, we firstly propose a data construction pipeline, PhyAugPipe, that exploits a vision-language (VLM) to filter text-video data pairs capturing rich physical interactions and phenomena from a arge T2V pool by parsing the entities and reasoning their actions and outcomes using our designed chain-of-thought",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 2,
          "region_type": "text",
          "region_id": "page2_region2"
        },
        "score": 0.406,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 2"
      },
      {
        "text": "video durations ranging from 2 to 10 seconds. This structured scaling of spatiotemporal facilitates stable convergence and strengthens the model’s capacity for detailed, coherent Similar to prior studies [18], we observe that flow matching-based training is sensitive to the shift hyper-parameter when video token lengths vary across stages. To training stability under such varying sequence conditions, we carefully design a series of strategies that adapt to different token lengths during the progressive training process. comprehensive post-training pipeline to further enhance the generative quality and task-",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 6,
          "region_type": "text",
          "region_id": "page6_region1"
        },
        "score": 0.404,
        "citation": "2511.18870v2.pdf, Page 6"
      },
      {
        "text": "Sora 2. https: 2025. Hunyuan Foundation Model Team. Hunyuanvideo: A systematic framework for large generative models, 2025. URL\\https://arxiv.org/abs/2412. 03603) Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Ranchen Ming, Xiaoniu Song, Xing Chen, Yu Zhou, and Deshan Sun et al. Step-video-t2v report: The practice, challenges, and future of video foundation model, 2025. URL //arxiv.org/abs/2502.10248 Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, and Jingfeng Zhang et al. Wan: and advanced large-scale video generative models. arXiv preprint arXiv:2503.203 14, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. Flashattention-3: Fast and accurate attention with asynchrony and low-precision, 2024. URL //arxiv.org/abs/2407 .08608 Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Chen, Yuankun Chen, Yutian Chen, Zhuofu Chen, and Jialei Cui et al. Kimi k2: Open intelligence, 2025. URL https: //arxiv.org/abs/2507. 20534| Hangting Chen, Peng Chen, Yiji Cheng, Yutao Cui, Xinchi Deng, Ying Dong, Kipper Tianpeng Gu, Xiusen Gu, et al. Hunyuanimage 3.0 technical report. arXiv preprint arXiv:2509.23951, 2025. Contributors. Pyscenedetect: A python library for video scene detection, 2020. https://github. com/Breakthrough/PySceneDetect| Accessed: 2023-11-20. Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Yan, and Weisi Lin. Exploring video quality assessment on user generated contents from and technical perspectives. In Proceedings of the IEEE/CVF International Conference Computer Vision, pages 20144-20154, 2023. Yang, Xufang Luo, Dongqi Han, Yunjian Xu, and Dongsheng Li. Mitigating hallucina- large vision-language models via dpo: On-policy data hold the key. In Proceedings of Computer Vision and Pattern Recognition Conference, pages 10610-10620, 2025.",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 13,
          "region_type": "list",
          "region_id": "page13_region0"
        },
        "score": 0.392,
        "citation": "2511.18870v2.pdf, Page 13"
      },
      {
        "text": "al or dat they use the LLM as their surrogate brain and outsource the ability of current T2V models is still limited and current LLM’s physics reasoning ability is also weak and erroneous, which may in turn mislead the T2V model when it the implicit physics, most current T2V foundation models are data pairs. However, collecting and annotating such data is extremely costly and labor-intensive. models obtained through such supervised fine-tuning (SFT) or training still exhibit limited physics bility. For instance, OpenAI Sora2 [7] and Google DeepMind Veo3.1 [20] often fail in complex human physical phenomena, as shown in Fig. 1, 4, 5, and 6. T a to provide contrastive signals discouraging physically inconsistent generations. reasoning process to it. Even so, the prompt ollows such guidance to generate videos. trained on massive collections of high-quality he key reason is that there are no negative",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 2,
          "region_type": "text",
          "region_id": "page2_region3"
        },
        "score": 0.383,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 2"
      },
      {
        "text": "am Davtyan, Sepehr Sameni, and Paolo Favaro. Efficient video prediction via sparsely conditioned flow matching. In ICCV, 2023. Google DeepMind. Veo-3: A scalable and controllable text-to-video model. https://storage.googleapis.com/ deepmind-media/veo/Veo-3-Tech-Report.pdf, 2025. Technical Report, Google DeepMind. 11",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 11,
          "region_type": "list",
          "region_id": "page11_region0"
        },
        "score": 0.379,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 11"
      }
    ],
    "sources": [
      "Research Paper on Machine Learning (Topic: Machine Learning) - Pages 3",
      "Computer Vision and Image Processing (Topic: Computer Vision) - Pages 10",
      "Hunyuan Foundation Model (Topic: Foundation Models) - Pages 2, 6, 8, 13",
      "Text-to-Video Generation with Physical Consistency (Topic: Video Generation) - Pages 1, 2, 11"
    ],
    "has_evidence": true,
    "retrieval_stats": {
      "total_chunks": 10,
      "papers_searched": [
        "2505.09388v1.pdf",
        "2511.18870v2.pdf",
        "605562216_1287553579874010_8068650067268945856_n.pdf",
        "2505.19147v3.pdf"
      ],
      "by_paper": {
        "2505.09388v1.pdf": 1,
        "2511.18870v2.pdf": 4,
        "605562216_1287553579874010_8068650067268945856_n.pdf": 4,
        "2505.19147v3.pdf": 1
      },
      "by_region_type": {
        "text": 6,
        "list": 3,
        "title": 1
      }
    },
    "cached_at": "2026-02-11T00:37:35.211669"
  },
  "What novel techniques are introduced for video captioning?": {
    "question": "What novel techniques are introduced for video captioning?",
    "answer": "The Hunyuan Foundation Model introduces several novel techniques for video captioning, including:\n\nThe model's Image-to-Video (I2V) Instructional Captioning module generates text that details the temporal evolution or transformation from the initial frame, highlighting changes in foreground subjects and the background environment [Evidence 5]. This module tackles the inherent trade-off between descriptive richness and factual accuracy, a significant and persistent challenge in video captioning.\n\nAdditionally, the model's Selective and Sliding Tile Attention mechanism dynamically prunes redundant spatiotemporal tokens, significantly reducing computational overhead for long video sequences and accelerating inference, with an end-to-end speedup of 1.87x in 10-second 720p video synthesis compared to [7] [Evidence 6].\n\nThe model also utilizes a large multimodal model with bilingual (Chinese-English) understanding, combined with ByT5, to enhance text generation accuracy in videos, and generates detailed captions for both images and videos [Evidence 6].\n\n---\n\n**Evidence:**\n- Evidence 1 (from Hunyuan Foundation Model): Introduces Image-to-Video (I2V) Instructional Captioning module for generating text that details temporal evolution or transformation from the initial frame.\n- Evidence 2 (from Hunyuan Foundation Model): Introduces Selective and Sliding Tile Attention mechanism for reducing computational overhead and accelerating inference.\n- Evidence 3 (from Hunyuan Foundation Model): Utilizes large multimodal model with bilingual (Chinese-English) understanding and ByT5 for enhancing text generation accuracy in videos.\n- Evidence 4 (from Hunyuan Foundation Model): Generates detailed captions for both images and videos.\n\n**Sources:**\n- Hunyuan Foundation Model (Topic: Foundation Models) - Pages 2-3\n- Hunyuan Foundation Model (Topic: Foundation Models) - Pages 2-3\n- Hunyuan Foundation Model (Topic: Foundation Models) - Page 3\n- Computer Vision and Image Processing (Topic: Computer Vision) - Page 10",
    "evidence": [
      {
        "text": "data modalities, we developed three specialized captioning models, each tailored for a Captioning: Generates descriptions for static images, leveraging the same methodol-",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 3,
          "region_type": "text",
          "region_id": "page3_region5"
        },
        "score": 0.592,
        "citation": "2511.18870v2.pdf, Page 3"
      },
      {
        "text": "dels are real-time game engines. In ICLR, 2025. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, 2017. Bram Wallace, Meihua Dang, Rafael Rafailov, Lingi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In CVPR, 2024. Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Duomin Wang, Wei Zuo, Aojie Li, Ling-Hao Chen, Xinyao Liao, Deyu Zhou, Zixin Yin, Xili Dai, Daxin Jiang, and Gang Yu. Universe-1: Unified audio-video generation via stitching of experts. arXiv preprint arXiv:2509.06155, 2025. Qinghe Wang, Yawen Luo, Xiaoyu Shi, Xu Jia, Huchuan Lu, Tianfan Xue, Xintao Wang, Pengfei Wan, Di Zhang, and Kun Gai. Cinemaster: A 3d-aware and controllable framework for cinematic text-to-video generation. In SIGGRAPH, 2025. Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, Jiagang Zhu, and Jiwen Lu. Drivedreamer: Towards real-world- drive world models for autonomous driving. In ECCV, 2024. Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. IJCV, 2025. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022. Yuqing Wen, Yucheng Zhao, Yingfei Liu, Fan Jia, Yanhui Wang, Chong Luo, Chi Zhang, Tiancai Wang, Xiaoyan Sun, and Xiangyu Zhang. Panacea: Panoramic and controllable video generation for autonomous driving. In CVPR, 2024.",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 13,
          "region_type": "list",
          "region_id": "page13_region0"
        },
        "score": 0.461,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 13"
      },
      {
        "text": "parameters to context processing. As shown below, both in text and vision domains, size growth has significantly slowed, while length continues to increase. This trend that the focus of research for efficient shifting from model-centric compression to compression. Comparison of Token Compression Methods and Random Token Dropping section presents a detailed comparison be- designed token compression methods and token dropping, the simplest baseline. The supports arguments in Section 4.1, show- current token compression techniques have limitations. Experiments span multi- domains: complex reasoning in language, im- video understanding in vision, and text- generation in AI content creation. This marks including GQA 2019) and MMB (Liu et al., 25% of visual tokens, following evaluation scripts!. We et al., 2024b), SparseVLM random dropping, and pooling. Figure 3 (b) shows that pooling outperform some attribute this to their shared formity, which mitigates attention-based methods like lights the negative impact of tion scores. We thus advocate as a key design principle in VideoLLMs: Video ated LLaVA-OneVision-7B video benchmarks: MVBench Long VideoBench (Wu et al., et al., 2024), and VideoMME",
        "source": {
          "paper": "2505.19147v3.pdf",
          "page": 17,
          "region_type": "text",
          "region_id": "page17_region1"
        },
        "score": 0.415,
        "citation": "2505.19147v3.pdf, Page 17"
      },
      {
        "text": "Jean-Marc Valin and Jan Skoglund. LPCNet: Improving Neural Speech Synthesis Through Linear Prediction. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5891-5895. IEEE, 2019. van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew W. Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. In The 9th ISCA Speech Synthesis Workshop, SSW 2016, Sunnyvale, CA, USA, September 13-15, 2016, page 125. ISCA, 2016. Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Huaming Wang, Jinyu Li, et al. Neural codec language models are zero-shot text to speech synthesizers. arXiv preprint arXiv:2301.02111, 2023a. Wang, Siqi Zheng, Yafeng Chen, Luyao Cheng, and Qian Chen. Cam++: A fast and efficient network for speaker verification using context-aware masking. 2023b. Wang, Mingqi Jiang, Ziyang Ma, Ziyu Zhang, Songxiang Liu, Linqin Li, Zheng Liang, Qixi Zheng, Rui Wang, Xiaoqin Feng, et al. Spark-tts: An efficient Ilm-based text-to-speech model with single-stream decoupled speech tokens. arXiv preprint arXiv:2503.01710, 2025. Yuancheng Wang, Haoyue Zhan, Liwei Liu, Ruihong Zeng, Haotian Guo, Jiachen Zheng, Qiang Zhang, Xueyao Zhang, Shunsi Zhang, and Zhizheng Wu. Maskgct: Zero-shot text-to-speech with masked generative codec transformer. arXiv preprint arXiv:2409.00750, 2024. Dongchao Yang, Dingdong Wang, Haohan Guo, Xueyuan Chen, Xixin Wu, and Helen Meng. Sim- plespeech: Towards simple and efficient text-to-speech with scalar latent transformer diffusion models. arXiv preprint arXiv:2406.02328, 2024.",
        "source": {
          "paper": "2505.07916v1.pdf",
          "page": 19,
          "region_type": "list",
          "region_id": "page19_region0"
        },
        "score": 0.383,
        "citation": "2505.07916v1.pdf, Page 19"
      },
      {
        "text": "Image-to-Video (I2V) Instructional Captioning: This novel module generates text de- the temporal evolution or transformation from the initial frame, detailing changes foreground subjects and the background environment. Richness-Hallucination Trade-off. A significant and persistent challenge in this do- inherent trade-off between descriptive richness and factual accuracy. Specifically, attempts",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 3,
          "region_type": "text",
          "region_id": "page3_region1"
        },
        "score": 0.582,
        "citation": "2511.18870v2.pdf, Page 3"
      },
      {
        "text": "parameter count while fully leveraging the potential of the model, yielding comparable to state-of-the-art video generation models. Super-Resolution Enhancement: We develop an efficient few-step super-resolution upscales outputs to 1080p. It enhances sharpness while correcting distortions, details and overall visual texture. Optimization: We introduce a novel Selective and Sliding Tile Attention mechanism that dynamically prunes redundant spatiotemporal tokens. This signifi- computational overhead for long video sequences and accelerates inference, end-to-end speedup of 1.87x in 10-second 720p video synthesis compared to [7]. Multimodal Understanding: Our framework utilizes a large multimodal model bilingual (Chinese-English) understanding, combined with ByT5 for dedicated to enhance text generation accuracy in videos. Additionally, detailed captions are generated for both images and videos. Training Optimization: We demonstrate that the Muon optimizer [8] signifi- convergence in video generation model training, while the multi-phase training strategy—spanning from pre-training to post-training stages—enhances aesthetic quality, and alignment with human preferences, thereby enabling of professional-grade content. comprehensive pipeline, beginning with meticulous data preparation—including captioning, as detailed in Section|2| Section [3]introduces the core architecture such as the unified diffusion transformer, the video super-resolution network, sparse attention. Section |4|describes training strategies for text-to-video and covering both pre-training and post-training methodologies. Finally, Section[5| evaluation of the model’s performance against state-of-the-art models using qualitative benchmarks. and Filtering comprises both image and video data. For image data, we adopted the acquisition outlined in [9], curating 5 billion images from a pool of over 10 billion for",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 2,
          "region_type": "list",
          "region_id": "page2_region4"
        },
        "score": 0.427,
        "citation": "2511.18870v2.pdf, Page 2"
      },
      {
        "text": "BS, ae ~= Human ~~ Critic Multi-trajectory Model Rollout Best caption () score=0.8 7\" corrector \\ ee: Figure 1: Caption Model Post-training Pipeline.",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 3,
          "region_type": "text",
          "region_id": "page3_region4"
        },
        "score": 0.411,
        "citation": "2511.18870v2.pdf, Page 3"
      },
      {
        "text": "Captioning: Produces encompass multi-level textual",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 3,
          "region_type": "title",
          "region_id": "page3_region6"
        },
        "score": 0.411,
        "citation": "2511.18870v2.pdf, Page 3"
      },
      {
        "text": "Scardapane, and and effective compression. on Empiri- Processing, pages Association for Zang, Yuhang Songyang Zhang, Yining Li, and 1 A pioneer- handling resolutions in Neural Infor- 37:42566-42592. Hongyuan Zhu, of embodied ai: [EEE Transac- Computational Intelli- Chen, Xizhou Zhu, Hongsheng Li, Jifeng Vision-rwkv: Effi- with rwkv-like arXiv:2403.02308. Hoefler, and post-training transformers. Conference on Luo, Lei Li, Wang, Chenyu Zhang, and | oth- comprehensive Ilms in video Zhongmin Wang, driving secu- IEEE Internet of Laurence Gold- Jason Phang, Nabeshima, and 1 dataset of di- arXiv preprint Cheng, and diffusion transformer Proceedings of the Vision and Pat- Minjia Zhang, Model tells you",
        "source": {
          "paper": "2505.19147v3.pdf",
          "page": 10,
          "region_type": "list",
          "region_id": "page10_region0"
        },
        "score": 0.406,
        "citation": "2505.19147v3.pdf, Page 10"
      },
      {
        "text": "arXiv:2501.08453, 2025. Peng Gao, Le Zhuo, Dongyang Liu, Ruoyi Du, Xu Luo, Longtian Qiu, Yuhang Zhang, Chen Lin, Rongjie Huang, Shijie Geng, et al. Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers. arXiv preprint arXiv:2405.05945, 2024. Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Fei-Fei Li, Irfan Essa, Lu Jiang, and José Lezama. Photorealistic video generation with diffusion models. In ECCV, 2024. Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. arXiv preprint arXiv:2211.13221, 2022. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. NeurIPS, 2022. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In JCLR, 2022. Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video generative modeling. In ICLR, 2025. Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video generative modeling. In JCLR, 2025. Xuan Ju, Tianyu Wang, Yugian Zhou, He Zhang, Qing Liu, Nanxuan Zhao, Zhifei Zhang, Yijun Li, Yuanhao Cai, Shaoteng Liu, et al.",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 12,
          "region_type": "list",
          "region_id": "page12_region0"
        },
        "score": 0.403,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 12"
      }
    ],
    "sources": [
      "MiniMax-Speech: Autoregressive Text-to-Speech Model (Topic: Text-to-Speech (TTS) Generation) - Pages 19",
      "Computer Vision and Image Processing (Topic: Computer Vision) - Pages 10, 17",
      "Hunyuan Foundation Model (Topic: Foundation Models) - Pages 2, 3",
      "Text-to-Video Generation with Physical Consistency (Topic: Video Generation) - Pages 12, 13"
    ],
    "has_evidence": true,
    "retrieval_stats": {
      "total_chunks": 10,
      "papers_searched": [
        "2511.18870v2.pdf",
        "605562216_1287553579874010_8068650067268945856_n.pdf",
        "2505.19147v3.pdf",
        "2505.07916v1.pdf"
      ],
      "by_paper": {
        "2511.18870v2.pdf": 5,
        "605562216_1287553579874010_8068650067268945856_n.pdf": 2,
        "2505.19147v3.pdf": 2,
        "2505.07916v1.pdf": 1
      },
      "by_region_type": {
        "text": 4,
        "list": 5,
        "title": 1
      }
    },
    "cached_at": "2026-02-11T00:37:52.043865"
  },
  "How is the richness-hallucination trade-off addressed?": {
    "question": "How is the richness-hallucination trade-off addressed?",
    "answer": "The richness-hallucination trade-off is addressed in the context of AI model optimization and foundation models. According to [Evidence 3], the Hunyuan Foundation Model integrates Reinforcement Learning (RL), specifically OPA-DPO, into its pipeline to strike a balance between maximizing descriptive detail and minimizing hallucinations. This approach is designed to optimize the model's performance while reducing the likelihood of hallucinations.\n\nIn the context of text-to-speech generation, the MiniMax-Speech model is shown to have a preference for superior performance, but the specific approach to addressing the richness-hallucination trade-off is not explicitly stated in [Evidence 6]. However, it can be inferred that the model's architecture and training objectives are designed to balance richness and hallucination.\n\nIn contrast, [Evidence 1] mentions that AI models with up to 4 speakers can surpass human performance in richness, but it does not provide information on how the richness-hallucination trade-off is addressed.\n\nThe provided evidence does not contain sufficient information to determine how the richness-hallucination trade-off is addressed in the context of multi-modal learning, computer vision, or machine learning.\n\n---\n**Evidence:**\n- Evidence 1 (from AI Model Optimization): Describes AI models with up to 4 speakers surpassing human performance in richness.\n- Evidence 2 (from Computer Vision and Image Processing): Discusses overlooked de-effectiveness and uniformity in image processing.\n- Evidence 3 (from Hunyuan Foundation Model): Describes the integration of Reinforcement Learning (RL) to minimize hallucinations and maximize descriptive detail.\n- Evidence 4 (from Multi-Modal AI Research): Discusses the normalization of total reward across a group to produce a path.\n- Evidence 5 (from AI Research and Development): Mentions the composition of AI research and development, but does not address the richness-hallucination trade-off.\n- Evidence 6 (from MiniMax-Speech: Autoregressive Text-to-Speech Model): Shows the preference for MiniMax-Speech's superior performance, but does not explicitly address the trade-off.\n- Evidence 7 (from Research Paper on Machine Learning): Discusses the flexibility of machine learning models to engage in reasoning or not, but does not address the trade-off.\n- Evidence 8 (from Computer Vision and Image Processing): Discusses the determination of F by quadratic attention and recent scaling approaches.\n- Evidence 9 (from Computer Vision and Image Processing): Discusses the dropping and methods of spatial uniformity compression.\n- Evidence 10 (from AI Research and Development): Mentions the works of Sun et al. (2024) and Yu et al. (2025), but does not address the richness-hallucination trade-off.\n\n**Sources:**\n- \"AI Model Optimization\" (Topic: Model Optimization) - Page 5\n- \"Computer Vision and Image Processing\" (Topic: Computer Vision) - Pages 3, 17\n- \"Hunyuan Foundation Model\" (Topic: Foundation Models) - Page 3\n- \"Multi-Modal AI Research\" (Topic: Multi-Modal Learning) - Page 5\n- \"AI Research and Development\" (Topic: AI Research) - Pages 1, 10\n- \"MiniMax-Speech: Autoregressive Text-to-Speech Model\" (Topic: Text-to-Speech (TTS) Generation) - Page 8\n- \"Research Paper on Machine Learning\" (Topic: Machine Learning) - Page 10",
    "evidence": [
      {
        "text": "with up to 4 speakers, demonstrably surpassing and richness—and",
        "source": {
          "paper": "2508.19205v1.pdf",
          "page": 5,
          "region_type": "title",
          "region_id": "page5_region6"
        },
        "score": 0.337,
        "citation": "2508.19205v1.pdf, Page 5"
      },
      {
        "text": "overlooked de- effectiveness may uniformity better maintain- deterministic strate- that complex reveals gaps in We tested image bench- and Manning, We retained official LLaVA FastV (Chen et al., 2025b),",
        "source": {
          "paper": "2505.19147v3.pdf",
          "page": 17,
          "region_type": "text",
          "region_id": "page17_region0"
        },
        "score": 0.325,
        "citation": "2505.19147v3.pdf, Page 17"
      },
      {
        "text": "integrate Reinforcement Learning (RL), specifically OPA-DPO [12], into our pipeline, as shown in Figure[T] This approach is designed to strike an maximizing descriptive detail while simultaneously minimizing hallucinations and @ vro-win €@ vpo-lose Step2: Ranking-DPO ‘ SFT Caption Model Worst caption rx) score=0.2 +...",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 3,
          "region_type": "list",
          "region_id": "page3_region3"
        },
        "score": 0.315,
        "citation": "2511.18870v2.pdf, Page 3"
      },
      {
        "text": "accepted premises path total reward is then normalized across the group to produce A;.",
        "source": {
          "paper": "2505.10554v2.pdf",
          "page": 5,
          "region_type": "text",
          "region_id": "page5_region10"
        },
        "score": 0.281,
        "citation": "2505.10554v2.pdf, Page 5"
      },
      {
        "text": "composition employing two key limitations of existing techniques: weighting is optimal. The contributions",
        "source": {
          "paper": "583127822_872908961738101_8157213289131333559_n.pdf",
          "page": 1,
          "region_type": "title",
          "region_id": "page1_region6"
        },
        "score": 0.279,
        "citation": "583127822_872908961738101_8157213289131333559_n.pdf, Page 1"
      },
      {
        "text": "preference for MiniMax-Speech’s superior",
        "source": {
          "paper": "2505.07916v1.pdf",
          "page": 8,
          "region_type": "title",
          "region_id": "page8_region7"
        },
        "score": 0.276,
        "citation": "2505.07916v1.pdf, Page 8"
      },
      {
        "text": "and “thinking” modes, providing users with the flexibility to choose whether the model should engage in reasoning or not, and to control the depth of thinking by specifying a token budget for",
        "source": {
          "paper": "2505.09388v1.pdf",
          "page": 10,
          "region_type": "text",
          "region_id": "page10_region10"
        },
        "score": 0.275,
        "citation": "2505.09388v1.pdf, Page 10"
      },
      {
        "text": "determined by F is fundamen- quadratic attention 2017), recent scaling: attention for O(n) 2020; Shen et al., O(n) scal- (Peng et al., 2023; Models like for O(n) com- al.). These re- approaches. (W): Reduc- computational",
        "source": {
          "paper": "2505.19147v3.pdf",
          "page": 3,
          "region_type": "text",
          "region_id": "page3_region6"
        },
        "score": 0.3,
        "citation": "2505.19147v3.pdf, Page 3"
      },
      {
        "text": "dropping and methods. We of spatial uni- bias (Sec. 4) in This also high- bias in atten- spatial uniformity compression. We evalu- al., 2024a) on et al., 2024c), MLVU (Zhou",
        "source": {
          "paper": "2505.19147v3.pdf",
          "page": 17,
          "region_type": "text",
          "region_id": "page17_region6"
        },
        "score": 0.278,
        "citation": "2505.19147v3.pdf, Page 17"
      },
      {
        "text": "works like Sun et al. (2024); Yu et al. (2025) would show blurred results of an unknown nature.",
        "source": {
          "paper": "583127822_872908961738101_8157213289131333559_n.pdf",
          "page": 10,
          "region_type": "text",
          "region_id": "page10_region0"
        },
        "score": 0.274,
        "citation": "583127822_872908961738101_8157213289131333559_n.pdf, Page 10"
      }
    ],
    "sources": [
      "MiniMax-Speech: Autoregressive Text-to-Speech Model (Topic: Text-to-Speech (TTS) Generation) - Pages 8",
      "Research Paper on Machine Learning (Topic: Machine Learning) - Pages 10",
      "Multi-Modal AI Research (Topic: Multi-Modal Learning) - Pages 5",
      "Computer Vision and Image Processing (Topic: Computer Vision) - Pages 3, 17",
      "AI Model Optimization (Topic: Model Optimization) - Pages 5",
      "Hunyuan Foundation Model (Topic: Foundation Models) - Pages 3",
      "AI Research and Development (Topic: AI Research) - Pages 1, 10"
    ],
    "has_evidence": true,
    "retrieval_stats": {
      "total_chunks": 10,
      "papers_searched": [
        "2508.19205v1.pdf",
        "2505.19147v3.pdf",
        "2511.18870v2.pdf",
        "2505.10554v2.pdf",
        "583127822_872908961738101_8157213289131333559_n.pdf",
        "2505.07916v1.pdf",
        "2505.09388v1.pdf"
      ],
      "by_paper": {
        "2508.19205v1.pdf": 1,
        "2505.19147v3.pdf": 3,
        "2511.18870v2.pdf": 1,
        "2505.10554v2.pdf": 1,
        "583127822_872908961738101_8157213289131333559_n.pdf": 2,
        "2505.07916v1.pdf": 1,
        "2505.09388v1.pdf": 1
      },
      "by_region_type": {
        "title": 3,
        "text": 6,
        "list": 1
      }
    },
    "cached_at": "2026-02-11T00:38:36.904181"
  },
  "What reward models are used for reinforcement learning?": {
    "question": "What reward models are used for reinforcement learning?",
    "answer": "According to the provided evidence, several reward models are used for reinforcement learning. In the context of Deep Learning Architectures [Evidence 1], the Agent Lightning client provides Intermediate Rewarding (AIR) mechanisms to convert system monitoring intermediate rewards, enabling the training framework to leverage these signals for more learning. This suggests that intermediate rewards are used to enhance the training process.\n\nIn another Deep Learning Architectures paper [Evidence 6], the component's input, output, and associated reward are mentioned, indicating that rewards are used as a component of the reinforcement learning process.\n\nAdditionally, the same Deep Learning Architectures paper [Evidence 9] describes a weighted combination of a format score and a correctness score as the reward employed during training. The format score is based on the model's ability to output in a specific format, while the correctness score is computed as the word-level F1 score.\n\nFurthermore, in the Multi-Modal AI Research paper [Evidence 2], a rule-based scheme is used to compute a scalar reward, combining Format Reward and Answer Format Reward checks structural rules.\n\nLastly, the Text-to-Video Generation with Physical Consistency paper [Evidence 5] mentions the use of a Physics-Aware Groupwise Direct Preference Optimization (PhyGDPO) algorithm, which suggests that a reward model is used to address issues with previous DPO algorithms.\n\nTherefore, the reward models used for reinforcement learning include intermediate rewards, a weighted combination of format and correctness scores, a rule-based scheme, and a Physics-Aware Groupwise Direct Preference Optimization algorithm.\n\n---\n**Evidence:**\n- Evidence 1 (from Deep Learning Architectures): Describes Intermediate Rewarding (AIR) mechanisms to convert system monitoring intermediate rewards.\n- Evidence 6 (from Deep Learning Architectures): Mentions rewards as a component of the reinforcement learning process.\n- Evidence 9 (from Deep Learning Architectures): Describes a weighted combination of a format score and a correctness score as the reward employed during training.\n- Evidence 2 (from Multi-Modal AI Research): Uses a rule-based scheme to compute a scalar reward.\n- Evidence 5 (from Text-to-Video Generation with Physical Consistency): Mentions a Physics-Aware Groupwise Direct Preference Optimization (PhyGDPO) algorithm.\n\n**Sources:**\n- Deep Learning Architectures (Topic: Deep Learning) - Pages 11, 6, 8, 9, 13\n- Multi-Modal AI Research (Topic: Multi-Modal Learning) - Page 5\n- Text-to-Video Generation with Physical Consistency (Topic: Video Generation) - Page 4\n- Deep Learning Architectures (Topic: Deep Learning) - Pages 6, 8, 9, 13",
    "evidence": [
      {
        "text": "not properly handled by the agent code, such as crashes or long-hanging tool calls, and ensures failures do not disrupt the overall training process. Failed tasks can be retried or reassigned to agent instances, and detailed logs are maintained for debugging and monitoring purposes. This minimizes downtime and maximizes training efficiency, allowing for continuous improvement of performance. Intermediate Rewarding (AIR) The delayed and sparse nature of rewards in RL training the learning process. Intermediate rewards, which provide feedback at various stages of execution, can significantly enhance the training process by offering more frequent and informative But it often requires high overhead such as costly human annotation or complex reward logic. How to mine intermediate rewards from the agent execution is of great importance to process. The Agent Lightning client provides AIR mechanisms to convert system monitoring intermediate rewards, enabling the training framework to leverage these signals for more learning. and Reward Services for Scalability Environments and reward functions are critical of RL training. For those environments and reward functions that are light-weight and locally, they can be executed directly within the same worker with the agent instance. But for",
        "source": {
          "paper": "2508.03680v1.pdf",
          "page": 11,
          "region_type": "text",
          "region_id": "page11_region0"
        },
        "score": 0.483,
        "citation": "2508.03680v1.pdf, Page 11"
      },
      {
        "text": "i=1 a response group, 7re¢ is the frozen instruction model, r; the scalar reward, and {j1,, 0, } Statistics. r; is computed via a rule-based scheme combining Format Reward and Answer Format Reward checks structural rules: the model",
        "source": {
          "paper": "2505.10554v2.pdf",
          "page": 5,
          "region_type": "text",
          "region_id": "page5_region7"
        },
        "score": 0.453,
        "citation": "2505.10554v2.pdf, Page 5"
      },
      {
        "text": "param- models, these cri- main approaches: auxiliary net- X + {s:}7_), A@ beyond the methods in- (Rao et al., 2024d; Kim et al., training to learn {s,}_,, and (ii) et al., 2024; employ pre-trained : KX + utilize parameter- without introduc- approaches can be computation meth- al., 2025; Chen Ge et al., 2024) calculations for to- such as using where a; repre- and (ii) ex- et al., 2023a;",
        "source": {
          "paper": "2505.19147v3.pdf",
          "page": 4,
          "region_type": "text",
          "region_id": "page4_region9"
        },
        "score": 0.425,
        "citation": "2505.19147v3.pdf, Page 4"
      },
      {
        "text": "training purposes compression, scaling to long contexts, etc. In addition, we plan to increase computational resources for reinforcement with a particular emphasis on agent-based RL systems that learn from environmental feedback. allow us to build agents capable of tackling complex tasks that require inference time scaling. Authors Contributors: An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Kai Kexin Le",
        "source": {
          "paper": "2505.09388v1.pdf",
          "page": 22,
          "region_type": "text",
          "region_id": "page22_region2"
        },
        "score": 0.424,
        "citation": "2505.09388v1.pdf, Page 22"
      },
      {
        "text": "algorithms are based on Bradley—Terry (BT) probabilistic model, which compares pairwise data showing limitations in capturing global and holistic preference signals and aligning with human feedback. previous DPO [39, 57] mainly use the T2V model itself as the reward [52], but the model itself exhibits physics reasoning ability as it usually generates physically inconsistent videos. To address these issues, we a Physics-Aware Groupwise Direct Preference Optimization (PhyGDPO), as shown in Fig. 3.",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 4,
          "region_type": "list",
          "region_id": "page4_region4"
        },
        "score": 0.421,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 4"
      },
      {
        "text": "the component, its input, output, and the associated reward",
        "source": {
          "paper": "2508.03680v1.pdf",
          "page": 6,
          "region_type": "text",
          "region_id": "page6_region11"
        },
        "score": 0.478,
        "citation": "2508.03680v1.pdf, Page 6"
      },
      {
        "text": "a separate to simplify training. For instance, (Shao et al., 2024) advantage by normalizing the reward of a response against the mean and standard deviation of",
        "source": {
          "paper": "2508.03680v1.pdf",
          "page": 8,
          "region_type": "title",
          "region_id": "page8_region6"
        },
        "score": 0.459,
        "citation": "2508.03680v1.pdf, Page 8"
      },
      {
        "text": "R= ye Th. f=1 model’s goal is to maximize the return. Data Extraction for RL the MDP formulation above, we need to collect trajectories containing all policy LLM decisions associated rewards for RL. To this end, we extract from each execution (i.e., Eq. (5)) only the information for updating the policy LLM parameterized by 6: the raw input, output of LLM",
        "source": {
          "paper": "2508.03680v1.pdf",
          "page": 7,
          "region_type": "text",
          "region_id": "page7_region1"
        },
        "score": 0.449,
        "citation": "2508.03680v1.pdf, Page 7"
      },
      {
        "text": "reward employed during training is a weighted combination of a format score R¢ormat and a correct- score. Specifically, the format score is 1 if the LLM can output in a specific format, where the corre- content must be enclosed by specific tokens, such as <think>. . .</think>, <query>. ..</query> <answer>.. .</answer>. And the correctness score Reorrectness is computed as the word-level F1 score",
        "source": {
          "paper": "2508.03680v1.pdf",
          "page": 13,
          "region_type": "text",
          "region_id": "page13_region4"
        },
        "score": 0.439,
        "citation": "2508.03680v1.pdf, Page 13"
      },
      {
        "text": "LightningRL: A Hierarchical RL Method for Optimizing LLMs in Agents Preliminary on Single-Turn Reinforcement Learning for LLMs advances in RL for LLMs have focused on single-call problems, where the model generates a to a prompt in one pass. These algorithms are primarily applied in domains such as mathematics puzzles (Guo et al., 2025; Xie et al., 2025). In this setting, given a task x, an LLM generates a output = (y1,...,yn) token by token from a policy 79, and each token is treated as an action. entire response is generated, a scalar reward r(x, output, 7) is assigned based on the correctness of the solution, where # is the ground truth to task x. The typical token-level loss can be as?: N",
        "source": {
          "paper": "2508.03680v1.pdf",
          "page": 8,
          "region_type": "text",
          "region_id": "page8_region1"
        },
        "score": 0.426,
        "citation": "2508.03680v1.pdf, Page 8"
      }
    ],
    "sources": [
      "Research Paper on Machine Learning (Topic: Machine Learning) - Pages 22",
      "Multi-Modal AI Research (Topic: Multi-Modal Learning) - Pages 5",
      "Computer Vision and Image Processing (Topic: Computer Vision) - Pages 4",
      "Deep Learning Architectures (Topic: Deep Learning) - Pages 6, 7, 8, 11, 13",
      "Text-to-Video Generation with Physical Consistency (Topic: Video Generation) - Pages 4"
    ],
    "has_evidence": true,
    "retrieval_stats": {
      "total_chunks": 10,
      "papers_searched": [
        "2508.03680v1.pdf",
        "2505.10554v2.pdf",
        "2505.19147v3.pdf",
        "2505.09388v1.pdf",
        "605562216_1287553579874010_8068650067268945856_n.pdf"
      ],
      "by_paper": {
        "2508.03680v1.pdf": 6,
        "2505.10554v2.pdf": 1,
        "2505.19147v3.pdf": 1,
        "2505.09388v1.pdf": 1,
        "605562216_1287553579874010_8068650067268945856_n.pdf": 1
      },
      "by_region_type": {
        "text": 8,
        "list": 1,
        "title": 1
      }
    },
    "cached_at": "2026-02-11T00:38:46.100559"
  },
  "How does flow matching-based training work?": {
    "question": "How does flow matching-based training work?",
    "answer": "Flow matching-based training is sensitive to the shift hyper-parameter when video token lengths vary across stages, as observed in prior studies [Evidence 1]. To address this issue, the Hunyuan Foundation Model employs a series of strategies that adapt to different token lengths during the progressive training process [Evidence 1]. This approach is designed to facilitate stable convergence and strengthen the model's capacity for detailed, coherent video generation.\n\nIn the context of text-to-video generation, flow matching is used to model the distribution of continuous speech features (latent), which are extracted from an encoder-decoder trained on audio [Evidence 3]. This suggests that flow matching-based training is particularly well-suited for tasks that involve modeling complex, continuous signals.\n\nInterestingly, the MiniMax-Speech: Autoregressive Text-to-Speech Model also employs flow matching, but in a different context. In this case, flow matching is used to model the distribution of continuous speech features, rather than mel-spectrograms [Evidence 3]. This highlights the flexibility of flow matching-based training and its potential applications in various domains.\n\nOverall, the evidence suggests that flow matching-based training is a powerful approach for modeling complex, continuous signals, but it requires careful adaptation to different token lengths and hyper-parameters to ensure stable convergence.\n\n---\n**Evidence:**\n- Evidence 1 (from Foundation Models): Describes the sensitivity of flow matching-based training to the shift hyper-parameter and the need for adaptive strategies to address this issue.\n- Evidence 3 (from Text-to-Speech (TTS) Generation): Shows how flow matching is used to model the distribution of continuous speech features in the context of text-to-video generation.\n- Evidence 4 (from AI Research): Mentions the use of flow matching in the context of model training, but does not provide specific details on how it works.\n- Evidence 5 (from Natural Language Processing): Does not provide any information on flow matching-based training.\n- Evidence 6 (from Video Generation): Does not provide any information on flow matching-based training.\n- Evidence 7 (from Machine Learning): Does not provide any information on flow matching-based training.\n- Evidence 8 (from Text-to-Speech (TTS) Generation): Lists various papers that use flow matching, but does not provide any information on how it works.\n- Evidence 9 (from Deep Learning): Does not provide any information on flow matching-based training.\n- Evidence 10 (from Foundation Models): Does not provide any information on flow matching-based training.\n\n**Sources:**\n- \"Hunyuan Foundation Model\" (Topic: Foundation Models) - Page 6\n- \"Deep Learning Architectures\" (Topic: Deep Learning) - Page 10\n- \"MiniMax-Speech: Autoregressive Text-to-Speech Model\" (Topic: Text-to-Speech (TTS) Generation) - Page 5\n- \"AI Research and Development\" (Topic: AI Research) - Page 10\n- \"Natural Language Processing Study\" (Topic: Natural Language Processing) - Page 7\n- \"Text-to-Video Generation with Physical Consistency\" (Topic: Video Generation) - Page 11\n- \"Research Paper on Machine Learning\" (Topic: Machine Learning) - Page 3\n- \"Deep Learning Architectures\" (Topic: Deep Learning) - Page 4",
    "evidence": [
      {
        "text": "video durations ranging from 2 to 10 seconds. This structured scaling of spatiotemporal facilitates stable convergence and strengthens the model’s capacity for detailed, coherent Similar to prior studies [18], we observe that flow matching-based training is sensitive to the shift hyper-parameter when video token lengths vary across stages. To training stability under such varying sequence conditions, we carefully design a series of strategies that adapt to different token lengths during the progressive training process. comprehensive post-training pipeline to further enhance the generative quality and task-",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 6,
          "region_type": "text",
          "region_id": "page6_region1"
        },
        "score": 0.527,
        "citation": "2511.18870v2.pdf, Page 6"
      },
      {
        "text": "is responsible for updating the model weights, while the rollout captures used by the trainer. For single-turn RL tasks, such as mathematical reasoning, the",
        "source": {
          "paper": "2508.03680v1.pdf",
          "page": 10,
          "region_type": "title",
          "region_id": "page10_region7"
        },
        "score": 0.5,
        "citation": "2508.03680v1.pdf, Page 10"
      },
      {
        "text": "has powerful context modeling capabilities. Our flow matching model is designed to model the distribution of continuous speech features (latent), which are extracted from an encoder-decoder trained on audio, rather than mel-spectrograms. When training this encoder-decoder module its encoder extracts these continuous speech features and its decoder is typically a neural",
        "source": {
          "paper": "2505.07916v1.pdf",
          "page": 5,
          "region_type": "text",
          "region_id": "page5_region6"
        },
        "score": 0.493,
        "citation": "2505.07916v1.pdf, Page 5"
      },
      {
        "text": "an development leaking any in the model training practice: In this work, we have only tested the souping of the ’ final’ posttrained checkpoints. As other works have shown, the models can be souped after pre-training Li et al. (2025), after Wortsman et al. (2022), with adapters’. We do not recommend souping of the checkpoints from different as well as souping unaligned or uncensored models with the aligned ones to avoid the inheritance of",
        "source": {
          "paper": "583127822_872908961738101_8157213289131333559_n.pdf",
          "page": 10,
          "region_type": "text",
          "region_id": "page10_region3"
        },
        "score": 0.475,
        "citation": "583127822_872908961738101_8157213289131333559_n.pdf, Page 10"
      },
      {
        "text": "Overall, Chain Expansion allows us to avoid training from scratch.",
        "source": {
          "paper": "2505.11820v2.pdf",
          "page": 7,
          "region_type": "title",
          "region_id": "page7_region8"
        },
        "score": 0.442,
        "citation": "2505.11820v2.pdf, Page 7"
      },
      {
        "text": "am Davtyan, Sepehr Sameni, and Paolo Favaro. Efficient video prediction via sparsely conditioned flow matching. In ICCV, 2023. Google DeepMind. Veo-3: A scalable and controllable text-to-video model. https://storage.googleapis.com/ deepmind-media/veo/Veo-3-Tech-Report.pdf, 2025. Technical Report, Google DeepMind. 11",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 11,
          "region_type": "list",
          "region_id": "page11_region0"
        },
        "score": 0.43,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 11"
      },
      {
        "text": "of our pretraining data, the details of our pretraining",
        "source": {
          "paper": "2505.09388v1.pdf",
          "page": 3,
          "region_type": "text",
          "region_id": "page3_region12"
        },
        "score": 0.422,
        "citation": "2505.09388v1.pdf, Page 3"
      },
      {
        "text": "Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning Prompt tuning can be comparable to fine-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602, 2021. Ma¢kiewicz and Waldemar Ratajczak. Principal components analysis (pca). Computers & Geosciences, 19(3):303-342, 1993. Mehta, Ruibo Tu, Jonas Beskow, Eva Székely, and Gustav Eje Henter. Matcha-tts: A fast tts architecture with conditional flow matching. In ICASSP 2024-2024 IEEE International Conference Acoustics, Speech and Signal Processing (ICASSP), pages 11341-11345. IEEE, 2024. Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In International conference on machine learning, pages 28492-28518. PMLR, 2023. Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International conference on machine learning, pages 1530-1538. PMLR, 2015. Tan, Jiawei Chen, Haohe Liu, Jian Cong, Chen Zhang, Yanqing Liu, Xi Wang, Yichong Leng, Yuanhao Yi, Lei He, et al. Naturalspeech: End-to-end text-to-speech synthesis with human-level quality. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(6):4234-4245, 2024.",
        "source": {
          "paper": "2505.07916v1.pdf",
          "page": 18,
          "region_type": "list",
          "region_id": "page18_region0"
        },
        "score": 0.465,
        "citation": "2505.07916v1.pdf, Page 18"
      },
      {
        "text": "that to ensure clear and precise referencing, in our paper, execution refers to the collected data during one Trajectory /episode emphasizes extracted data used for RL training.",
        "source": {
          "paper": "2508.03680v1.pdf",
          "page": 4,
          "region_type": "text",
          "region_id": "page4_region1"
        },
        "score": 0.448,
        "citation": "2508.03680v1.pdf, Page 4"
      },
      {
        "text": "of subsequent T2V & I2V training. the",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 6,
          "region_type": "title",
          "region_id": "page6_region6"
        },
        "score": 0.437,
        "citation": "2511.18870v2.pdf, Page 6"
      }
    ],
    "sources": [
      "MiniMax-Speech: Autoregressive Text-to-Speech Model (Topic: Text-to-Speech (TTS) Generation) - Pages 5, 18",
      "Research Paper on Machine Learning (Topic: Machine Learning) - Pages 3",
      "Natural Language Processing Study (Topic: Natural Language Processing) - Pages 7",
      "Deep Learning Architectures (Topic: Deep Learning) - Pages 4, 10",
      "Hunyuan Foundation Model (Topic: Foundation Models) - Pages 6",
      "AI Research and Development (Topic: AI Research) - Pages 10",
      "Text-to-Video Generation with Physical Consistency (Topic: Video Generation) - Pages 11"
    ],
    "has_evidence": true,
    "retrieval_stats": {
      "total_chunks": 10,
      "papers_searched": [
        "2511.18870v2.pdf",
        "2508.03680v1.pdf",
        "2505.07916v1.pdf",
        "583127822_872908961738101_8157213289131333559_n.pdf",
        "2505.11820v2.pdf",
        "605562216_1287553579874010_8068650067268945856_n.pdf",
        "2505.09388v1.pdf"
      ],
      "by_paper": {
        "2511.18870v2.pdf": 2,
        "2508.03680v1.pdf": 2,
        "2505.07916v1.pdf": 2,
        "583127822_872908961738101_8157213289131333559_n.pdf": 1,
        "2505.11820v2.pdf": 1,
        "605562216_1287553579874010_8068650067268945856_n.pdf": 1,
        "2505.09388v1.pdf": 1
      },
      "by_region_type": {
        "text": 5,
        "title": 3,
        "list": 2
      }
    },
    "cached_at": "2026-02-11T00:39:16.721284"
  },
  "What strategies are used for training stability?": {
    "question": "What strategies are used for training stability?",
    "answer": "The strategies used for training stability in various AI models are diverse and context-dependent. According to the papers on Natural Language Processing, Deep Learning Architectures, and Multi-Modal Learning, several techniques are employed to ensure stable training.\n\nOne approach is the use of pretraining [Evidence 1]. In the context of Natural Language Processing, pretraining is used to improve the model's performance on downstream tasks [Evidence 4]. However, it's worth noting that the Hunyuan Foundation Model also mentions the importance of fine-tuning (SFT) and human feedback alignment (RLHF) for training stability [Evidence 8].\n\nAnother strategy is the use of reward normalization, which is discussed in the context of Deep Learning Architectures [Evidence 9]. This technique involves normalizing the reward of a response against the mean and standard deviation of the reward distribution. This can help stabilize the training process.\n\nAdditionally, the use of separate rollouts can also contribute to training stability [Evidence 3]. In the context of Deep Learning Architectures, the rollout is responsible for updating the model weights, while the trainer captures the used by the rollout.\n\nIt's also worth mentioning that the Multi-Modal Learning study suggests using simple objectives, such as REINFORCE, to isolate the impact of initialization and ensure performance gains arise from alignment rather than optimization differences [Evidence 7].\n\nHowever, the AI Research and Development paper cautions against souping of checkpoints from different models or unaligned models, as this can lead to the inheritance of unwanted properties [Evidence 5].\n\nLastly, the Text-to-Video Generation with Physical Consistency paper highlights the limitations of current T2V models in physics reasoning and the need for contrastive signals to discourage physically inconsistent generations [Evidence 6].\n\n---\n**Evidence:**\n- Evidence 1 (from Natural Language Processing Study): Describes the use of pretraining in Natural Language Processing.\n- Evidence 2 (from Hunyuan Foundation Model): Mentions the use of subsequent T2V & I2V training.\n- Evidence 3 (from Deep Learning Architectures): Discusses the use of separate rollouts for updating model weights.\n- Evidence 4 (from Research Paper on Machine Learning): Describes the use of pretraining data in machine learning.\n- Evidence 5 (from AI Research and Development): Warns against souping of checkpoints from different models.\n- Evidence 6 (from Text-to-Video Generation with Physical Consistency): Highlights the limitations of current T2V models in physics reasoning.\n- Evidence 7 (from Multi-Modal AI Research): Suggests using simple objectives for training stability.\n- Evidence 8 (from Hunyuan Foundation Model): Mentions the use of fine-tuning (SFT) and human feedback alignment (RLHF).\n- Evidence 9 (from Deep Learning Architectures): Discusses the use of reward normalization for training stability.\n- Evidence 10 (from Deep Learning Architectures): Mentions the use of Reinforce.\n\n**Sources:**\n- Natural Language Processing Study (Topic: Natural Language Processing) - Page 14\n- Hunyuan Foundation Model (Topic: Foundation Models) - Page 6\n- Deep Learning Architectures (Topic: Deep Learning) - Page 10\n- Research Paper on Machine Learning (Topic: Machine Learning) - Page 3\n- AI Research and Development (Topic: AI Research) - Page 10\n- Text-to-Video Generation with Physical Consistency (Topic: Video Generation) - Page 2\n- Multi-Modal AI Research (Topic: Multi-Modal Learning) - Page 5\n- Hunyuan Foundation Model (Topic: Foundation Models) - Page 6\n- Deep Learning Architectures (Topic: Deep Learning) - Page 8\n- Deep Learning Architectures (Topic: Deep Learning) - Page 1",
    "evidence": [
      {
        "text": "used in pretraining.",
        "source": {
          "paper": "2505.11820v2.pdf",
          "page": 14,
          "region_type": "text",
          "region_id": "page14_region7"
        },
        "score": 0.437,
        "citation": "2505.11820v2.pdf, Page 14"
      },
      {
        "text": "of subsequent T2V & I2V training. the",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 6,
          "region_type": "title",
          "region_id": "page6_region6"
        },
        "score": 0.428,
        "citation": "2511.18870v2.pdf, Page 6"
      },
      {
        "text": "is responsible for updating the model weights, while the rollout captures used by the trainer. For single-turn RL tasks, such as mathematical reasoning, the",
        "source": {
          "paper": "2508.03680v1.pdf",
          "page": 10,
          "region_type": "title",
          "region_id": "page10_region7"
        },
        "score": 0.391,
        "citation": "2508.03680v1.pdf, Page 10"
      },
      {
        "text": "of our pretraining data, the details of our pretraining",
        "source": {
          "paper": "2505.09388v1.pdf",
          "page": 3,
          "region_type": "text",
          "region_id": "page3_region12"
        },
        "score": 0.364,
        "citation": "2505.09388v1.pdf, Page 3"
      },
      {
        "text": "an development leaking any in the model training practice: In this work, we have only tested the souping of the ’ final’ posttrained checkpoints. As other works have shown, the models can be souped after pre-training Li et al. (2025), after Wortsman et al. (2022), with adapters’. We do not recommend souping of the checkpoints from different as well as souping unaligned or uncensored models with the aligned ones to avoid the inheritance of",
        "source": {
          "paper": "583127822_872908961738101_8157213289131333559_n.pdf",
          "page": 10,
          "region_type": "text",
          "region_id": "page10_region3"
        },
        "score": 0.348,
        "citation": "583127822_872908961738101_8157213289131333559_n.pdf, Page 10"
      },
      {
        "text": "al or dat they use the LLM as their surrogate brain and outsource the ability of current T2V models is still limited and current LLM’s physics reasoning ability is also weak and erroneous, which may in turn mislead the T2V model when it the implicit physics, most current T2V foundation models are data pairs. However, collecting and annotating such data is extremely costly and labor-intensive. models obtained through such supervised fine-tuning (SFT) or training still exhibit limited physics bility. For instance, OpenAI Sora2 [7] and Google DeepMind Veo3.1 [20] often fail in complex human physical phenomena, as shown in Fig. 1, 4, 5, and 6. T a to provide contrastive signals discouraging physically inconsistent generations. reasoning process to it. Even so, the prompt ollows such guidance to generate videos. trained on massive collections of high-quality he key reason is that there are no negative",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 2,
          "region_type": "text",
          "region_id": "page2_region3"
        },
        "score": 0.341,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 2"
      },
      {
        "text": "objective in place of more complex objectives such as REINFORCE++. These choices SimpleRL-Zoo and help isolate the impact of initialization, ensuring performance gains arise alignment rather than optimization differences.",
        "source": {
          "paper": "2505.10554v2.pdf",
          "page": 5,
          "region_type": "text",
          "region_id": "page5_region6"
        },
        "score": 0.325,
        "citation": "2505.10554v2.pdf, Page 5"
      },
      {
        "text": "fine-tuning (SFT), and human feedback alignment (RLHF), applied separately",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 6,
          "region_type": "text",
          "region_id": "page6_region3"
        },
        "score": 0.361,
        "citation": "2511.18870v2.pdf, Page 6"
      },
      {
        "text": "a separate to simplify training. For instance, (Shao et al., 2024) advantage by normalizing the reward of a response against the mean and standard deviation of",
        "source": {
          "paper": "2508.03680v1.pdf",
          "page": 8,
          "region_type": "title",
          "region_id": "page8_region6"
        },
        "score": 0.36,
        "citation": "2508.03680v1.pdf, Page 8"
      },
      {
        "text": "that enables Reinforce-",
        "source": {
          "paper": "2508.03680v1.pdf",
          "page": 1,
          "region_type": "text",
          "region_id": "page1_region8"
        },
        "score": 0.358,
        "citation": "2508.03680v1.pdf, Page 1"
      }
    ],
    "sources": [
      "Research Paper on Machine Learning (Topic: Machine Learning) - Pages 3",
      "Multi-Modal AI Research (Topic: Multi-Modal Learning) - Pages 5",
      "Natural Language Processing Study (Topic: Natural Language Processing) - Pages 14",
      "Deep Learning Architectures (Topic: Deep Learning) - Pages 1, 8, 10",
      "Hunyuan Foundation Model (Topic: Foundation Models) - Pages 6",
      "AI Research and Development (Topic: AI Research) - Pages 10",
      "Text-to-Video Generation with Physical Consistency (Topic: Video Generation) - Pages 2"
    ],
    "has_evidence": true,
    "retrieval_stats": {
      "total_chunks": 10,
      "papers_searched": [
        "2505.11820v2.pdf",
        "2511.18870v2.pdf",
        "2508.03680v1.pdf",
        "2505.09388v1.pdf",
        "583127822_872908961738101_8157213289131333559_n.pdf",
        "605562216_1287553579874010_8068650067268945856_n.pdf",
        "2505.10554v2.pdf"
      ],
      "by_paper": {
        "2505.11820v2.pdf": 1,
        "2511.18870v2.pdf": 2,
        "2508.03680v1.pdf": 3,
        "2505.09388v1.pdf": 1,
        "583127822_872908961738101_8157213289131333559_n.pdf": 1,
        "605562216_1287553579874010_8068650067268945856_n.pdf": 1,
        "2505.10554v2.pdf": 1
      },
      "by_region_type": {
        "text": 7,
        "title": 3
      }
    },
    "cached_at": "2026-02-11T00:39:31.119071"
  },
  "What are the main contributions and capabilities of these models?": {
    "question": "What are the main contributions and capabilities of these models?",
    "answer": "The provided evidence suggests that these models have various capabilities and contributions, which I'll outline below.\n\nThe AI Research and Development paper [Evidence 1] mentions that the candidate models are still helpful in souping with SoCE compared to their role in performance, indicating that these models can be used in conjunction with other models to improve performance. Additionally, [Evidence 8] shows that SoCE retains most of the capabilities of individual models in the soup, suggesting that these models can be combined to achieve better results.\n\nThe Research Paper on Machine Learning [Evidence 2] mentions that models can be used to construct responses into models, implying that these models can be used to generate responses or answers to questions.\n\nThe Natural Language Processing Study [Evidence 3] discusses the Chain-of-Model (CoM) architecture, which is an innovative design to optimize the architecture of foundation models. CoM introduces an advanced design to optimize the scaling architecture, and it is better suitable for large-scale model sizes rather than smaller ones. This suggests that CoM can be used to improve the performance of large-scale models.\n\nThe Computer Vision and Image Processing paper [Evidence 4] mentions that the overhead of model-centric model quantization is increasing, but the research has shown that the model size was reduced while maintaining performance gains. This suggests that these models can be used to improve the efficiency of computer vision tasks.\n\nThe Deep Learning Architectures paper [Evidence 5] mentions that the building components for agents can be used to construct models, implying that these models can be used to build complex systems or agents.\n\nThe MiniMax-Speech: Autoregressive Text-to-Speech Model paper [Evidence 6] mentions that the model can be used to generate speech, suggesting that these models can be used for text-to-speech tasks.\n\nThe Multi-Modal AI Research paper [Evidence 7] discusses the merged model's performance across all three abilities, which is at least slightly better than its pre-merged counterparts. This suggests that these models can be used to improve the performance of multi-modal tasks.\n\nThe AI Research and Development paper [Evidence 10] mentions that models undergoing a process can be used to improve their performance, suggesting that these models can be used to improve their capabilities through training or fine-tuning.\n\nOverall, these models have various capabilities, including:\n\n* Improving performance through souping with other models [Evidence 1, Evidence 8]\n* Generating responses or answers to questions [Evidence 2]\n* Optimizing the architecture of foundation models [Evidence 3]\n* Improving the efficiency of computer vision tasks [Evidence 4]\n* Building complex systems or agents [Evidence 5]\n* Generating speech [Evidence 6]\n* Improving the performance of multi-modal tasks [Evidence 7]\n* Improving their performance through training or fine-tuning [Evidence 10]\n\n---\n\n**Evidence:**\n\n- Evidence 1 (from AI Research and Development): Suggests that candidate models are still helpful in souping with SoCE compared to their role in performance.\n- Evidence 2 (from Research Paper on Machine Learning): Mentions that models can be used to construct responses into models.\n- Evidence 3 (from Natural Language Processing Study): Discusses the Chain-of-Model (CoM) architecture, which is an innovative design to optimize the architecture of foundation models.\n- Evidence 4 (from Computer Vision and Image Processing): Mentions that the overhead of model-centric model quantization is increasing, but the research has shown that the model size was reduced while maintaining performance gains.\n- Evidence 5 (from Deep Learning Architectures): Mentions that the building components for agents can be used to construct models.\n- Evidence 6 (from MiniMax-Speech: Autoregressive Text-to-Speech Model): Mentions that the model can be used to generate speech.\n- Evidence 7 (from Multi-Modal AI Research): Discusses the merged model's performance across all three abilities, which is at least slightly better than its pre-merged counterparts.\n- Evidence 8 (from AI Research and Development): Shows that SoCE retains most of the capabilities of individual models in the soup.\n- Evidence 9 (from Research Paper on Machine Learning): Mentions that models can be used to demonstrate abilities in various tasks.\n- Evidence 10 (from AI Research and Development): Mentions that models undergoing a process can be used to improve their performance.\n\n**Sources:**\n\n- AI Research and Development (Topic: AI Research) - Pages 6, 8, 10, 14\n- Research Paper on Machine Learning (Topic: Machine Learning) - Pages 1, 15\n- Natural Language Processing Study (Topic: Natural Language Processing) - Page 19\n- Computer Vision and Image Processing (Topic: Computer Vision) - Page 1\n- Deep Learning Architectures (Topic: Deep Learning) - Page 3\n- MiniMax-Speech: Autoregressive Text-to-Speech Model (Topic: Text-to-Speech (TTS) Generation) - Page 1\n- Multi-Modal AI Research (Topic: Multi-Modal Learning) - Page 8",
    "evidence": [
      {
        "text": "is useful beyond tool the candidate models is still helpful in souping with SoCE compared to the role in performance",
        "source": {
          "paper": "583127822_872908961738101_8157213289131333559_n.pdf",
          "page": 6,
          "region_type": "text",
          "region_id": "page6_region5"
        },
        "score": 0.545,
        "citation": "583127822_872908961738101_8157213289131333559_n.pdf, Page 6"
      },
      {
        "text": "responses) into models—such",
        "source": {
          "paper": "2505.09388v1.pdf",
          "page": 1,
          "region_type": "title",
          "region_id": "page1_region3"
        },
        "score": 0.512,
        "citation": "2505.09388v1.pdf, Page 1"
      },
      {
        "text": "are designed from different perspectives to construct experts. In other words, CoM is orthogonal to MoE, meaning both of them can be applied within the same architecture advantages from each. We will explore how to combine CoM and MoE architecture, and work. When do we need the Chain-of-Model architecture? Chain-of-Model (CoM) architecture can be considered as an innovative design to optimize architecture (e.g., increasing model capabilities) of foundation models, rather than altering network (e.g., Transformer). Therefore, it can be generalized to any model with different such as CNNs or parallelized RNNs. However, this does not mean we can apply CoM any scenario. Based on the designs of CoM architecture, it can be considered as a of multiple nested sub-models. Each sub-model will contribute to the capability of models. More specifically, the first chain (sub-model) plays a critical role in CoM as it directly determines the understanding capability of the entire model. Therefore, if of the model is not sufficient to exhibit the power of the scaling law or lacks enough it is unnecessary to use the architecture of the chain-of-model method. In other words, structure is better suitable for large-scale model sizes, rather than smaller ones. This also CoM is an advanced design to optimize the scaling architecture. perspective of methodology, Chain-of-Model (CoM) introduces an innovative and brand-",
        "source": {
          "paper": "2505.11820v2.pdf",
          "page": 19,
          "region_type": "text",
          "region_id": "page19_region1"
        },
        "score": 0.483,
        "citation": "2505.11820v2.pdf, Page 19"
      },
      {
        "text": "of performance ever-increasing com- early 2024, the overhead was growth in parame- requirements. the research model-centric model quanti- al., 2023), net- et al., 2024), al., 2015; Gou (Yu et al., 2020). overhead by natural response model size was gains. limits, the Meanwhile, emerged: the",
        "source": {
          "paper": "2505.19147v3.pdf",
          "page": 1,
          "region_type": "text",
          "region_id": "page1_region2"
        },
        "score": 0.47,
        "citation": "2505.19147v3.pdf, Page 1"
      },
      {
        "text": "the building components for agents can",
        "source": {
          "paper": "2508.03680v1.pdf",
          "page": 3,
          "region_type": "title",
          "region_id": "page3_region5"
        },
        "score": 0.435,
        "citation": "2508.03680v1.pdf, Page 3"
      },
      {
        "text": "addition, the model",
        "source": {
          "paper": "2505.07916v1.pdf",
          "page": 1,
          "region_type": "text",
          "region_id": "page1_region5"
        },
        "score": 0.414,
        "citation": "2505.07916v1.pdf, Page 1"
      },
      {
        "text": "highest weight coefficient. Because abduction appears least frequently, we report its count here. The merged model’s performance across all three abilities is at least on slightly better than its pre-merged counterparts. meta-ability alignment enables us to inject additional desired capabilities into the us endow it with any specific reasoning skills we choose and tailor its behavior to needs. Third, scientific discovery workflows might profit from models that can hypothesise (abduction), test (deduction), and generalise (induction) in silico before costly",
        "source": {
          "paper": "2505.10554v2.pdf",
          "page": 8,
          "region_type": "text",
          "region_id": "page8_region2"
        },
        "score": 0.414,
        "citation": "2505.10554v2.pdf, Page 8"
      },
      {
        "text": "(2024a). examine the proportion of tasks solved by SoCE that were also solved by each individual model. SoCE completes 97.2% of the tasks solved by xLAM-2-70b-fe-r (Prabhakar et al., 2025), 97.1% of those solved CoALM-70B (Acikgoz et al., 2025), and 97.1% of those solved by watt-tool-70B gm8.ai (2024a). This indicates that retains most of the capabilities of individual models in the soup.",
        "source": {
          "paper": "583127822_872908961738101_8157213289131333559_n.pdf",
          "page": 14,
          "region_type": "text",
          "region_id": "page14_region9"
        },
        "score": 0.526,
        "citation": "583127822_872908961738101_8157213289131333559_n.pdf, Page 14"
      },
      {
        "text": "tasks (e.g., mathematics, agent, and coding), demonstrating ities of Qwen3-235B-A22B among open-source models.",
        "source": {
          "paper": "2505.09388v1.pdf",
          "page": 15,
          "region_type": "text",
          "region_id": "page15_region8"
        },
        "score": 0.481,
        "citation": "2505.09388v1.pdf, Page 15"
      },
      {
        "text": "important to note that models undergoing",
        "source": {
          "paper": "583127822_872908961738101_8157213289131333559_n.pdf",
          "page": 10,
          "region_type": "title",
          "region_id": "page10_region6"
        },
        "score": 0.478,
        "citation": "583127822_872908961738101_8157213289131333559_n.pdf, Page 10"
      }
    ],
    "sources": [
      "MiniMax-Speech: Autoregressive Text-to-Speech Model (Topic: Text-to-Speech (TTS) Generation) - Pages 1",
      "Research Paper on Machine Learning (Topic: Machine Learning) - Pages 1, 15",
      "Multi-Modal AI Research (Topic: Multi-Modal Learning) - Pages 8",
      "Natural Language Processing Study (Topic: Natural Language Processing) - Pages 19",
      "Computer Vision and Image Processing (Topic: Computer Vision) - Pages 1",
      "Deep Learning Architectures (Topic: Deep Learning) - Pages 3",
      "AI Research and Development (Topic: AI Research) - Pages 6, 10, 14"
    ],
    "has_evidence": true,
    "retrieval_stats": {
      "total_chunks": 10,
      "papers_searched": [
        "583127822_872908961738101_8157213289131333559_n.pdf",
        "2505.09388v1.pdf",
        "2505.11820v2.pdf",
        "2505.19147v3.pdf",
        "2508.03680v1.pdf",
        "2505.07916v1.pdf",
        "2505.10554v2.pdf"
      ],
      "by_paper": {
        "583127822_872908961738101_8157213289131333559_n.pdf": 3,
        "2505.09388v1.pdf": 2,
        "2505.11820v2.pdf": 1,
        "2505.19147v3.pdf": 1,
        "2508.03680v1.pdf": 1,
        "2505.07916v1.pdf": 1,
        "2505.10554v2.pdf": 1
      },
      "by_region_type": {
        "text": 7,
        "title": 3
      }
    },
    "cached_at": "2026-02-11T00:40:13.263578"
  },
  "How do open-source models compare to closed-source alternatives?": {
    "question": "How do open-source models compare to closed-source alternatives?",
    "answer": "The comparison between open-source and closed-source models is an interesting one. According to [Evidence 9], the flagship model Qwen3-235B-A22B, an open-source model, demonstrates state-of-the-art overall performance among open-source models, surpassing strong baselines such as DeepSeek-R1 and DeepSeek-V3. Moreover, it is also highly competitive to closed-source models, such as Gemini2.5-Pro and GPT-40, in both thinking and non-thinking modes [Evidence 9]. This suggests that open-source models can hold their own against closed-source alternatives.\n\nHowever, it's worth noting that the paper on AI Research and Development [Evidence 8] highlights the benefits of model souping, which allows for the aggregation and reuse of open-source models, potentially leading to improved performance. This could be a key advantage for open-source models, as they can be combined and reused to achieve better results.\n\nIn contrast, [Evidence 1] mentions that the yearly increase in computational scale leads to growing inequality in academic access to models, which might limit the accessibility of closed-source models. Nevertheless, the same paper [Evidence 7] suggests that the presented method can propose a new perspective for the OSS community to reuse and revive models of the same architecture, which could help bridge the gap.\n\nOverall, while open-source models may not always match the performance of closed-source models, they can still be highly competitive and offer unique benefits, such as accessibility and reusability.\n\n---\n**Evidence:**\n- Evidence 1 (from AI Research and Development): Discusses the growing inequality in academic access to models due to the yearly increase in computational scale.\n- Evidence 2 (from Research Paper on Machine Learning): Does not directly compare open-source and closed-source models.\n- Evidence 3 (from Computer Vision and Image Processing): Does not address the comparison between open-source and closed-source models.\n- Evidence 4 (from Multi-Modal AI Research): Introduces a new online RL method, but does not compare open-source and closed-source models.\n- Evidence 5 (from Natural Language Processing Study): Asks about the differences between two architectures, but does not address the comparison between open-source and closed-source models.\n- Evidence 6 (from AI Research and Development): Discusses the role of a candidate model in souping, but does not compare open-source and closed-source models.\n- Evidence 7 (from AI Research and Development): Highlights the benefits of model souping for open-source models.\n- Evidence 8 (from AI Research and Development): Presents a method for aggregating and reusing open-source models, which can lead to improved performance.\n- Evidence 9 (from Research Paper on Machine Learning): Compares the performance of an open-source model (Qwen3-235B-A22B) to closed-source models.\n- Evidence 10 (from AI Research and Development): Discusses the benefits of including a strong parent model in souping, but does not compare open-source and closed-source models.\n\n**Sources:**\n- AI Research and Development (Topic: AI Research) - Pages 6, 8, 9\n- Research Paper on Machine Learning (Topic: Machine Learning) - Page 15\n- Computer Vision and Image Processing (Topic: Computer Vision) - Page 1\n- Multi-Modal AI Research (Topic: Multi-Modal Learning) - Page 3\n- Natural Language Processing Study (Topic: Natural Language Processing) - Page 19",
    "evidence": [
      {
        "text": "Impact: While the yearly increase in computational scale leads to growing inequality in academic access to models (see the detailed analysis in Kogkalidis and Chatzikyriakidis (2024)), the presented method proposes accessible low cost opportunities for the broader community efforts and also promotes the iterative reuse of",
        "source": {
          "paper": "583127822_872908961738101_8157213289131333559_n.pdf",
          "page": 9,
          "region_type": "text",
          "region_id": "page9_region15"
        },
        "score": 0.52,
        "citation": "583127822_872908961738101_8157213289131333559_n.pdf, Page 9"
      },
      {
        "text": "tasks (e.g., mathematics, agent, and coding), demonstrating ities of Qwen3-235B-A22B among open-source models.",
        "source": {
          "paper": "2505.09388v1.pdf",
          "page": 15,
          "region_type": "text",
          "region_id": "page15_region8"
        },
        "score": 0.416,
        "citation": "2505.09388v1.pdf, Page 15"
      },
      {
        "text": "of performance ever-increasing com- early 2024, the overhead was growth in parame- requirements. the research model-centric model quanti- al., 2023), net- et al., 2024), al., 2015; Gou (Yu et al., 2020). overhead by natural response model size was gains. limits, the Meanwhile, emerged: the",
        "source": {
          "paper": "2505.19147v3.pdf",
          "page": 1,
          "region_type": "text",
          "region_id": "page1_region2"
        },
        "score": 0.346,
        "citation": "2505.19147v3.pdf, Page 1"
      },
      {
        "text": "well. Kumar et al. proposes SCoRe, an online RL method where a model improves its own answers, bridging the offline self-correction gap. Several",
        "source": {
          "paper": "2505.10554v2.pdf",
          "page": 3,
          "region_type": "text",
          "region_id": "page3_region7"
        },
        "score": 0.306,
        "citation": "2505.10554v2.pdf, Page 3"
      },
      {
        "text": "What are the differences between the Chain-of-Model (CoM) and Mixture-of-Experts architectures?",
        "source": {
          "paper": "2505.11820v2.pdf",
          "page": 19,
          "region_type": "text",
          "region_id": "page19_region6"
        },
        "score": 0.3,
        "citation": "2505.11820v2.pdf, Page 19"
      },
      {
        "text": "is useful beyond tool the candidate models is still helpful in souping with SoCE compared to the role in performance",
        "source": {
          "paper": "583127822_872908961738101_8157213289131333559_n.pdf",
          "page": 6,
          "region_type": "text",
          "region_id": "page6_region5"
        },
        "score": 0.381,
        "citation": "583127822_872908961738101_8157213289131333559_n.pdf, Page 6"
      },
      {
        "text": "aggregation and reuse within the community. that the presented method can propose a new perspective for the OSS community to reuse and revive models of the same architecture. Acknowledgments like to express our sincere gratitude to Andrew Budker and Ricardo Silveira Cabral for their support throughout We also thank Shishir Patil for his insightful advice on the Berkeley Function Calling Leaderboard evaluations.",
        "source": {
          "paper": "583127822_872908961738101_8157213289131333559_n.pdf",
          "page": 9,
          "region_type": "text",
          "region_id": "page9_region0"
        },
        "score": 0.363,
        "citation": "583127822_872908961738101_8157213289131333559_n.pdf, Page 9"
      },
      {
        "text": "benchmarks). By souping models, we allow covering these different skills well, as showcased by an improvement overall metric. The Shapley value indicates the relative contributions of the sub-models using souping as the function in the context of the skills of the set of all models. our Shapley computation by the following parameters; MGSM as the benchmark, the performance metric is accuracy, the characteristic function is souping and the set of candidates are (1) 4 open-source (OSS) models on LLama-7B (2) 6 pairs of the OSS models(3) 4 possible triplets these for model souping using a set of four models, evaluating all possible combinations. Separately, we also apply the SoCE framework to the same set. in Figure 6b, our analysis reveals that model contributions are not uniform; candidates and subsets selected exhibit significantly higher Shapley values. This finding underscores the critical role of SoCE’s candidate",
        "source": {
          "paper": "583127822_872908961738101_8157213289131333559_n.pdf",
          "page": 8,
          "region_type": "text",
          "region_id": "page8_region2"
        },
        "score": 0.355,
        "citation": "583127822_872908961738101_8157213289131333559_n.pdf, Page 8"
      },
      {
        "text": "Our flagship model, Qwen3-235B-A22B, demonstrates the state-of-the-art overall performance among open-source models in both the thinking and non-thinking modes, surpassing strong baselines such as DeepSeek-R1 and DeepSeek-V3. Qwen3-235B-A22B is also highly competitive to closed-source models, such Gemini2.5-Pro, and GPT-40, its",
        "source": {
          "paper": "2505.09388v1.pdf",
          "page": 15,
          "region_type": "text",
          "region_id": "page15_region7"
        },
        "score": 0.352,
        "citation": "2505.09388v1.pdf, Page 15"
      },
      {
        "text": "mean to 37.0). contrast, consistently including a strong parent model, as candidates, reliably increases the average score. We also extend the same method of analyses to other benchmarks Figures 6c and 6a). Discussion general case, the presented method can be further explored as a broader solution for adding new domain or",
        "source": {
          "paper": "583127822_872908961738101_8157213289131333559_n.pdf",
          "page": 9,
          "region_type": "text",
          "region_id": "page9_region1"
        },
        "score": 0.351,
        "citation": "583127822_872908961738101_8157213289131333559_n.pdf, Page 9"
      }
    ],
    "sources": [
      "Research Paper on Machine Learning (Topic: Machine Learning) - Pages 15",
      "Multi-Modal AI Research (Topic: Multi-Modal Learning) - Pages 3",
      "Natural Language Processing Study (Topic: Natural Language Processing) - Pages 19",
      "Computer Vision and Image Processing (Topic: Computer Vision) - Pages 1",
      "AI Research and Development (Topic: AI Research) - Pages 6, 8, 9"
    ],
    "has_evidence": true,
    "retrieval_stats": {
      "total_chunks": 10,
      "papers_searched": [
        "583127822_872908961738101_8157213289131333559_n.pdf",
        "2505.09388v1.pdf",
        "2505.19147v3.pdf",
        "2505.10554v2.pdf",
        "2505.11820v2.pdf"
      ],
      "by_paper": {
        "583127822_872908961738101_8157213289131333559_n.pdf": 5,
        "2505.09388v1.pdf": 2,
        "2505.19147v3.pdf": 1,
        "2505.10554v2.pdf": 1,
        "2505.11820v2.pdf": 1
      },
      "by_region_type": {
        "text": 10
      }
    },
    "cached_at": "2026-02-11T00:40:27.017320"
  },
  "What improvements are achieved through supervised fine-tuning?": {
    "question": "What improvements are achieved through supervised fine-tuning?",
    "answer": "According to the paper on Natural Language Processing Study [Evidence 2], Chain-tuning can boost performance by just fine-tuning approximately 42% of the model parameters. This suggests that supervised fine-tuning can lead to significant improvements in model performance.\n\nAdditionally, the paper on Multi-Modal AI Research [Evidence 6] mentions that parameter-space model merging improves average accuracy across math, coding, and science by 2% on a 32B model over the instruction-tuned baseline. This implies that supervised fine-tuning can also lead to improvements in multi-modal learning tasks.\n\nHowever, the paper on AI Research and Development [Evidence 4] warns against souping of checkpoints from different models, as it can lead to the inheritance of unwanted characteristics. This suggests that careful consideration is needed when applying supervised fine-tuning to avoid potential drawbacks.\n\nThe provided evidence does not contain sufficient information to determine the specific improvements achieved through supervised fine-tuning in other areas, such as text-to-speech generation or computer vision.\n\n---\n**Evidence:**\n- Evidence 2 (from Natural Language Processing Study): Chain-tuning can boost performance by just fine-tuning approximately 42% of the model parameters.\n- Evidence 6 (from Multi-Modal AI Research): Parameter-space model merging improves average accuracy across math, coding, and science by 2% on a 32B model over the instruction-tuned baseline.\n- Evidence 4 (from AI Research and Development): Souping of checkpoints from different models can lead to the inheritance of unwanted characteristics.\n\n**Sources:**\n- \"Natural Language Processing Study\" (Topic: Natural Language Processing) - Pages 8-9\n- \"Multi-Modal AI Research\" (Topic: Multi-Modal Learning) - Pages 2-3\n- \"AI Research and Development\" (Topic: AI Research) - Page 10\n- \"Hunyuan Foundation Model\" (Topic: Foundation Models) - Page 6\n- \"Deep Learning Architectures\" (Topic: Deep Learning) - Page 8\n- \"Research Paper on Machine Learning\" (Topic: Machine Learning) - Page 10\n- \"MiniMax-Speech: Autoregressive Text-to-Speech Model\" (Topic: Text-to-Speech (TTS) Generation) - Page 18\n- \"Computer Vision and Image Processing\" (Topic: Computer Vision) - Page 5\n- \"Natural Language Processing Study\" (Topic: Natural Language Processing) - Page 8\n- \"Multi-Modal AI Research\" (Topic: Multi-Modal Learning) - Page 3",
    "evidence": [
      {
        "text": "fine-tuning (SFT), and human feedback alignment (RLHF), applied separately",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 6,
          "region_type": "text",
          "region_id": "page6_region3"
        },
        "score": 0.612,
        "citation": "2511.18870v2.pdf, Page 6"
      },
      {
        "text": "the baseline, and then fine-tune the expanded chain. To demonstrate this capability, we benchmark [29] for a faster validation. We report the results within Table 4, and more details can refer to Appendix A.1. From Table 4, we observe Chain-tuning can boost performance by just fine-tuning approximately 42% of the model parameters. Besides, it is with parameter-efficient fine-tuning approaches, such as LoRA [30]. dev set results. Each task is reported by accuracy. “+ CT” means Chain Tuning, and brackets mean the fine-tuned parameters and the total parameters. SST-2 COLA MNLI MRPC QNLI QQP RTE WNILI Avg. 67.89 32.98 35.87 49.02 51.77 51.65 54.87 43.66 48.46 92.32 54.87 82.26 62.01 82.87 5.22 59.21 53.52 67.79",
        "source": {
          "paper": "2505.11820v2.pdf",
          "page": 9,
          "region_type": "text",
          "region_id": "page9_region2"
        },
        "score": 0.556,
        "citation": "2505.11820v2.pdf, Page 9"
      },
      {
        "text": "a separate to simplify training. For instance, (Shao et al., 2024) advantage by normalizing the reward of a response against the mean and standard deviation of",
        "source": {
          "paper": "2508.03680v1.pdf",
          "page": 8,
          "region_type": "title",
          "region_id": "page8_region6"
        },
        "score": 0.509,
        "citation": "2508.03680v1.pdf, Page 8"
      },
      {
        "text": "an development leaking any in the model training practice: In this work, we have only tested the souping of the ’ final’ posttrained checkpoints. As other works have shown, the models can be souped after pre-training Li et al. (2025), after Wortsman et al. (2022), with adapters’. We do not recommend souping of the checkpoints from different as well as souping unaligned or uncensored models with the aligned ones to avoid the inheritance of",
        "source": {
          "paper": "583127822_872908961738101_8157213289131333559_n.pdf",
          "page": 10,
          "region_type": "text",
          "region_id": "page10_region3"
        },
        "score": 0.493,
        "citation": "583127822_872908961738101_8157213289131333559_n.pdf, Page 10"
      },
      {
        "text": "greater improvement (RL) phase. To achieve this objective effectively, it is preferable to minimize both the number of samples and the training steps during this preparatory phase. Reasoning RL query-verifier pairs used in the Reasoning RL stage must satisfy the following four criteria: (1) They used during the cold-start phase. (2) They are learnable for the cold-start model. (3) They are challenging as possible. (4) They cover a broad range of sub-domains. We ultimately collect a total query-verifier pairs, and employed GRPO (Shao et al., 2024) to update the model parameters. observe that using a large batch size and a high number of rollouts per query, along with off-policy to improve sample efficiency, is beneficial to the training process. We have also addressed how and the model’s to increase remain",
        "source": {
          "paper": "2505.09388v1.pdf",
          "page": 10,
          "region_type": "text",
          "region_id": "page10_region2"
        },
        "score": 0.474,
        "citation": "2505.09388v1.pdf, Page 10"
      },
      {
        "text": "raises overall accuracy by more than 10% relative to a vanilla instruction-tuned incorporate the three competencies into a single network, Rather than training the model task corpus, we utilize parameter-space model merging to integrate these meta-abilities. merging improves average accuracy across math, coding, and science by 2% on and 4% on a 32B model over the instruction-tuned baseline, demonstrating the strong of merged meta-abilities. to evaluate whether meta-ability alignment offers a stronger foundation for subsequent resumed domain-specific RL training from a checkpoint that had already been aligned it with the same procedure applied to an instruction-tuned model. Starting from the checkpoint raises the attainable performance ceiling: after identical continual domain- training, the model achieves an average gain of about 2% over its instruction-only Our key contributions are as follows:",
        "source": {
          "paper": "2505.10554v2.pdf",
          "page": 2,
          "region_type": "text",
          "region_id": "page2_region4"
        },
        "score": 0.469,
        "citation": "2505.10554v2.pdf, Page 2"
      },
      {
        "text": "Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning Prompt tuning can be comparable to fine-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602, 2021. Ma¢kiewicz and Waldemar Ratajczak. Principal components analysis (pca). Computers & Geosciences, 19(3):303-342, 1993. Mehta, Ruibo Tu, Jonas Beskow, Eva Székely, and Gustav Eje Henter. Matcha-tts: A fast tts architecture with conditional flow matching. In ICASSP 2024-2024 IEEE International Conference Acoustics, Speech and Signal Processing (ICASSP), pages 11341-11345. IEEE, 2024. Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In International conference on machine learning, pages 28492-28518. PMLR, 2023. Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International conference on machine learning, pages 1530-1538. PMLR, 2015. Tan, Jiawei Chen, Haohe Liu, Jian Cong, Chen Zhang, Yanqing Liu, Xi Wang, Yichong Leng, Yuanhao Yi, Lei He, et al. Naturalspeech: End-to-end text-to-speech synthesis with human-level quality. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(6):4234-4245, 2024.",
        "source": {
          "paper": "2505.07916v1.pdf",
          "page": 18,
          "region_type": "list",
          "region_id": "page18_region0"
        },
        "score": 0.443,
        "citation": "2505.07916v1.pdf, Page 18"
      },
      {
        "text": "preserves information through weighted of tokens, offering a more nuanced than direct elimination. Training and Inference Targets - How Data-centric Compression Benefits? Stage Data-centric compression meth- contribute to improving both the quality and of model training. These benefits can be categorized into two aspects: enhancing quality and increasing training efficiency. Enhancing Training Quality Improvement in quality can be achieved through methods quality to training et al., 2024b; Lee et al., 2022a; Wenzek et al., 2019; Gao et 2024b; Wang et al., 2025b). include rule-based heuristics Penedo et al., 2024), deduplication et al., 2022a; Penedo et al., 2023), and scoring strategies guage models (Wenzek et al., 2020; Li et al., 2024b; Wettig et et al., 2024; Wang et al., 2025a). Formally, consider a where each X; =",
        "source": {
          "paper": "2505.19147v3.pdf",
          "page": 5,
          "region_type": "text",
          "region_id": "page5_region10"
        },
        "score": 0.427,
        "citation": "2505.19147v3.pdf, Page 5"
      },
      {
        "text": "partially reduce the tuning costs and mitigate catastrophic forgetting via preserving the",
        "source": {
          "paper": "2505.11820v2.pdf",
          "page": 8,
          "region_type": "text",
          "region_id": "page8_region1"
        },
        "score": 0.549,
        "citation": "2505.11820v2.pdf, Page 8"
      },
      {
        "text": "well. Kumar et al. proposes SCoRe, an online RL method where a model improves its own answers, bridging the offline self-correction gap. Several",
        "source": {
          "paper": "2505.10554v2.pdf",
          "page": 3,
          "region_type": "text",
          "region_id": "page3_region7"
        },
        "score": 0.456,
        "citation": "2505.10554v2.pdf, Page 3"
      }
    ],
    "sources": [
      "MiniMax-Speech: Autoregressive Text-to-Speech Model (Topic: Text-to-Speech (TTS) Generation) - Pages 18",
      "Research Paper on Machine Learning (Topic: Machine Learning) - Pages 10",
      "Multi-Modal AI Research (Topic: Multi-Modal Learning) - Pages 2, 3",
      "Natural Language Processing Study (Topic: Natural Language Processing) - Pages 8, 9",
      "Computer Vision and Image Processing (Topic: Computer Vision) - Pages 5",
      "Deep Learning Architectures (Topic: Deep Learning) - Pages 8",
      "Hunyuan Foundation Model (Topic: Foundation Models) - Pages 6",
      "AI Research and Development (Topic: AI Research) - Pages 10"
    ],
    "has_evidence": true,
    "retrieval_stats": {
      "total_chunks": 10,
      "papers_searched": [
        "2511.18870v2.pdf",
        "2505.11820v2.pdf",
        "2508.03680v1.pdf",
        "583127822_872908961738101_8157213289131333559_n.pdf",
        "2505.09388v1.pdf",
        "2505.10554v2.pdf",
        "2505.07916v1.pdf",
        "2505.19147v3.pdf"
      ],
      "by_paper": {
        "2511.18870v2.pdf": 1,
        "2505.11820v2.pdf": 2,
        "2508.03680v1.pdf": 1,
        "583127822_872908961738101_8157213289131333559_n.pdf": 1,
        "2505.09388v1.pdf": 1,
        "2505.10554v2.pdf": 2,
        "2505.07916v1.pdf": 1,
        "2505.19147v3.pdf": 1
      },
      "by_region_type": {
        "text": 8,
        "title": 1,
        "list": 1
      }
    },
    "cached_at": "2026-02-11T00:40:30.918925"
  },
  "What tasks can the models perform besides text-to-video?": {
    "question": "What tasks can the models perform besides text-to-video?",
    "answer": "The Hunyuan Foundation Model can perform tasks beyond text-to-video generation, including image-to-video generation, text generation, and multimodal understanding. According to [Evidence 9], the Hunyuan Foundation Model assesses text-to-video generation using a comprehensive rating, but it also evaluates image-to-video generation, which encompasses image-video consistency, instruction responsiveness, visual quality, and structural motion effects. This suggests that the model has the capability to perform image-to-video generation.\n\nAdditionally, [Evidence 3] mentions that the Hunyuan Foundation Model utilizes a large multimodal model for bilingual (Chinese-English) understanding, combined with ByT5 for dedicated text generation accuracy in videos. This implies that the model can perform text generation tasks.\n\nFurthermore, [Evidence 8] discusses the trend of research shifting from model-centric compression to compression, and mentions that the focus of research is now on efficient compression techniques for various domains, including text generation in AI content creation. This suggests that the Hunyuan Foundation Model can perform text generation tasks.\n\nLastly, [Evidence 1] mentions that the model can perform multimodal understanding, which involves understanding both text and vision domains.\n\n---\n\n**Evidence:**\n\n- Evidence 1 (from Text-to-Video Generation with Physical Consistency): The Hunyuan Foundation Model can perform multimodal understanding, involving understanding both text and vision domains.\n- Evidence 3 (from Hunyuan Foundation Model): The Hunyuan Foundation Model can perform image-to-video generation, text generation, and multimodal understanding.\n- Evidence 8 (from Computer Vision and Image Processing): The focus of research is now on efficient compression techniques for various domains, including text generation in AI content creation.\n- Evidence 9 (from Hunyuan Foundation Model): The Hunyuan Foundation Model assesses text-to-video generation and image-to-video generation using a comprehensive rating.\n\n**Sources:**\n\n- Text-to-Video Generation with Physical Consistency (Topic: Video Generation) - Pages 10, 11\n- Hunyuan Foundation Model (Topic: Foundation Models) - Pages 2, 8, 9\n- Computer Vision and Image Processing (Topic: Computer Vision) - Pages 8, 9, 10, 17",
    "evidence": [
      {
        "text": "dels are real-time game engines. In ICLR, 2025. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, 2017. Bram Wallace, Meihua Dang, Rafael Rafailov, Lingi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In CVPR, 2024. Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Duomin Wang, Wei Zuo, Aojie Li, Ling-Hao Chen, Xinyao Liao, Deyu Zhou, Zixin Yin, Xili Dai, Daxin Jiang, and Gang Yu. Universe-1: Unified audio-video generation via stitching of experts. arXiv preprint arXiv:2509.06155, 2025. Qinghe Wang, Yawen Luo, Xiaoyu Shi, Xu Jia, Huchuan Lu, Tianfan Xue, Xintao Wang, Pengfei Wan, Di Zhang, and Kun Gai. Cinemaster: A 3d-aware and controllable framework for cinematic text-to-video generation. In SIGGRAPH, 2025. Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, Jiagang Zhu, and Jiwen Lu. Drivedreamer: Towards real-world- drive world models for autonomous driving. In ECCV, 2024. Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. IJCV, 2025. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022. Yuqing Wen, Yucheng Zhao, Yingfei Liu, Fan Jia, Yanhui Wang, Chong Luo, Chi Zhang, Tiancai Wang, Xiaoyan Sun, and Xiangyu Zhang. Panacea: Panoramic and controllable video generation for autonomous driving. In CVPR, 2024.",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 13,
          "region_type": "list",
          "region_id": "page13_region0"
        },
        "score": 0.548,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 13"
      },
      {
        "text": "Xiv:1805.09501. 2024. FlashAttention-2: Faster attention with parallelism and work partitioning. In Inter- Conference on Learning Representations 10 Alessio Devoto, Yu Zhao, Pasquale Minervini. 2024. 1_2 norm-based strategy for kv In Proceedings of the 2024 cal Methods in Natural Language 18476-18499, Miami, Florida, Computational Linguistics. Xiaoyi Dong, Pan Zhang, Cao, Bin Wang, Linke Ouyang, Haodong Duan, Wenwei others. 2024. Internlm-xcomposer2-4khd: ing large vision-language model from 336 pixels to 4k hd. mation Processing Systems, Jiafei Duan, Samson Yu, Hui Li and Cheston Tan. 2022. A From simulators to research tions on Emerging Topics in gence, 6(2):230-244. Yuchen Duan, Weiyun Wang, Lewei Lu, Tong Lu, Yu Qiao, Dai, and Wenhai Wang. 2024. cient and scalable visual architectures. arXiv preprint Elias Frantar, Saleh Ashkboos, Dan Alistarh. 2022. GPTQ: quantization for generative In Proceedings of the Learning Representations. Chaoyou Fu, Yuhan Dai, Shuhuai Ren, Renrui Zhang, Zhou, Yunhang Shen, Mengdan ers. 2024. Video-mme: The evaluation benchmark of analysis. arXiv preprint Cong Gao, Geng Wang, Weisong and Yanping Chen. 2021. rity: State of the art and challenges. Things Journal, 9(10):7572-7595. Leo Gao, Stella Biderman, Sid ing, Travis Hoppe, Charles Horace He, Anish Thite, others. 2020. The pile: An verse text for language arXiv:2101.00027. Shanghua Gao, Pan Zhou, Shuicheng Yan. 2023. Masked is a strong image synthesizer. IEEE/CVF Conference on tern Recognition, pages Suyu Ge, Yunan Zhang, Liyuan Jiawei Han, and Jianfeng Gao.",
        "source": {
          "paper": "2505.19147v3.pdf",
          "page": 10,
          "region_type": "list",
          "region_id": "page10_region1"
        },
        "score": 0.508,
        "citation": "2505.19147v3.pdf, Page 10"
      },
      {
        "text": "parameter count while fully leveraging the potential of the model, yielding comparable to state-of-the-art video generation models. Super-Resolution Enhancement: We develop an efficient few-step super-resolution upscales outputs to 1080p. It enhances sharpness while correcting distortions, details and overall visual texture. Optimization: We introduce a novel Selective and Sliding Tile Attention mechanism that dynamically prunes redundant spatiotemporal tokens. This signifi- computational overhead for long video sequences and accelerates inference, end-to-end speedup of 1.87x in 10-second 720p video synthesis compared to [7]. Multimodal Understanding: Our framework utilizes a large multimodal model bilingual (Chinese-English) understanding, combined with ByT5 for dedicated to enhance text generation accuracy in videos. Additionally, detailed captions are generated for both images and videos. Training Optimization: We demonstrate that the Muon optimizer [8] signifi- convergence in video generation model training, while the multi-phase training strategy—spanning from pre-training to post-training stages—enhances aesthetic quality, and alignment with human preferences, thereby enabling of professional-grade content. comprehensive pipeline, beginning with meticulous data preparation—including captioning, as detailed in Section|2| Section [3]introduces the core architecture such as the unified diffusion transformer, the video super-resolution network, sparse attention. Section |4|describes training strategies for text-to-video and covering both pre-training and post-training methodologies. Finally, Section[5| evaluation of the model’s performance against state-of-the-art models using qualitative benchmarks. and Filtering comprises both image and video data. For image data, we adopted the acquisition outlined in [9], curating 5 billion images from a pool of over 10 billion for",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 2,
          "region_type": "list",
          "region_id": "page2_region4"
        },
        "score": 0.501,
        "citation": "2511.18870v2.pdf, Page 2"
      },
      {
        "text": "am Davtyan, Sepehr Sameni, and Paolo Favaro. Efficient video prediction via sparsely conditioned flow matching. In ICCV, 2023. Google DeepMind. Veo-3: A scalable and controllable text-to-video model. https://storage.googleapis.com/ deepmind-media/veo/Veo-3-Tech-Report.pdf, 2025. Technical Report, Google DeepMind. 11",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 11,
          "region_type": "list",
          "region_id": "page11_region0"
        },
        "score": 0.502,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 11"
      },
      {
        "text": "Scardapane, and and effective compression. on Empiri- Processing, pages Association for Zang, Yuhang Songyang Zhang, Yining Li, and 1 A pioneer- handling resolutions in Neural Infor- 37:42566-42592. Hongyuan Zhu, of embodied ai: [EEE Transac- Computational Intelli- Chen, Xizhou Zhu, Hongsheng Li, Jifeng Vision-rwkv: Effi- with rwkv-like arXiv:2403.02308. Hoefler, and post-training transformers. Conference on Luo, Lei Li, Wang, Chenyu Zhang, and | oth- comprehensive Ilms in video Zhongmin Wang, driving secu- IEEE Internet of Laurence Gold- Jason Phang, Nabeshima, and 1 dataset of di- arXiv preprint Cheng, and diffusion transformer Proceedings of the Vision and Pat- Minjia Zhang, Model tells you",
        "source": {
          "paper": "2505.19147v3.pdf",
          "page": 10,
          "region_type": "list",
          "region_id": "page10_region0"
        },
        "score": 0.502,
        "citation": "2505.19147v3.pdf, Page 10"
      },
      {
        "text": "images in InternVL3 (Zhu et al., 2025) 10K-frame videos in Video-XL-2 (Qin et al., enabling strong performance on image (Bai 2025), video (Yang et al., 2025b), and multi- reasoning tasks (Shen et al., 2025). More Complex Content in Generation AI content generation has advanced sig- with the application of Transformers to domains (Peebles and Xie, 2023; Brooks 2024; Li et al., 2024f). Early diffusion mod- Stable Diffusion (Rombach et al., 2022) only 512 x 512 resolution images. With now successfully applied to gener- (Peebles and Xie, 2023; Gao et al., 2023; et al., 2024; Li et al., 2024f), DiT-based produce high-quality 4K images in PixArt- et al., 2024a) and hour-long videos in (Brooks et al., 2024). These models capture spatiotemporal dependencies, enabling content generation (Labs, 2024; Yang 2024c; Wan et al., 2025a; Kang et al., 2025). these advancements across domains have outstanding performance, they now et al., 2024b; Zou et al., 2025). (1) Efficient Computation Since computational efficiency architectural design, optimizing tal. Unlike Transformers with complexity O(n?) (Vaswani methods achieve linear or (i) linear attention reformulates complexity (Katharopoulos et 2021); (ii) RWKV combines ing with transformer parallelism Duan et al., 2024); (iii) State Mamba use structured state plexity (Gu and Dao, 2023; quire retraining, motivating (II) Model-centric Compression ing parameter complexity and memory costs. Model centric, transforming W to a W’=T(W), where with I as the compression",
        "source": {
          "paper": "2505.19147v3.pdf",
          "page": 3,
          "region_type": "text",
          "region_id": "page3_region7"
        },
        "score": 0.495,
        "citation": "2505.19147v3.pdf, Page 3"
      },
      {
        "text": "Text-to-Video Generation Models generation [5-7, 18, 25-27, 30, 62, 71, 74, 77| has witnessed significant progress. Many existing works follow [49] to adopt a Transformer [56] to predict the noise in diffusion [9, 22, 24, 35, 37, 42-44, 51] or estimate the field in flow matching [8, 14, 19, 23, 29, 31, 34, 58, 59, 73, 75]. Although high visual quality is achieved, modeling the underlying physics-related effects remains challenging. Two recent works, DiffPhy [72] PhyT2V [67], adopt an LLM to extend the text prompt with explicit physics laws and phenomena, and then generates the video or finetunes a T2V model with extended prompts. Yet, these prompt extension-based are easily misled by the mistakes of LLM and struggle to learn the implicit physics. Different from these we aim to explore the physics reasoning potential of the T2V model itself.",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 10,
          "region_type": "text",
          "region_id": "page10_region3"
        },
        "score": 0.491,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 10"
      },
      {
        "text": "parameters to context processing. As shown below, both in text and vision domains, size growth has significantly slowed, while length continues to increase. This trend that the focus of research for efficient shifting from model-centric compression to compression. Comparison of Token Compression Methods and Random Token Dropping section presents a detailed comparison be- designed token compression methods and token dropping, the simplest baseline. The supports arguments in Section 4.1, show- current token compression techniques have limitations. Experiments span multi- domains: complex reasoning in language, im- video understanding in vision, and text- generation in AI content creation. This marks including GQA 2019) and MMB (Liu et al., 25% of visual tokens, following evaluation scripts!. We et al., 2024b), SparseVLM random dropping, and pooling. Figure 3 (b) shows that pooling outperform some attribute this to their shared formity, which mitigates attention-based methods like lights the negative impact of tion scores. We thus advocate as a key design principle in VideoLLMs: Video ated LLaVA-OneVision-7B video benchmarks: MVBench Long VideoBench (Wu et al., et al., 2024), and VideoMME",
        "source": {
          "paper": "2505.19147v3.pdf",
          "page": 17,
          "region_type": "text",
          "region_id": "page17_region1"
        },
        "score": 0.481,
        "citation": "2505.19147v3.pdf, Page 17"
      },
      {
        "text": "TableB]and Table/4] we assess text-to-video generation using a comprehensive rating that considers five key dimensions: text-video consistency, aesthetic quality of individual quality, structural stability, and motion effects. For image-to-video generation, the encompasses image-video consistency, instruction responsiveness, visual quality, structural motion effects.",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 8,
          "region_type": "text",
          "region_id": "page8_region0"
        },
        "score": 0.479,
        "citation": "2511.18870v2.pdf, Page 8"
      },
      {
        "text": "Weber. 2019. Common voice: A massively- multilingual speech corpus. arXiv preprint Hameed Azeemi, Ihsan Qazi, and Agha Ali Raza. Data pruning for efficient model pruning in machine translation. In Findings of the Associ- for Computational Linguistics: EMNLP 2023, 236-246. Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Lu, and 29 others. 2023a. Qwen technical arXiv preprint arXiv:2309.16609. Shuai Bai, Shusheng Yang, Shijie Wang, Tan, Peng Wang, Junyang Lin, Chang Zhou, Jingren Zhou. 2023b. Qwen-VL: A frontier large vision-language model with versatile abilities. arXiv arXiv:2308.12966. Chang, Junjie Hu, midkv: Dynamic kv caching pyramidal information arXiv:2406.02069. Jiaao Chen, Zichao Yang, and Diyi Linguistically-informed for semi-supervised text ings of the 58th Annual Meeting for Computational Linguistics, Junsong Chen, Chongjian Ge, Lewei Yao, Xiaozhe Ren, Luo, Huchuan Lu, and Zhenguo Weak-to-strong training of 4k text-to-image generation. ence on Computer Vision, pages Liang Chen, Haozhe Zhao, Tianyu yang Lin, Chang Zhou, and An image is worth 1/2 tokens play inference acceleration for models. In Proceedings of the on Computer Vision.",
        "source": {
          "paper": "2505.19147v3.pdf",
          "page": 9,
          "region_type": "list",
          "region_id": "page9_region3"
        },
        "score": 0.477,
        "citation": "2505.19147v3.pdf, Page 9"
      }
    ],
    "sources": [
      "Computer Vision and Image Processing (Topic: Computer Vision) - Pages 3, 9, 10, 17",
      "Hunyuan Foundation Model (Topic: Foundation Models) - Pages 2, 8",
      "Text-to-Video Generation with Physical Consistency (Topic: Video Generation) - Pages 10, 11, 13"
    ],
    "has_evidence": true,
    "retrieval_stats": {
      "total_chunks": 10,
      "papers_searched": [
        "605562216_1287553579874010_8068650067268945856_n.pdf",
        "2505.19147v3.pdf",
        "2511.18870v2.pdf"
      ],
      "by_paper": {
        "605562216_1287553579874010_8068650067268945856_n.pdf": 3,
        "2505.19147v3.pdf": 5,
        "2511.18870v2.pdf": 2
      },
      "by_region_type": {
        "list": 6,
        "text": 4
      }
    },
    "cached_at": "2026-02-11T00:40:39.233080"
  },
  "How is motion quality and temporal consistency improved?": {
    "question": "How is motion quality and temporal consistency improved?",
    "answer": "The Hunyuan Foundation Model enhances motion quality and temporal consistency through several techniques. Firstly, it utilizes a checkpoint to enable high-quality Super-Fine-Tuning (SFT), which rigorously selects clips per task, filtered strictly considering aesthetic and motion smoothness [Evidence 4]. This results in evident improvements in visual quality, aesthetic appeal, and temporal consistency for both generation tasks [Evidence 4]. Additionally, the model employs Reinforcement Learning from Human Feedback (RLHF) strategies tailored for T2V tasks, mainly aimed at correcting artifacts and enhancing motion quality [Evidence 4]. Furthermore, the model applies online reinforcement learning (RL) during post-training to correct motion quality [Evidence 4]. The model also uses a Video Super-Resolution Network to upscale 1080p, significantly enhancing the final visual fidelity [Evidence 5]. Moreover, the model filters out low-quality videos through aesthetic scoring and noise & artifacts evaluation, allowing it to focus on high-quality video segments [Evidence 6]. This results in precise and nuanced aesthetic quality applications [Evidence 6].\n\nThe Text-to-Video Generation with Physical Consistency paper also shows improvements in motion quality and temporal consistency. The authors propose a method that generates videos with more realistic physical dynamics, deformation-free body motion, and object interactions, effectively capturing phenomena such as force transfer, material deformation, light and flame propagation [Evidence 2]. This is demonstrated through visual comparisons on challenging physical actions or phenomena, where the proposed method shows superior performance compared to existing open- and closed-source models [Evidence 2]. The authors also conduct a visual analysis in Fig. 7 (a) for their proposed method, showing that it generates coherent body motion and improves physics plausibility [Evidence 7].\n\nOverall, the Hunyuan Foundation Model and the Text-to-Video Generation with Physical Consistency paper both demonstrate improvements in motion quality and temporal consistency through various techniques, including checkpoint-based SFT, RLHF, online RL, video super-resolution, and aesthetic scoring.\n\n---\n**Evidence:**\n- Evidence 1 (from Foundation Models): The title of the paper mentions \"overall video perception quality\".\n- Evidence 2 (from Video Generation): The paper shows visual comparisons on challenging physical actions or phenomena, demonstrating the proposed method's ability to generate videos with more realistic physical dynamics.\n- Evidence 3 (from Foundation Models): The paper states that the Hunyuan Foundation Model enhances visual quality, motion stability, and overall temporal coherence.\n- Evidence 4 (from Foundation Models): The paper describes the use of checkpoint-based SFT, RLHF, and online RL to improve motion quality and temporal consistency.\n- Evidence 5 (from Foundation Models): The paper mentions the use of a Video Super-Resolution Network to upscale 1080p and enhance visual fidelity.\n- Evidence 6 (from Foundation Models): The paper describes the use of aesthetic scoring and noise & artifacts evaluation to filter out low-quality videos.\n- Evidence 7 (from Video Generation): The paper shows a visual analysis in Fig. 7 (a) demonstrating the proposed method's ability to generate coherent body motion and improve physics plausibility.\n\n**Sources:**\n- \"Hunyuan Foundation Model\" (Topic: Foundation Models) - Pages 1, 3, 7, 8\n- \"Text-to-Video Generation with Physical Consistency\" (Topic: Video Generation) - Pages 9, 10",
    "evidence": [
      {
        "text": "on overall video perception quality.",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 8,
          "region_type": "title",
          "region_id": "page8_region11"
        },
        "score": 0.6,
        "citation": "2511.18870v2.pdf, Page 8"
      },
      {
        "text": "Results. The visual comparisons on challenging physical actions or phenomena including gymnastics, basketball, glass smashing, polo, squash, handspring, refraction, and combustionare shown in Fig. 1, 4, 5, Our method generates videos with more realistic physical dynamics, deformation-free body motion, and object interactions, effectively capturing phenomena such as force transfer, material deformation, light and flame propagation. Compared to existing open- and closed-source models, PhyGDPO shows superior",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 9,
          "region_type": "text",
          "region_id": "page9_region4"
        },
        "score": 0.461,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 9"
      },
      {
        "text": "enhances visual quality, motion stability, and overall temporal coherence.",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 8,
          "region_type": "text",
          "region_id": "page8_region5"
        },
        "score": 0.576,
        "citation": "2511.18870v2.pdf, Page 8"
      },
      {
        "text": "checkpoint, enabling upon a high-quality SFT, we utilize rigorously selected clips per task, filtered strictly considering aesthetic and motion smoothness. Through CT and SFT, we observe evident improvements in visual quality, aesthetic appeal, and temporal consistency for both generation tasks. employ Reinforcement Learning from Human Feedback (RLHF) strategies tailored for T2V tasks, both mainly aimed at correcting artifacts and enhancing motion quality. task, we apply online reinforcement learning (RL) during post-training to correct",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 7,
          "region_type": "text",
          "region_id": "page7_region1"
        },
        "score": 0.557,
        "citation": "2511.18870v2.pdf, Page 7"
      },
      {
        "text": "producing initial video sequences with resolutions from 480p to 720p and durations of 5 The second stage utilizes a dedicated Video Super-Resolution Network to upscale 1080p, significantly enhancing the final visual fidelity. The main contributions of this summarized as follows:",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 1,
          "region_type": "text",
          "region_id": "page1_region1"
        },
        "score": 0.489,
        "citation": "2511.18870v2.pdf, Page 1"
      },
      {
        "text": "videos across four dimensions: sharpness, detail retention, noise & artifacts, and This allows us to discard clips with poor visual quality. Filtering: We applied an aesthetic scoring operator based on to evaluate the of the videos, filtering out those with low aesthetic scores. these filtering stages, approximately 800 million high-quality video segments play a crucial role in video generation, significantly influencing precise",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 3,
          "region_type": "list",
          "region_id": "page3_region2"
        },
        "score": 0.462,
        "citation": "2511.18870v2.pdf, Page 3"
      },
      {
        "text": "in improving T2V physics plausibility. In addition, we conduct a visual analysis in Fig. 7 (a) for PhyGDPO. gradually using our proposed techniques, the tennis in the generated video no longer presents ghosting and its floating motion on the water surface better conforms to the physics laws of fluid buoyancy. vs. SOTA DPO. For fair comparison with two SOTA DPO methods: Flow-DPO [38] and VideoDPO, we them to finetune Wan2.1-1.3B with the same data and settings. As reported in Tab. 2c, PhyGDPO surpasses and VideoDPO across all tracks by large margins, especially on the hard action track, where it yields improvement. We conduct a visual comparison in Fig. 7 (b) on the challenging weightlifting action. Flow-DPO VideoDPO generate distorted or unstable body poses. In contrast, PhyGDPO generates coherent body motion",
        "source": {
          "paper": "605562216_1287553579874010_8068650067268945856_n.pdf",
          "page": 10,
          "region_type": "text",
          "region_id": "page10_region4"
        },
        "score": 0.458,
        "citation": "605562216_1287553579874010_8068650067268945856_n.pdf, Page 10"
      },
      {
        "text": "and proficient model establishes a new state-of-the-art among generation models. By releasing the code and model weights, community with a high-performance foundation that lowers the creation and research, making advanced video generation ac- broader audience. All open-source assets are publicly available at //github.com/Tencent-Hunyuan/HunyuanVideo-1.5 remarkable advancements in video generation models, with closed- Kling2.5 [I], Veo3.1 [2], and Sora2 achieving state-of-the-art (SOTA) open-source domain, models such as HunyuanVideo [4], StepVideo [5], and as notable contenders. However, despite these strides, most SOTA models accessibility and community-driven innovation. Wan2.2 [6] employs a (MoE) architecture that leverages two 14-billion-parameter expert models Although this MoE architecture enhances visual fidelity by leveraging different denoising stages, it inherently introduces computational inefficiencies, multiple large parameter sets (totaling 27B parameters with 14B activated), demands. While Wan2.2 [6] has attempted to address this with a more variant, which utilizes a high-compression 3D VAE to reduce memory model still exhibits limitations. Specifically, its generative capabilities, motion stability across frames and achieving the nuanced aesthetic quality applications, continue to fall short of practical demands. This situation",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 1,
          "region_type": "text",
          "region_id": "page1_region2"
        },
        "score": 0.454,
        "citation": "2511.18870v2.pdf, Page 1"
      },
      {
        "text": "models struggle to effectively differentiate fine-grained motion quality. Therefore, an offline optimization stage using Direct Preference Optimization (DPO). For a balanced prompt set (from LLMs-generated prompts and training video covering diverse dimensions (motion, scene, subject, etc.). Using a selected high-quality we generate N video candidates per prompt to create non-repetitive pairs. These pairs",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 7,
          "region_type": "text",
          "region_id": "page7_region6"
        },
        "score": 0.435,
        "citation": "2511.18870v2.pdf, Page 7"
      },
      {
        "text": "these challenges, we introduce to achieve state-of-the-art visual",
        "source": {
          "paper": "2511.18870v2.pdf",
          "page": 1,
          "region_type": "title",
          "region_id": "page1_region4"
        },
        "score": 0.422,
        "citation": "2511.18870v2.pdf, Page 1"
      }
    ],
    "sources": [
      "Hunyuan Foundation Model (Topic: Foundation Models) - Pages 1, 3, 7, 8",
      "Text-to-Video Generation with Physical Consistency (Topic: Video Generation) - Pages 9, 10"
    ],
    "has_evidence": true,
    "retrieval_stats": {
      "total_chunks": 10,
      "papers_searched": [
        "2511.18870v2.pdf",
        "605562216_1287553579874010_8068650067268945856_n.pdf"
      ],
      "by_paper": {
        "2511.18870v2.pdf": 8,
        "605562216_1287553579874010_8068650067268945856_n.pdf": 2
      },
      "by_region_type": {
        "title": 2,
        "text": 7,
        "list": 1
      }
    },
    "cached_at": "2026-02-11T00:40:49.331117"
  }
}